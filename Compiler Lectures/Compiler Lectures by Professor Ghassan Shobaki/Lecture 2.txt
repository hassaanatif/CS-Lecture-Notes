
                                                     ==========================================
                                                     Lecture 2: Register Allocation Concepts 
                                                     ===========================================

- Optimization done during the the optimzization phase are not really low-level optimizations that target a specific processor or that try to do a good 
  utilization of the capabilities of the hardware. 
- Optimizations that do good utilization of the hardware are in the backend. So the backend will have those low-level optimiztions that will optimize 
  a specific hardware. 

===============================================
Basic idea of common subexpression elimination
==============================================
- The idea of CSE is to reuse instead of computing something multiple times.
- Now in order to do that, it has to store the result somewhere from where it has the easy access. If it doesn't have easy access then that defeats
  the purose. If you don't have a fast enough device that will store this, then you will not do an optimization. So in those cases, it may be 
  faster to recompute instead of reusing.
- For this kind of reuse, the best storage device is the register. So you want to be in a CPU register. Whcih is the fastest storage device. 
- When you keep doing optimizations like these, that require reuse and optimizations, then you are increasing the demand for registers. And 
  increasing the demand for register is not a good thing because it may end up slowing the performance because there are a limited number of 
  physical registers on the machine. So this is something that we will face at the backend.
- And if you have 200 different variables in your function and you have 300 temporaries, why do you think has so many temporaries? 
- Well lets say that we have to compute an expression: x = a * b
   - so it will first load a into r1, then b into r2 and then compute a * b and store in r3. In this example, we have two variables and then we 
     have a temporary. 
- Well the first step in the code generation is the abstract syntax tree and some compiler may immediately convert this code into a linear 
  assembly-like representation while some compilers may delay this a little bit and work on the tree representation of the code. 

==============================================================================
Difference between Linear Representation and Tree Representation of the ccode
===============================================================================
- The assembly-like syntax defining the code is what we call the "linear representation of the code" (also known as ABSTRACT ASSEMBLY), while the 
  AST is what we call the tree representation of the code.
- It is also worth noting that a compier will typically use multiple intermediate representations throughout the code generation. So first it 
  will typically generate an AST and then it will move to a linear representation, which is abstract asesmbly and then it will start making the 
  abstract assembly more real i.e targeting the real machine.

==========================
Competition for registers
==========================
-Let's say you have 200 variables and 400 temporaries, but your machine only has 16 registers, so there is going to be a competition for registers. 


========================
"The Register Pressure"
========================
- The point is that optimization increases the demand for more registers. They increase what we call the "register pressure". And when that 
  register pressure increases, not all of these virtual registers can be stored in physical registers. So somem of these virtual registers 
  may get stored in memory. But when they end up in memory then that is slow.  So this means that we could end up slowing the execution. 
- So some optimizations that appear to be good and that eliminate redundancies , or they are doing a lot of reuse. They may end up increasing 
  the register pressure, increasing the competition for registers. And some of the variables and the temporaries will not be stored in 
  registers. They will be placed in memory and in that case our execution may be slowed down.


================================
Instruction Selection briefing
================================
- So essentially instruction selection is selecting the best sequence of instructions. You can apply it to the tree representation of the code. 
  You can also apply it to a linear representation of the code. 
- Instruction selection is about utilizing, or making the best use of the target processor. Or selecting the most efficient sequence of machine
  instructions for this intermediate representation (ABSTRACT ASSEMBLY). 

- Example: Load a -> r1
           Load b -> r2  
       mul r1, r2 -> r3

- Now we are doing the above code assuming that the machine doesn't have a multiply instruction that can operate on memory operands. But if the 
  target machine has a CISC instruction that can operate on memory operands then we can replace these load instructions with that multitply   
  instructions instead of loading the operands into individual registers and then multiplying. 
- AND AGAIN WE HAVE TO REMEMBER THAT LOAD INSTRUCTIONS CAN BE COSTLY.


===========================================================================
Algorithm for converting this Abstract Syntax Tree into Abstract Assembly
===========================================================================
- We have to perform a tree traversal. 
- But what kind of tree traversal? A DEPTH FIRST TREE TRAVERSAL!!


=====
ILOC
=====
- The book introduces an intermediate representation called ILOC 
- ILOC is expanded as "Intermediate Language for Optimizing Compiler".
- A load ijnstruction is written LoadAIr(arp), @a -> r1  pronounced load address immediate r activation record pointer address of "a" into r1.
- So the address is in a register and this register has a pointer to the activation record for the current routine that we are compiling. 
- And this address @a is an immediate number i.e the offset of a. 
- Each active function has a location on the stack that we call the activation record of this funnction and it has the local variables of this 
  function. It has the paramaters of this function, the return address, or the return value of the function etc.
- Each of these activation records will have an address associated with them. 
- The address of the the activation record will be stored inside a register called "the activation record pointer register". It is a special 
  register that holds the starting address of the activation record of the current function. 
- So to compute the actual address of a variable,  
- So the above LoadAI instruction just means that: A load instruction takes a register, the base is the address of the activation record pointer 
  and the address is the offset for that variable. And a simple "load a -> r1" is a shorthand notation for this.  
- In the above notation, "a" is not a variable it is an offset. And also, we are assuming that all our variables are stored at a certain offset 
  from the address of the stack frame. 

====================================================================================
How does the register allocator map these virtual registers into physical registers
====================================================================================
- Assume that the target machine has 3 physical registers: p1, p2, and p3. 
- And let's say that we have 7 virtual registers. 
- So the job of the register allocator is to do the mapping between these. 
- So how do we map these? 
- Let us consider the following code: 
        load a -> r1 
        load b -> r2 
   mult r1, r2 -> r3
        load c -> r4 
        load d -> r5 
        div r4, r5 -> r6
        add r3, r6 -> r7
        store   r7 -> x

- Now when we are mapping this, we will assume that we have 3 registers:  so a goes into r1, b goes into r2. Now for the multiply instruction 
  do we need a sepearte register or can we reuse one of the already used registers? Well if the virtual register isn't being used sometime later
  during the code then we can easily reuse and store our multiplication result into the register r1 


=========================
Live range of a register
=========================
- It is the distance between the definition (putting a value in a register) and all the way way up to its last use (the last time, we used the 
  register). 

=====================================
Stronger competition for registers
=====================================
- When we have longer live ranges, we have a stronger competition for registers. So we say that we have high register pressure. 
- By the "register pressure", in this context, we mean the number of registers that are live at a certain point. 
- By the way, we are assuming that the registers that we are using for the source operands can be reused (if the live range of that particular
  register stops at that point).  For example:
   div r4, r5 -> r6 

- If after the above code the registers r4 and r5 are not used then we can say that the live range of both these registers stops at that line, 
  and thus we can use either one of these two registers to store our output.Because you will read these values of r4 and r5 before you write to
  the r6


=====================================================
Reducing the available registers to just 2 registers
=====================================================
- Let us try to map those above virtual registers to the physical ones that we have available. What can we do? Well we are safe up until line #4, 
  we can keep reusing registers. However, when we get to line 5, the problem occurs that we now don't have any registers available because both 
  registers are live right now. 
- SO what to do? Well can store the value of one of the registesr into a memory location. So the updated abstract assembly code would be something 
  like: 
       load a -> r1 
       load b -> r2 
  mult r1, r2 -> r3
 store p1 -> mem-temp
       load c -> r4
       load d -> r5
   div r4, r5 -> r6
load mem-temp -> p2
   add r3, r6 -> r7
   store r7 -> x


======================
What is the point here
======================
- WHen we have fewer registers, the register allocator could not map each virtual register to a physical register. It then had to do some storing 
  and loading. In this case r3 did not get completely mapped into a physical register. In-fact, we put it into a physical register temporarily 
  and then we stored it in a memory location and then we loaded it again when we needed it. So this kind of store is known as "Spilling" or a 
  "spill".

=======================================
Register Spilling OR Spilling OR Spill
========================================
- When we don't have enough room for a variable on the REGISTER FILE, we will spill it to memory and then we will load it again when needed.
- This means that we have high demand for registers and the register allocator will not be able to meet that demand then it will be forced to 
  spill some of its variables onto the memory from the register. 

===========
Spill Code
===========
- The store and load code involved in the register spiling process. 
- These slow down the performance of the code. Or the execution of code. 
- We are adding instructions (load and store), and these instructions will use the machinery surface, the functional units and all the machinery 
  sources and they will be using the memory system.
- AND THE MEMORY SYSTEM HAS A LIMITED BANDWIDTH. 
- SO we are executing more loads and stores and that is slowing our excecution. 

==========================================
More on the job of the Register Allocator
==========================================
- The job of the register allocator is to do the mapping with minimal spill. 
- The job of the register allocator is the minimize these spills. 
- It tries to accomodate all the virtual registers and map them into physical registers. 

==========================
QUESTION TO CONSIDER HERE
==========================
i) With the architecture of the today's processor, when it spills over to the memory, will it go into the L1 Cache? 
A: Well, even if it does, genreallly speaking, we can never really control if something is in L1 cache or not. But mostly likely, when we are 
   spilling, these spills are going to go to the stack. And when they go to the stack, they are getting reused within a short period of time, 
   so they are very likely to be in the L1 cache. So most likely, things will be hitting in the L1 caches and there will not be cache misses. 
   But even if you hit the L1 cache, this is still slower than storing them in a register. L1 cache is slower than a register. Accessing L1
   cache will take two or three cycles. While accessing a register is instantaneous. You can access a register immediately. 
   So definitely L1 cache is much faster than main memory and you can access it in two to three to four cycles. While to access main memory, you 
   need hundreds of cycles. So it is two orders of magnitude faster than main memory but it is still slower than a register. And we are also using
   other resources. We are using the memory system and your memory system has a limited bandwidth. You are excecuting more memory executions. And 
   memory executiions are expensive. Overall you have limited resources in the machine. And these spill instructions that we are adding are going
   to use resources. They will use issued slots. And these instructions will use an issued slot that could be used for another instruction. 
   Also, ofcourse if the spill misses the cache, it will be a huge performance hit. 

=================
Interesting notes
=================
- Optimization increases the demand for registers and one of these optimizations that increase the demand for registers is the common subexpression
  elimination. The other one is the another optimization at the backend which is instruction scheduling. Instruction scheduling increases the 
  demand for registers. The basic idea of the instruction scheduling is hiding the latencies of the code. 
  