========================================
Lecture 0 : Introduction and Syallabus
========================================

=================================================
How do we do the following operation: z = x + y
=================================================
If the machine is a RISC architecture, it will do the following: 
- load x into register 1
- load y into register 2 
- add r1r2 
- store r3 -> z
- Since assembly doesn't know variable names and if x is going to be a local variable stored on the stack then this x in the above instruction will
  be replaced by the offset for that particular variable relative to the starting address for that particular function. 
- The starting point for that particular function is also called the "activation record pointer".


===================
Compiler components
====================

==========
Linker
==========
- Linker links your program to the library functions 
- It chooses a location to put the object code in memory when the program is loaded 

========
Linking
========
- Your program may consist of multiple source files and each source file will get translated into one object file and linking is about linking 
  these object files into an executable.
- Each object file might not be executable by itself.
- In a nutshell, it will resolve all internal or external references and in addition to that, it does one very important thing: 
- It OPTIMIZES YOUR CODE!
- Even though the word optimization is misleading, because a compiler doesn't generate optimal code all the time. 
- If someone ever figures out how to build a compiler that produces optimal code alll the timme then research in compiler optimization will stop. 
- But the compiler produces good code. To make it more efficient. To make it better. 
- The focus of the course is doing the basic translation from high level language to machine code. We will cover fair amount of optimizations but 
  this will not be the main focus of the ocurse. 


=============================
Intermediate Representation
=============================
- The compiler doens't directly go from highh level code to low level code. There is usually some intermediate level. And a very typpical 
  intermediate level representation of the code is abstract syntax tree. Which is a tree represetnation of the code.  
- After generating the intermediate representation, this is then passed into the OPTIMIZER and then the optimizer will apply some transformations 
  to improve the quality of the code and eliminate redundancies. And it will OPTIMIZE IR (intermediate represnation). 
- So this AST is ONE OF THE INTERMEDIATE REPERESTNATIONS. A compiler typically uses multiple intermediate representations. 

===========
Back end 
===========
- This is the last part of the compiler as it does code generation. So it is going to do: 
  i) Instruction selection 
 ii) Instruction scheduling  
iii) Register Allocation (deciding which variables will go into which registers)

- And then it will generate assembly. 


================
Interesting Note
=================
- 60% of the compiler construction course will be focused on understanding how the code goes from the front end to the optimizer.

- The process of going from font end to the optimzer involves what we call scanning and parsing.  And then we have semantic analysis. 


=================
Layman summaries
=================
1. Scanning: You look at your input andd divide it up into tokens and words.                                                                    
2. Parsing : Parsing takes these words and forms it into sentences. Which is a complete program. 
3. Semantic Analysis: Checking if the sentences that we have formed are meaningful. 


===========
Semantic
===========
- It refers to a program that is grammatically correct but it is MEANINGLESS. 
- An example of semantic error is a type mismatch. In English, it could be: The apple at the boy. 


- Normally your program will have a lot more variables than you have registers on the CPU.


===========================================
Basic constructs of a programming language
===========================================
- A programming language will have expressions and assignments. 
- Conditionals (if...else). We can't write a meaningful program without these
- Loops 
- Alll these above are the essential features of a programming language. 


========
Fun fact
========
- Electronics is built on nuclear theory because before people started exploring nuclear theory, they didn't know about the electron. And when
  they started exploring nuclear theory, they started studying the structure of the electrons etc etc 
- So anything theoretical MUST NOT be underestiamted. You never know of its practical applications in the future.
- Theory opens new horizons for us. 


                                                     ==========================================
                                                     Lecture 1: structure and Major Components
                                                     ===========================================
- A simple compiler may use multiple intermediate representations to represent the code. 
- In compiler construction, we refer to variables (such as a, or b) as "symbolic information. 
- In assembly language, we don't have symbolic infromation. Instead of that, we have memory addresses. 
- Once the compiler generates an abstract syntax tree, it no longer needs the symbolic information. 
- But in practice, there are reasons for keeping symbolic information throughout the compilation process. That reason is DEBUGGING. 


==================
First phase intro
==================
- The first phase that converts the high level code into abstract syntax tree is called scanning or lexing. 
- Scanning is a low level analysis of the input program. 
- The scanner looks at our whole program as one big string and it will divide it up into tokens or lexemes.
- Consider an example: 
    x12 = a3 + b45 
- The first token from the above example will be "x12". Now how did the compiler know what the first token was? So we know that are we only allowed
  to have letters possibly followed by numbers as the variable names, so we can generalize this in a way by saying that it knew that the equal sign
  marked as the end of the first token based on a CERTAIN RULE. And we call these rules as REGULAR EXPRESSIONS. 
- There is no rule in the programming that says that something starts with an equal sign, so hench the second token will just be an equal sign
  or the assignment operator.  
- The implementation of a lexer will involve a finite state automata. 
- If your program passes the scanning phase, it means that all the tokens, all the parts of speech are correct

=========
Parsing
=========
- Also known as syntactic analysis. 
- It basically builds words or sentences
- The equivalent of a sentence in a programming language is a statement 
- The equivalent of a word in programming is a word.
- A grammar is what we will be using to define the syntax of the different legal statements. 
- Basically scanner is built using regular expressions and finite automatas. And parsing is done with the help of grammars.
- If your program pases this syntactic stage then it means that it will contain legal statements

===================
Semantic Analyzer 
===================
- The third part of the frontend is the semantic analyzer. 
- Here we check if the statement makes sense. 
- If your program passes this stage then only it will be a valid program. 

==========================
Few more important points
===========================
- Parsing or checking syntax can be done locally. 
- But in order to check the semantics, we have to do a more deeper and a more global kind of analysis. We can just match it up against the rules 
  i.e grammar.
- So we if we have to do a semantic analysis on an assignment like this: x = a + b; ,  we have link that statement up with the declarations at
  the locations where each of those variables have been defined. And they may have appeared much earlier in the code.
- In order for compilers to do this, they process a declaration, they're going to create an entry for each variable in what we call a symbol 
  table. So these declarations go into the symbol table. That symbol table will have all the differnet information regarding the defined 
  variables. It's type, it's scope etc. And then when this variable is used, the compiler is going to look up that variable in the symbol table 
  and check its type and then make a check whetehr this statement is meaningful or not. 
- We will spend more than half of the time studying the front-end. 


============
Optimizer 
============
- Here, we will initially have some form of intermedatory represetnation. 
- Not all commpilers will have AST as an intermediate representation
- Some compilers will have a linear intermediate representation 
- In this phase, the compiler will be applying some transfomrations to improve the quality of the code. 
- In theory, the optimizations that are applied here should be machine indepndent. 
- Everything that is machine specific, that are related to the generation of the code for our target machine should be ideally done at the backend.
- Some compilers generate code for many different machines and many different processors. The extreme example is a gcc compiler. Which means that 
   the backend is complicated. For a good design, theere will be multiple backends. Or this backend will be applied in a way in which we have some 
   common stuff that applies to all machines and all processors. So there is some processor specific stuff. 
- Backend may be divided into abstract or general stuff that applies to all processors. And then the other areas are specific to target #1, target
  #2 etc. These targets are specific processors. Like intel x86, ARM, GPU, embedded processors. 

=========================
Common optimizations done
=========================
- One of the most common optimizations done is common subexpression elimination (CSE) , example: x = a + b + c    AND y = d/(a+b)
- From the above (a+b) is a common subexpression. So a compiler will do this, it will just compute a+b and put it in some temporary. 
- Transformation: t = a + b; , x = t + c; , y = d/t;
 - Generally speaking, there's no guarannteed way that will make the code faster. Because it may make things better from one aspect but it may make
  something else worse. 
- So in the above example, the only way that it would benefit us is if the t was stored in a register and not in memory, because in some computers
  loading from memory is order of magnitudes slower. In-fact, sometimes, recomputing values may be faster than loading data from the memory. 
  Because in the above example, a+b is an add operation that can be done in a single cycle. Though even if the data is in cache, bringing it 
  may take more time than recomputing A + B. The above optimization would only work if T was in the register for a long time. And with this 
  variable reserving a register for a long time, other variables may not have the chance to be stored in a register. And so we victimize other 
  variables this way. 
- Optimization is not a beffiting word here because it is not gauranteed to make things even better. Not to mention optimality. It is not 
  guaranteed too make things better. It can make things worse. But it hopes to make things better. 
- But compiler optimizations make things better most of the time.
- So in theory, optimization is machine independent but if you think about it deeply, it will not be 100% machine independent. Because when we 
  start talking about memories, caches, register etc. Whether this will be good or bad will depend on how much cache will have, how many caches, 
  registers etc we have. 
- That is why in a real compiler, this optimizer will often have some machine dependent variables. 
- So this CSE optimization is a good example of machine dependent optimization because it will be a good optimization for a machine that has 
  enough registers otherwise for a machine with not too many registers, it will not be a good optimizaiton.

===========
Backend 
===========
- This is the part that will truly be machine DEPENDENT.
- Here we will do instruction selection and instruction scheduling and register allocation. 
- To go from AST (Abstract Syntax Tree) to assembly, 
=====================
Instruction selection
======================
- So instruction selection is selecting instructions. 
- Abstract syntax tree is a very logical hiearchial representation of expression computation and assignment 
- To go from  an AST to assembly it is common to generate some generic or abstract assembly. Like the assembly for some RISC machines, because they 
  don't have many instructions. So you are generating basic assembly that is the instructions that you are using are likely to be on all 
  processors. All processors will have add, load etc. So using hte basic operations. 

Example:  x = a*b+c*d; 

- In the above example we do the following: 
   load a -> r1 
   load  b -> r2
   mul   r1, r2
   ..

- So in the above example we are assuming that we will have infinite number of registers. This is obviously a non-realistic assumption. And compilers
  do this at some point before they get to register allocation. So before they get to register allocation, the compilers assume that they have 
  infinite number of registers.
- The job of a register allocator is that it will take all these virtual registers (VIRTUALLLL) and map them into real or physical registers. 
- Another observation from the above code is that there we are generating a code sequences (loading a, b into register and then multiplying 
  registers). BUT on some machines there are complex instructions that may operatoe on memory operators. In-fact, if we have a memory operation
  or a multiply operation and then putting that could be in memory or in registers.  ON A CISC MACHINE, instead of these multiple registsers and 
  lines of code to multiply two numbers, you may have just a single instrucction that does just that and stores the result in some register or 
  in one of the variables. For example: 
   mult a,b -> r1
- The mmachine may also have an arithmetic operation that performs an operations on two memory operands and puts them in a seperate memory location
  and that is a CISC instruction or a COMPLEX instruction. That the machinee itself consists of multiple simple risc instructions. 
- SO WHAT IS THE POINT HERE? THe point here is that which instruction shouldd get selected and what is the most efficient sequence of instructions 
  is the job of INSTRUCTION SELECTION.


============================================
Point to pick up from instruction selection
============================================
- instruction selection will take the abstract syntax tree, a high level code represnetation and convert it into an abstract low level assembly, 
  which is not quite machine code yet. But when it goes through instruciton selection, it will be replaced by actual instructions. So when we 
  go through instruction selection, we are making the code more real. So we start off with virtual or abstract things and gradually we make them 
  little by little more real. And then we will end up with a real code that will run on a real machine.

========================
Instruction scheduling
=======================
- These ordering of intructions is not neccessarily the best ordering for executing the seuqnece of isntructions. 
- Because a load instruction in particular have latencies. So a load instruction may take a few cyclces to excecute. When you get to this multiply
  you will have to wait for this load or the result of this load to become available and this can wait 2 or 3 or 4 cycles depending ont the machin
  -es. So we can instead re-order the instructions to hide latencies. 

                                                     ==========================================
                                                     Lecture 2: Register Allocation Concepts 
                                                     ===========================================

- Optimization done during the the optimzization phase are not really low-level optimizations that target a specific processor or that try to do a good 
  utilization of the capabilities of the hardware. 
- Optimizations that do good utilization of the hardware are in the backend. So the backend will have those low-level optimiztions that will optimize 
  a specific hardware. 

===============================================
Basic idea of common subexpression elimination
==============================================
- The idea of CSE is to reuse instead of computing something multiple times.
- Now in order to do that, it has to store the result somewhere from where it has the easy access. If it doesn't have easy access then that defeats
  the purose. If you don't have a fast enough device that will store this, then you will not do an optimization. So in those cases, it may be 
  faster to recompute instead of reusing.
- For this kind of reuse, the best storage device is the register. So you want to be in a CPU register. Whcih is the fastest storage device. 
- When you keep doing optimizations like these, that require reuse and optimizations, then you are increasing the demand for registers. And 
  increasing the demand for register is not a good thing because it may end up slowing the performance because there are a limited number of 
  physical registers on the machine. So this is something that we will face at the backend.
- And if you have 200 different variables in your function and you have 300 temporaries, why do you think has so many temporaries? 
- Well lets say that we have to compute an expression: x = a * b
   - so it will first load a into r1, then b into r2 and then compute a * b and store in r3. In this example, we have two variables and then we 
     have a temporary. 
- Well the first step in the code generation is the abstract syntax tree and some compiler may immediately convert this code into a linear 
  assembly-like representation while some compilers may delay this a little bit and work on the tree representation of the code. 

==============================================================================
Difference between Linear Representation and Tree Representation of the ccode
===============================================================================
- The assembly-like syntax defining the code is what we call the "linear representation of the code" (also known as ABSTRACT ASSEMBLY), while the 
  AST is what we call the tree representation of the code.
- It is also worth noting that a compier will typically use multiple intermediate representations throughout the code generation. So first it 
  will typically generate an AST and then it will move to a linear representation, which is abstract asesmbly and then it will start making the 
  abstract assembly more real i.e targeting the real machine.

==========================
Competition for registers
==========================
-Let's say you have 200 variables and 400 temporaries, but your machine only has 16 registers, so there is going to be a competition for registers. 


========================
"The Register Pressure"
========================
- The point is that optimization increases the demand for more registers. They increase what we call the "register pressure". And when that 
  register pressure increases, not all of these virtual registers can be stored in physical registers. So somem of these virtual registers 
  may get stored in memory. But when they end up in memory then that is slow.  So this means that we could end up slowing the execution. 
- So some optimizations that appear to be good and that eliminate redundancies , or they are doing a lot of reuse. They may end up increasing 
  the register pressure, increasing the competition for registers. And some of the variables and the temporaries will not be stored in 
  registers. They will be placed in memory and in that case our execution may be slowed down.


================================
Instruction Selection briefing
================================
- So essentially instruction selection is selecting the best sequence of instructions. You can apply it to the tree representation of the code. 
  You can also apply it to a linear representation of the code. 
- Instruction selection is about utilizing, or making the best use of the target processor. Or selecting the most efficient sequence of machine
  instructions for this intermediate representation (ABSTRACT ASSEMBLY). 

- Example: Load a -> r1
           Load b -> r2  
       mul r1, r2 -> r3

- Now we are doing the above code assuming that the machine doesn't have a multiply instruction that can operate on memory operands. But if the 
  target machine has a CISC instruction that can operate on memory operands then we can replace these load instructions with that multitply   
  instructions instead of loading the operands into individual registers and then multiplying. 
- AND AGAIN WE HAVE TO REMEMBER THAT LOAD INSTRUCTIONS CAN BE COSTLY.


===========================================================================
Algorithm for converting this Abstract Syntax Tree into Abstract Assembly
===========================================================================
- We have to perform a tree traversal. 
- But what kind of tree traversal? A DEPTH FIRST TREE TRAVERSAL!!


=====
ILOC
=====
- The book introduces an intermediate representation called ILOC 
- ILOC is expanded as "Intermediate Language for Optimizing Compiler".
- A load ijnstruction is written LoadAIr(arp), @a -> r1  pronounced load address immediate r activation record pointer address of "a" into r1.
- So the address is in a register and this register has a pointer to the activation record for the current routine that we are compiling. 
- And this address @a is an immediate number i.e the offset of a. 
- Each active function has a location on the stack that we call the activation record of this funnction and it has the local variables of this 
  function. It has the paramaters of this function, the return address, or the return value of the function etc.
- Each of these activation records will have an address associated with them. 
- The address of the the activation record will be stored inside a register called "the activation record pointer register". It is a special 
  register that holds the starting address of the activation record of the current function. 
- So to compute the actual address of a variable,  
- So the above LoadAI instruction just means that: A load instruction takes a register, the base is the address of the activation record pointer 
  and the address is the offset for that variable. And a simple "load a -> r1" is a shorthand notation for this.  
- In the above notation, "a" is not a variable it is an offset. And also, we are assuming that all our variables are stored at a certain offset 
  from the address of the stack frame. 

====================================================================================
How does the register allocator map these virtual registers into physical registers
====================================================================================
- Assume that the target machine has 3 physical registers: p1, p2, and p3. 
- And let's say that we have 7 virtual registers. 
- So the job of the register allocator is to do the mapping between these. 
- So how do we map these? 
- Let us consider the following code: 
        load a -> r1 
        load b -> r2 
   mult r1, r2 -> r3
        load c -> r4 
        load d -> r5 
        div r4, r5 -> r6
        add r3, r6 -> r7
        store   r7 -> x

- Now when we are mapping this, we will assume that we have 3 registers:  so a goes into r1, b goes into r2. Now for the multiply instruction 
  do we need a sepearte register or can we reuse one of the already used registers? Well if the virtual register isn't being used sometime later
  during the code then we can easily reuse and store our multiplication result into the register r1 


=========================
Live range of a register
=========================
- It is the distance between the definition (putting a value in a register) and all the way way up to its last use (the last time, we used the 
  register). 

=====================================
Stronger competition for registers
=====================================
- When we have longer live ranges, we have a stronger competition for registers. So we say that we have high register pressure. 
- By the "register pressure", in this context, we mean the number of registers that are live at a certain point. 
- By the way, we are assuming that the registers that we are using for the source operands can be reused (if the live range of that particular
  register stops at that point).  For example:
   div r4, r5 -> r6 

- If after the above code the registers r4 and r5 are not used then we can say that the live range of both these registers stops at that line, 
  and thus we can use either one of these two registers to store our output.Because you will read these values of r4 and r5 before you write to
  the r6


=====================================================
Reducing the available registers to just 2 registers
=====================================================
- Let us try to map those above virtual registers to the physical ones that we have available. What can we do? Well we are safe up until line #4, 
  we can keep reusing registers. However, when we get to line 5, the problem occurs that we now don't have any registers available because both 
  registers are live right now. 
- SO what to do? Well can store the value of one of the registesr into a memory location. So the updated abstract assembly code would be something 
  like: 
       load a -> r1 
       load b -> r2 
  mult r1, r2 -> r3
 store p1 -> mem-temp
       load c -> r4
       load d -> r5
   div r4, r5 -> r6
load mem-temp -> p2
   add r3, r6 -> r7
   store r7 -> x


======================
What is the point here
======================
- WHen we have fewer registers, the register allocator could not map each virtual register to a physical register. It then had to do some storing 
  and loading. In this case r3 did not get completely mapped into a physical register. In-fact, we put it into a physical register temporarily 
  and then we stored it in a memory location and then we loaded it again when we needed it. So this kind of store is known as "Spilling" or a 
  "spill".

=======================================
Register Spilling OR Spilling OR Spill
========================================
- When we don't have enough room for a variable on the REGISTER FILE, we will spill it to memory and then we will load it again when needed.
- This means that we have high demand for registers and the register allocator will not be able to meet that demand then it will be forced to 
  spill some of its variables onto the memory from the register. 

===========
Spill Code
===========
- The store and load code involved in the register spiling process. 
- These slow down the performance of the code. Or the execution of code. 
- We are adding instructions (load and store), and these instructions will use the machinery surface, the functional units and all the machinery 
  sources and they will be using the memory system.
- AND THE MEMORY SYSTEM HAS A LIMITED BANDWIDTH. 
- SO we are executing more loads and stores and that is slowing our excecution. 

==========================================
More on the job of the Register Allocator
==========================================
- The job of the register allocator is to do the mapping with minimal spill. 
- The job of the register allocator is the minimize these spills. 
- It tries to accomodate all the virtual registers and map them into physical registers. 

==========================
QUESTION TO CONSIDER HERE
==========================
i) With the architecture of the today's processor, when it spills over to the memory, will it go into the L1 Cache? 
A: Well, even if it does, genreallly speaking, we can never really control if something is in L1 cache or not. But mostly likely, when we are 
   spilling, these spills are going to go to the stack. And when they go to the stack, they are getting reused within a short period of time, 
   so they are very likely to be in the L1 cache. So most likely, things will be hitting in the L1 caches and there will not be cache misses. 
   But even if you hit the L1 cache, this is still slower than storing them in a register. L1 cache is slower than a register. Accessing L1
   cache will take two or three cycles. While accessing a register is instantaneous. You can access a register immediately. 
   So definitely L1 cache is much faster than main memory and you can access it in two to three to four cycles. While to access main memory, you 
   need hundreds of cycles. So it is two orders of magnitude faster than main memory but it is still slower than a register. And we are also using
   other resources. We are using the memory system and your memory system has a limited bandwidth. You are excecuting more memory executions. And 
   memory executiions are expensive. Overall you have limited resources in the machine. And these spill instructions that we are adding are going
   to use resources. They will use issued slots. And these instructions will use an issued slot that could be used for another instruction. 
   Also, ofcourse if the spill misses the cache, it will be a huge performance hit. 

=================
Interesting notes
=================
- Optimization increases the demand for registers and one of these optimizations that increase the demand for registers is the common subexpression
  elimination. The other one is the another optimization at the backend which is instruction scheduling. Instruction scheduling increases the 
  demand for registers. The basic idea of the instruction scheduling is hiding the latencies of the code. 
  
                                                  ============================================
                                                    LECTURE 3: Instruction Scheduling Concepts
                                                  =============================================

===========================================
What does a virtual register correspond to
===========================================
- When we are using, we are making an assumption that we have an infinite amount of registers. And a virtual register in the source program
  coressponds to a temporary or an intermediate result. 

=========================================================
What happens to a variable when an assembly is generated
=========================================================
- The variable is replaced by an offset relative to the activation record pointer.


==============
Pipelining
==============
- With pipelining, you can start a new instruction even if some instruction have not been completed yet. 

====================
Single issue machine
====================
- In such a machine, in each cycle, we can start a new instruction but assuming that this instruction is independent of the previous instruction.

=================
Data Dependence graph
=================
- When the currently executed instruction is dependent on another instruction then the compiler will create a dependence graph for it. 
- Demonstrated in Lecture 3 (video 4) , time: 9mins 30 seconds.
- In our above a * b + c * d example, the data dependence graph is essentially an inverted abstract syntax tree.
- But generally speaking that that is not always the case. The dependence graph is more complicated than that.


=================
Latiences
=================
- Some instruction have latencies. And these latencies can be costly in terms of performance. 
- Let us assume that for the load, we have a latency of 3 cycles. 
- Mulitply has 2 cycles
- Divide has 5 cycles
- Store has 2 cycles
- Add has 1 cycle 
- Usually we mention the latencies that are greater than 1
- Though these whole latencies deppend on the processor model. 

===============================
Assumptions that we are making
===============================
- We are assumping that each load will hit the cache because we are assuming that load will take just 3 cycle
- Otherwise, bringing something from the main memory takes like 100s of cycles. So it is two orders magnitude of bringing something from the cache)
 
===============================================================================
Can we always assume that something that has been loaded before is in the cache
================================================================================
No. Because it may have gotten locked out of the cache by something else. Also, compile time information will never match runtime information 100%.
We can model the processor all that we want at the compile time but it will not 100% match the runtime information. Also remember that compiler 
is allowed to guess when it comes to performance. However it is not allowed to guess when it comes to correctness.


============================================
How will the instruction scheduler fit here
===========================================
- The instruction scheduler will look at all these lines and it will try to reorder these insntructionsn to minimize the number of cycles. 
- An instruction scheduler may notice that these loads in the first few lines have longer latencies and these loads are independent, so I can 
  execute all the loads in the very beginning and do everything else afterwards.

=================================================
How the compiler hides the latencies of the load
=================================================
- SO in our example above (screenshot 1773), the compiler hid the latencies by placing the independent instructions between the load instruction
  and the consumer of the load instruction (which was the multiply in the case of a).
- By doing this, we are doing more instruction level parallelism. We are doing more instructions in parllalel. In our case, we are doing more
  loads in parallel.
 
============================
Which scheduling is better?
============================
- So when we try to achieve instruction level parallelism then that will require us to have more registers. Because that is going to increase
  the peak register pressure or the max register pressure.  
- So the question arises, should the compiler minimize the number of LIVE REGISTERS and increase the number of cycles or should the compiler 
  reduce the number of cycles and increase the number of LIVE REGISTERS?  The answer depends on the processor and the architecture.
- The idea of using more registers and minimizing cycles is very intuitive and can be understood with the help of a cooking analogy. If you're 
  trying to cook meat, vegetables, pasta and beef all at the same time then obviously you're going to need more containers. The idea is also that
  nothing comes for free.

================================================================
Hard problem to solve: NP Completeness of Instruction Scheduling
=================================================================
- Any realistic formulation of instruction scheduling is NP-Complete 
- So in-fact minimizing any of these two things: the number of cycles or the number of registers , in general is NP-Complete.
- So even if our itnent is find the ordering that will minimize the number of cycles. Even then, with long latency instruction that is an 
  NP-Complete problem. And even if our sole objective is minimizing the number of registers that is an NP-Complete problem.
- So in general, there is no polynomial time solution for these. Which means that there is no polynomial time algoirthm that can always compute
  the optimal solution to this problem. 
- Not having a polynomial time algorithm means that the algorithm that you have are super-polynomial or exponentials or factorials. For practical
  purposes, if you have a problem with n = 100 (where n is the number of instructions) then 2^100 is a huggeee number. If you do the math for this
  you will millions of billions of years.
- That is why compilers don't solve these problems exactly. So they use HEURISTICS. And these heuristics work well in most cases but not in all 
  cases. So, a good heuristic is an algorithm that is not guaranteed to give you the optimal solution but it gives you something close to the 
  optimal solution in most situations.


===========================================
Out of order processor vs In Order process
===========================================
- An out of order processor will do scheduling in such a way that it will minimize the number of cycles at runtime. While it will not do a 
  perfect job but it is going to do a good job. Nothing is perfect btw. The processor is not perfect and the compiler not perfect. 
- So an out of order processor will try its best to hide the latencies and it will try to use the registers that are not visible to the programmers
  to achieve more parallelism.
- So if your processor has an out of order execution and it has a good out of order execution engine, you better focus on minimizing the number 
  of registers. Because minimizing the number of registers will cause the register allocator to generate fewer spills (loads and stores). So that's
  much more important  than minimizing the number of cycles because the processor is going to try to minimize the number of cycles. Now will the 
  processor do a perfect job at this? The processor has limitations. So in terms of scheduling to minimize the number of cycles, it has limitations


===================================
But what does "Out of order means"
==================================
- Consider "screenshot 1773 (in screenshots)", before the multiply completes (on line#3) the processor is going to start the load from line #4, 
  trying to do more parallelism.
- But what are the limitations of a processor? Why is the processor limited? Why can't the processor do a perfect job?  The answer is that it 
  can only look so far ahead i.e the SCOPE OF THE COMPILER is a issue. So this is going to be in the reorder buffer 


====================
Reorder buffer
====================
- There is a buffer. That will just have 10s of instructions in it. But in general, it will only have limited scope. So the processor can only 
  look at this limited scope (which is also the size of the reorder buffer). 

====================
Scheduling regions
====================
- While the compiler, can look at the bigger piece of code. Even though
  there are limitations on the compiler side too. There is no compiler that schedules a whole program at once. Because scheduling a whole program
  at once is just too complicated. 
- But it is going to divide the program in regions. And these regions are called "scheduling regions". And these scheduling regions are limited 
  in size, too. But they can be bigger than the reorder buffer on the hardware. So, the size of the scheduling region, you know...the compiler 
  can look at hundreds of instructions and sometimes it can look at  thousands of instructions at the same time. 

============================================
Branching make things even more complicated
============================================
- They make them more complicated for the hardware. And they make them more complicated for the compiler. Bcause - in fact, you know, having 
  branches is one of the reasons why a compiler cannot look at the big piece of code. Usually, the compiler stops at the branch.

================================
Boundaries of scheduling regions
=================================
- Branches are what define the boundaries of scheduling regions.

=======================
Speculative execution
=======================
- Branching makes things complicated for the hardware as well.
- If the hardware starts to execute beyond the branch instruction then it cannot commit it. We call this kind of execution "Speculative execution".
- The hardware can only commit speculative execution until that branch is resolved. Branches complicate things but all meanningful programs have 
  branches.

=============
Intel x86
=============
It is an out of order processor. Even ARM processors (modern ones) are out of order processors.  

==========
Summary 
==========
- We know that instruction scheduling and register allocation are. 
- We know how both these things conflict with each other (whether to minimize the number of cycles or the number of registesr). 
- It is typical in compiler optimization for things to conflict with each other. So improving something can make something else worse. That is why 
  generating good, efficient (optimized) code is a non-trivial task. There are a lot of tradeoffs. Lots of conflicting objectives. That is why it 
  is a very complicated process.
- Compilers hopefully generate good code in most cases 
- So far in the series of lectures, we have covered the "what of a compiler". What compilers do. What the different parts of the compilers do. We 
  have not covered the "hows" of the compiler. How it does it. We haven't started a specific algorithm for doing instruction scheduling.


                                 =============================================================
                                 Lecture 4: Scanning: Finite Automata and Regular Expressions    (JUST INTRODUCTION)                        
                                 ============================================================

=============================
How do compilers do scanning?
=============================
They do it through finite automata and regular expressions. Regular expressions, grammar and automatas are  the tools for recognizing languages. It is also
important to realize that a programming language is a UNION of a bunch of other languages. For example: 

X12 = 32 ; 
--  - -- -
We have tokenized it as the language of all the valid identifiers (i), an assignment operator language (ii), a language consisting of an unsigned constant 
(iii) and an end of statement symbol (iv). So as we can see 4 different languages.
So the union of all these languages will give us a valid program (or a valid string).

================================
From scanning point of view only
=================================
From scanning point of view, a valid program is a program that consists of a sequence of valid tokens. And each token belongs to a valid part of speech 
(for example a valid identifier).


===============
Some old terms
===============
Alphabet: Finite set of symbols
String:  Sequence of symbols from a given alphabet 
Language:  set of strings 
Formal language: To distinguish it from human/natural and programming languages. There is a relation between programming language and a formal
                 language. Language doesn't have to be meaningful or interesting.  Now in this course, we will be defining some languages that 
                 are useful.  

- In a DFA, we have to have a transition for every symbol in the alphabet set.

============================
What does Finite Automata do
============================
It checks for the validity of a string. We can identify REGULAR LANGUAGES with the help of finite automatas and regular expressions. WE can also
use regular grammar to recognize regular languages. 


======================================================
What can you do in an NFA that you cannot do in a DFA
=======================================================
- The empty transition
- More than one transition for a symbol in the alphabet set.
- No transition for some of the symbols in the alphabet set.

==================================
Which one is easier to work with?
==================================
- An NFA.
- Also, every DFA is an NFA. But the opposite is not true.
- There are dead paths in NFA. 
- The NFA is a compact way of representing multiple paths in a more compact way. Usually NFA is more compact than a DFA. It is more comapct because it is 
  encoding multiple paths and multiple possibilities.
- NFA has a large number of paths encoded in it. NFA is a compact way of encoding a large number of paths.
- While an NFA is easier to write, it is very HARD to trace because it is non-deterministic.


=========================================
Which automata is more easier to program
=========================================
- A DFA. Because a DFA is almost ready for programming. It's just a table (transition table) that tells you where  to go. So all that you need to program
  it is a switch statement.
- While NFAs are easier to write, we can more easily represent rules using regular expressions. So a regular expression is much easier to express a general
  rule than an NFA.

==========================================
What can you have in a regular expression
==========================================
- If our Sigma is equal to {0, 1} and we have a regular expression: (0+1)*111(0+1)* then we can write it shorthandedly as (Sigma)*111(Sigma)*  because 
  sigma closure is the union of all possible symbols and zero union 1 is sigma.


========================================
Coverting Regular Expressions into NFAs (Thompson's Construction)
========================================
- Because a regular expression has epsilons in it then that means that it can be easily converted into an NFA. 
- We use Thompson construction to convert a regular expression into an NFA.


=================================================================
So how does this fit into the context of scanners and compilers
=================================================================
So in-fact what happens in scanning...scanners and compilers are built by writing regular expressions for the different parts or for the valid tokens in 
a language like identifiers, floating points, if-statements, whatever. Every valid token...there is a rule that defines it using regular expression. Then 
we take this regular expression that we build for the low level syntax of the langauge (or the micro-syntax)...you know the syntax at the word level. Then
we convert that regular expression into an NFA and then we take that NFA and convert it into a DFA. 
SO we start with a regular expression because a regular expression is the easiest way for humans to DEFINE A RULE for a regular language. But we end up
with a DFA why?? Because we can code it easily!! Trying to write a program that implement a regular expresison means first converting it into an NFA and 
then converting it into a DFA and then programming it. 


===========================================
A tool that will generate a scanner for us
===========================================
What we feed into this tool is a file that has a regular expression in it and the tool converts the regular expression into an NFA and then that NFA into
a DFA. So that conversion is done automatically by the tool that we will be using. It gives you the C-Code. It's called "Flex".


                                 =====================================================================
                                 Lecture 5: Scanning: Regular Expressions for programming Constructs                   
                                 =====================================================================

====================
Why do we use these
====================
Scanner is that stage in the compiler in which we recognize words or tokens. WE DO NOTTTTTT recognize sentences (that correspond to statements in the 
programming). 

==============================================
Main distinction between scanning and parsing
==============================================
- Recognizing words is the scanning phase
- Recognizing sentences is the parsing phase.


=================================
Language of all parts of speech
=================================
The language of all parts of speech is going to be the union of the individual languages like the langauge for the identifier, the language for the constant
, the language for the brnaching statements like if, else etc.

=================================
Sigma in our programming language
==================================
When it comes to programming languages, our sigma is going to be the set of all valid symbols in the programming language that we are desinging a compiler
for. So we will have all the numbers, all the alphabetic characters and we will have upper case alphabets and all the operators: 

 Sigma = [0-9] U {a-z} U {A-Z}  + {- , +, *,...}

====================================================
Example: Regular expression for an unsigned integer
====================================================
We would write it something like: 0 UNION (1-9).(0-9)*   //This doesn't allow leading zeros
- If we were to write (0-9)* , we would have two problems: There can be leading zeros and the second is that it will allow empty symbol (nothing). So it 
                               would make it valid for us to write something like : x = ;


===========================================================
Example: Regular expression for a floating point number
===========================================================
- The assumption is that first off any integer is a floating point number
- RE: 0 U [1-9][0-9]* . [0-9]*  
      ---------------   ---------
      Integer part      real part

- Now what if we wanted to make the floating point optional: Well in that case, we will have to do a union with an epsilon string but we have to keep in 
  mind that since concatenation has a HIGHER PRECEDENCE than UNION therefore we use brackets to override the precendence.

                 RE becomes: (0 U [1-9][0-9]*)((.[0-9]*) U Epsilon) 



================================================
Example: Regular expression for a variable name
================================================
RULES ARE: i) You MUST start with a letter. ii) Then you can have letters and numbers . iii) You can also have underscores in the name 

([a-z] U [A-Z])(([a-z] U [A-Z]) U [0-9] U (_))*


        ===========================================================
         Situation: If we wanted to set a variable length limit?
        ============================================================
         In that case, let us consider K = (([a-z] U [A-Z]) U [0-9] U (_)) 
         Then let us consider that our variable character limit is 5 characters, so our regular expression becomes:
         RE: ([a-z] U [A-Z])(Epsilon U X U X^1 U X^2 U X^3 U X^4) 


=========================================
Example: Regular expression for a string
=========================================
Intuitely we might think that this is: "Sigma*" but don't forget taht Sigma* will have the closing brackets as well so this is false. We as part of our 
convenience, we use an extended regular expression: 
                                                      " ^"* "
Where the carrot sign means the compliement. Or it means ANYTHING BUTTTT the closing brackets. Without using this extended regular expression, we would
have to manually list out all the alphabets except for the closing brackets.And that would be very tedious.


                                 =====================================================================
                                 Lecture 6: Scanning:  Converting a Regular Expression into an NFA                
                                 =====================================================================


====================================================
Constructing an NFA for a trivial regular expression
====================================================
- It will be an NFA for a single letter regular expression. 
- Let's say that our alphabet set SIGMA = {a,b} then we can just construct an NFA that accepts just "a" and hence our regular expression for it will be "a".


        =============================================
        Standard operations over a regular expression
        ==============================================
        A standard regular expression can have any one of the following operations performed on it:
          i) Union, ii) Concatenation , iii) Star (Kleene's Closure)

        If a regular expression has any more of these operations then it is an extension to the notation of the regular expressions.

        ============================================
         Regular expressions as building blocks
        =============================================
         Regular expressions are the building blocks. We know how to construct an NFA for a single letter regular expression, so we that it is TRIVIAL. 
         Now if we can build an NFA for the 3 regular expression OPERATIONS then we can build an NFA for ANY LANGUAGE!!!!


        ===================================
         Breaking down the problem
        ===================================
         Now let us suppose that we have two NFAs : NFA1 and NFA2. So I have two NFAs (both for different languages) and I would like to construct an NFA
         for the UNION. Now the question is: 
                                            "Can I assume, without any loss of generality, that an NFA has a single accept state? Is this a valid assumption
                                             or not...that an NFA has a single accept state."
         Answer: Well, on the surfce, it may look like that we are losing generality, but in fact we are not losing generality. I can assume, without any 
                 loss of generality, that any NFA has a single accept state BECAUSE YOU CAN MAP ALL THE ACCEPT STATES INTO A SINGLE ACCEPT STATE.

        =============================
         NFA for a Union Operation
        ============================= 
         WE just put together a single start state and then we branch (like we did in Kleene's theorem). 
 
        ==================================
         NFA for Concatenation Operation
        ==================================
         This one is quite easy. We just link the end state of the first NFA to the start state of the second NFA and then we will use only the accept state
         of the second NFA. 

        ====================================
         NFA for Kleene's Closure or Star
        ====================================
         This one is a bit tricky. You might be tempted to have an epsilon transition from the final state to the start state. BUTTT! DON'T FORGET THAT 
         closure means that we can ACCEPT AN EPSILON as well. And if we make the start state as one of the final states then that WILL BE DISASTEROUS. 
         It will be disasterous becuase it will then end up accept a lot of the other stuff that shouldn't be accepted. THe right way to do this by 
         introducing a single start state and make it a final state and then introduce an epsilon transition from the final state to the OLD START STATE.


         =============================
          Picture as a demonstration
         =============================
          Reference: Picture #1944

        =============================================
        So how will we construct an NFA for any REGEX 
        =============================================
         We will use Thompson algorithm: We will first create TRIVIAL NFAs for each of the symbols in our REGEX. And then we will slowly construct NFAs for
         the operations on each of those symbols inside the regular expression using the rules that we talked of above. 
 
          
        ====================================
        Problem with the Thompson Procedure?
        ====================================
        It will introduce a ridiculously large number of epsilon transitions and minimizing the redundant epsilon transitions is NOT A TRIVIAL TASK.

          


                                 =========================================================================
                                 Lecture 7: Scanning:  Converting an NFA into a DFA (SUBSET CONSTRUCTION) PART 1          
                                 =========================================================================


===================================
Number of possible paths in an NFA
===================================
- In terms of the number of possible paths, if n is the size of the string then in the worst case, we will have an expponential function of n depending on
  the branching factor of n.

=======================
Screenshot reference
=======================
Screenshot #1946


                                 =========================================================================
                                 Lecture 8: Scanning:  Converting an NFA into a DFA (SUBSET CONSTRUCTION) PART 2              
                                 =========================================================================

========================================
Why does NFA have some many transitions
========================================
For two reasons: 
i) For a single symbol, you may have multiple transitions
ii) An epsilon transition


==============================
How many possible DFA states
==============================
If we have "n" DFA states then we can have 2^n states. 
Example: Let us consider that we have states n1, n2 and n3 then we can have the following states:
            i) n1 ii) n2 iii) n3  iv) n1, n2  v) n2, n3  vi) n1, n3 vii) n1, n2, n3  viii) empty-set

- The above states are the power set of the {n1, n2, n3} 


============
Power set
============
- Power set refers to the set of all the possible subsets. 

==============================================
Thinking of epsilon closure ALGORITHMICALLLY 
==============================================
- If we think of an epsilon-closure as a graph then we can apply ANY KIND OF TRAVERSAL and traverse only those edges where we have an epsilon transition.
  

===================================================
The new start after the conversion from NFA to DFA
===================================================
- The new start state after converting from NFA to DFA will be the EPSILON CLOSURE of the start state in the NFA. Why? Because the NFA's start state is a 
  state that you can never stay in. There is an epsilon transition out of it. So the start state of the NFA is an unstable state so you cannot stay in it.

===================================================================
Any unreachable states can be deleted after the subset construction
===================================================================
- After successfully converting to a DFA, you may get rid of any unreachable states.



                                 =========================================================================
                                 Lecture 9: Scanning:  Converting an NFA into a DFA (SUBSET CONSTRUCTION) PART 3          
                                 =========================================================================

=========================
Fixed point computation
=========================
- The type of computation that stops when when the state of our algorithm is not changing. Or when we stop discovering new states. In case of subset 
  construction, we only stop when we stop discovering new states.

============
Full list
============
Complete list of discovered DFA states.

==========
Work List
==========
List of DFA states that we are still working on that have been discovered but have not been processed yet. 


===============================================
Worst case scenario for subset construction
===============================================
The worst case scenario for a subset construction algorithm is 2^n (or an EXPONENTIAL FUNCTION). 

========================================================================
Why the worst case scenario is very unlikely in the subset construction
========================================================================
In practice, the worst case scenario for a subset construction algorithm is very unlikely and in practice this algorithm works just fine. Why is that? Well 
it is because in practice you would only discover a subset of the possible states.
This is ONE ALGORITHM WHERE THE WORST CASE IS VERY UNLIKELY. But even if it is slow, REMEMBER THAT THIS WILL NOT BE DONE AT COMPILE TIME. This is something
that will be done when we are building a compiler.


==============================================================================================
Keeping "Building a compiler" and "Compile the application" and "Run the application" separate
==============================================================================================
So if we are talking about let's say GOOGLE SEARCH ENGINE then this application is run by billions of people many many times a day while the compilation...
it gets compiled by the software company that makes that application. But building a compiler is done even less frequently. So if you are using Visual C++, 
that is built by Microsoft. So the point is that it is not done very frequently (building a compiler). Whether it is a open source or a closed source 
compiler. And the algorithms like subset construction etc are done when we are building a compiler and not when we are compiling an application.


==================================
Algorithm for subset construction
==================================
d0 = epsilon-closure (n0)

//add d0 to work-list and full-list 

1. while work-list is not empty 
2.  d = worklist.ExtractFirst()
3.  for every symbol c in the Sigma 
4.    t = epsilon-closure (delta (d, c)) 
5.    T[d][c] = t   //REMEMBER THAT A DFA CAN BE BUILT BY USING A TWO DIMENSIONAL ARRAY. LOOK UP SCREENSHOT #1957
6.
7.    if t is not in full-list  //MEANING THAT WE HAVE DISCOVERED A NEW STATE
8.         add t to worklist and full-list
 





                            ============================================================
                            Compilers Lecture 10: Scanning (7): Implementation, Part I
                            ============================================================
===============================
Tools that we are going to use
===============================
We will write a file that has regular expressions in it. And that file will have rules for different syntactic categories in the language. So you will have
RE for integers, RE for identifiers, RE for floats.
The REs are for the words or the syntactic categories for the language. So we will be taking these words and building up sentences from them in the 
parsing stage. Then there will be REs for different keywords. For example: if (which is a trivial RE), RE for for, RE for while. The RE for a keyword is the 
keyword itself because it is a language that has a single string in it which is that keyword. 
Then the union of all the above REs will be a big RE which will be our programming language. 

=================================================
Scanner generator and its internal implementation
=================================================
We have a scanner generator which takes REs and gives us C code for the scanner (which is also known as the scanner code). And internally it does that by 
converting this RE into an NFA using the THOMPSON algorithm. Then it does the SUBSET construction and it converts it into a DFA. Then it minimizes the DFA.
There are algos for minimizing the states of a DFA. DFA minimization algo gives you the minimal DFA. And then you finally generator the code for the scanner
or the DFA. The scanner code is going to be one of the many source files that constitute the compiler source code. So it is only going to be one stage of 
the compiler. 

============
Flex tool
============
This is the tool that we will be using to generate scanner code.

==========================================================================
Difference between compiling applications and building the compiler itself
==========================================================================
All of this will be done while building the compiler. It will not be done while we are compiling the applications.

=================================================================================================
EXAMPLE RE: RE for keywords starting with alphabets and ending with alphabets OR numbers OR BOTH
=================================================================================================
(START STATE) [a-z] -> (SELF LOOP) [a-z] [0-9] //for an identifier 
(START STATE) [0-9] -> (SELF LOOP) [a-z] [0-9] //integers with leading zeros allowed
(START STATE) f -> o -> (ACCEPT STATE) r       //for the "for" keyword. Also each keyword can have its own branch of the NFA
(START STATE) n -> e -> (ACCEPT STATE) w       //for the "new" keyword 
...and then you will have separate NFA branches for other syntactic categories like the above. 

================
The ease of NFA
================
The NFA is easier to understand and it is easier for humans because there are fewer states. But the corresponding DFA will have more states. 

===================
Matching X123 = 59
===================
Our scanner will take the first branch. And the "X" itself will lead to an accept state. But it will still keep scanning UNTILLLL it hits a different token
or delimeter that is not part of the regular expression. Or until it gets to a lexeme that doesn't match the branch taken for this REGEX. So we know that 
there is no transition shown explicitly for the "=" lexeme, so it will lead to a TRAP state or a DEAD state. 
So the scanner will have to ROLLBACK to the last accept and in this case the last accept was X123. So the scanner will identify this lexeme as an identifier.
Next up the "=" will be identified trivially by another branch of the NFA which will lead to an "assignment" syntactic category. 
Next when it gets to the 5, it will then roll back and identify "=" as a valid word. 

=================
End of statement
=================
So this is the process that it will follow and it will stop when it gets to the ";". It recognizess a semicolon as an end of statement. 

==================
The longest match 
==================
In order for the scanner to work, it will have to follow the longest match and not the first match.

=================================
What if we have multiple matches
=================================
What if you have "for"? So if you have "for" then for will match two accept states for an identifier and for a special keyword. Though keep in mind that we
are talking in context of NFA, it will eventually get converted into a CORRESPONDING DFA. So what will we happen if we have multiple matches? Well we 
SPECIFIY PRIORITIES.

=====================
Specifying prorities
=====================
Well this is something that the scanner generator will allow you to do. You can specify priorities. In the FLEX scanner generator, in the file for the RE, 
the priority is based on the order of the regular expression file. So if you specified a Regular expression for a keyword before you specified a RE for an
identifier then in case of multiple matches, it will always recognize the lexeme as a KEYWORD.

===========================================
An altnerative solution for identification
===========================================
Instead of having separate branches for special keywords and identifiers, we can identify everything as "Identifiers" first and then WE CAN CHECK
to see if that identifier matches a keyword or not. You have to check every identifier in a table called a "KEYWORD TABLE" and that table will
have all the keywords in it (if, else, new, for, while etc). So whenever you match an identifier, you check it in the table. If the identifier
is present in the keyword table then it is a keyword otherwise it is a REGULAR IDENTIFIER. 

=======================
Efficient Keyword table
========================
In order to make the keyword table efficient, we should be implementing it as a HASH TABLE because it is the perfect data structure for implementing
this keyword. In fact you can easily do PERFECT HASHING EASILY HERE. But why? Because you know EXACTLY the number of enteries in the table and 
there aren't many enteries in the table. So if you have 50 keywords then 50 enteries will all you have in this table. And if you can do perfect
hashing because the number of enteries is limited, this means that searching for the lexeme that you have matched in CONSTANT or O(1) time
SEARCH.

========================================================================================
Tradeoffs of separate REs or BRANCHES in an NFA as opposed to the KEYWORD TABLE approach
=========================================================================================
- Space disadvantage when implementing as separate REs because when the NFA will get converted into the corresponding DFA then it will have a 
  lot more states in the NFA and in the corresponding DFA as well. The resulting DFA will be much bigger when we have keywords represented as 
  REs.
- In terms of speed, in the hash table implementation, we will have to perform checking. So for every lexeme that matches an identifier we will 
  have to do some checking in the table. And in MOST CASES the result will NOT BE FOUND. WHY? Because in a typical programming language, you have
  more variables names than you have keywords. So most lexemes will be var names or func names. So it is like that for every 100 checks, any 5 
  will be hit the keyword table. So this checking is an OVERHEAD. Even though it is O(1) but it still takes time. It is not zero time. And this
  checking will be done when we are actually compiling the applications. So this is something that will affect the compile speed. It will be done
  when we are actually compiling a million line program. So there is an overhead here. Although it is not too bad because of O(1) but there is 
  still some overhead here. 

============================
Writing the scanner by hand
============================
The scanner is easy enough to be written by hand unlike the parser. Parsing is much more complicated than scanning. So if you have to build a 
scanner by hand, it is not that bad. It is non-trivial ofcourse  but is much easier than building a parser.


========================================================
About the next lecture: Code generation for the scanner
========================================================
We will discuss how the scanner is getting generated. Given a DFA, how will generate a code for that DFA. There will be two approaches:
i) Table driven approach (2D Array)
ii) Directed-Coded approach

Always remember that a DFA can be viewed as a 2D array in which the rows are the states and the columns are the symbols of the alphabets. For
example:
  s0  a  b  c....0  1  2
  s1  .  .  . ... . . . 
  s2
  s3




                            ============================================================
                            Compilers Lecture 11: Scanning (8): Implementation, Part II
                            ============================================================


============================
The "Done by hand" approach
============================
This pertains to code generation for DFA simulation. This code can be written manually or it can be done by hand. You can do the whole scanning process by 
hand. Some compilers use this approach. This means that it is manually coded. And other compilers use a scannner genertors to do this. A scanner generator
like Lex or Flex is fed regular expressions and it gives you the scanner code. 


===========================
Two schemes of generating code
============================
i) Table driven approach
ii) Direct coded approach

===========================
 Table driven approach
============================
Here, a DFA will be a 2D table or a 2D array.  Where the rows are your states (s0, s1,..sN). And the columns are your symbols (0, 1, 2...9, a,b,..z, =, ;..).

=================
The code will be based on table
=================
Based on the current state and also based on the current character that we are reading, we look it up in the table and then we move to the next state. And in
these states, there will be a state that represents an error, the state "Se". 

=============================
The explicit error state "Se"
=============================
What an error state in the NFA corresponds to is a "no transition". In a DFA, you must have an EXPLICIT error state. In NFA, this state is implicit. We need
this state because we need to check if the string that we are scanning stops matching. 

 FOR EXAMPLE: match the following: 
                                  xyz12 = abc;

In the above example, we will match until we hit the "=" character. Hence we will identify it as a lexeme.

============================
Labeling every accept state
============================
For every accept state, you need to label it as that corresponding syntactic category. For example the syntactic category in the above example is an 
identifier. 

======================================
Backtracking to the last accept state
======================================
When you get to the error state, you need to backtrack to the last accept state that you visited. So you then end up taking the longest match. Or the longest
lexeme that matches one of the syntactic categories in this language. This is also known as "roll back". In general, you may want to rollback or backtrack 
a few states back, but most of the times it is going to be just one state back. So that is why in a typical implementation, you put the states in a stack.
So in order to go to the latest accept state, you do the "pop" operation.

===============================
Starting over after recognition
================================
Once you successfully recognize the lexeme and the corresponding syntactic category, you start over. In the above, you start over matching from the "=" 
sign. 

=====================
Stack operation here
=====================
Popping from the stack is also known as "roll back". In general, you may want to rollback or backtrack  a few states back, but most of the times it is 
going to be just one state back. So that is why in a typical implementation, you put the states in a stack. So in order to go to the latest accept state, 
you do the "pop" operation. And you also need to have a mechanism to determine whether a particular state is an ACCEPT state or not. In a typical 
implementation, you may have an object and there could be a label in that particular object that tells whether it is an accept state or not. 

===========================
Resolving multiple matches
===========================
A particular accept state may belong to more than one syntactic categories and one way to resolve this conflict would be to give priorities to the different
syntactic categories. And a simple way of specifying the categories is just to go by the order in which they are listed in the regular expression file. The
file that you are using to generate a scanner for.

===========================
Redundancy in the table
===========================
Where is the redundancy in our table? Something that is taking unncessarily more space than it should? So all the numbers would be treated equally in most 
cases. So if two columns (like a, b etc) most likely will have the same transitions because they are treated equally. So if you multiple columns like these
then you can categorize the symbols of the alphabets by the "letters" category. And then the "digits" categories. Your actual transition table will then
be based on the categories. 

===========================
The categorization table
============================
You will have your categorization table look like this: 
                                                         0   1    2    3    4     a     b
                                                      digit digit .............letter...letter

And then the transition table will look like this: 
                                                      digit   letter   ;   
                                                   s0
                                                   s1
                                                   s2

So essentially there are symbols that you will have their own categories. And then there are multiple symbols that will get mapped to the same category. 
So this will minimize the number of columns.


=======================================
What if leading zeros were not allowed?
========================================
In that case, the above digits will probably be replaced by a non-zero digits category and then the zero will have its own category. And REMEMBER! You can 
put symbols in the same category only if they have identical transitions in all cases, for all states (if the corresponding columns are similar). 


==================================================================
What is the price that we are paying by dividing into two tables? 
==================================================================  
The price that we are paying here is two times table lookups. So although it is space efficient, it is not performance efficient. But sometimes, though, space
CAN AFFECT THE SPEED.

==================================
Space can AFFECT THE SPEED AS WELL
==================================
So using a lot of space can SLOW DOWN YOUR PROGRAM SOMETIMES. WHY?? BECAUSE OF CACHING. Sometimes using a lot of space means that your
data set doesn't fit in the cache which means a lot of slow down. It obviously depends on the size of your cache and whether the WHOLE DATA that your program
is accessing at a given point or phase can fit into the cache. 


================================================
Can we have identical states or identical rows?
================================================
If you don't do DFA minimization then you may have identical rows. DFA minimization is about merging multiple states that have identical transitions on all
symbols into state. You identify them as equivalent states. If you do DFA minimization you will NOT have identical rows but you will have identical columns.


=======================================
Psuedocode-- Adopting two table lookup
=======================================
state = s0 
lexeme = empty  
stack.push(NullState)
stack.clear() //BUT DO NOT GET RID OF THE NULL STATE

while (state != Se) 
  char = GetNextChar()
  lexeme = lexeme + char
  stack.push(state) 
  if (state belongs to set of accept states) 
       Stack.clear() //BUT DO NOT GET RID OF THE NULL STATE, also we're getting rid of everything because we only care about the LATEST ACCEPT STATE WHILE ROLLBACK.
   
  cat = CatTable[char]
  state = TransitionTable[state][cat]


//Exits loop and now we have to roll back. 

  while (state != SetOftheACCEPTStates && state != NULL) 
      state = stack.pop() 
      truncate lexeme //remove last character
      Rollback        //move the pointer in the lexeme to the previous character
  
  if (state BELONGS to SetOfAcceptStates) 
     return Type[state]   //This is assuming that the type table will have a way to resolve the conflict for multiple syntactic categories.
  else 
     return error  //we couldn't match.


============================
What to do about the lexeme
============================
Your lexeme is a string that you are saving in a certain buffer. And to truncate it, you just go back to the previous index in the buffer. How to return the
lexeme? In fact, when you write an actual scanner, it will be saved in a certain variable. So there will be a certain variable that saves the lexeme. So 
there is a return value which is the syntactic category and the lexeme itself will be saved in a certain variable for holding the lexeme. You need both. You
need to know that it is an identifier but knowing that it is an identifier is not enough. You need an actual name as a compiler because the compiler needs
to create a variable with that name.

=====================
Direct coded scanner
=====================
A direct coded scanner is a one that does not use tables. We will not have a loop. We will have an initialization or start code. Then instead of having a 
loop, there will be states. So instead of using the tables, you DIRECTLY CODE THE TRANSITION TO THE NEXT STATE.

==================================
Pseudocode -- DIRECT CODED SCANNER
==================================
Start: 
      state = s0 
      lexeme = empty
      stack.clear() 
      stack.push(NULL) 

S1: 
     char = GetNextChar() 
     lexeme = lexeme + char 
     if (state belongs to set of accept states) 
         stack.clear() 
     
     if (char == 'a')  goto s2
else if (char == ' ') goto s5
else goto End 


End: 
   while (state DOES NOT BELONG TO SET OF ACCEPT STATES && STATE != NULL) 
      state = stack.pop() 
      truncate lexeme 
      Rollback 
    
   if state belongs to Set of Accept States
      return Type [State] 
   else 
      return error 


=============================================
Advantages and disadvantages of Direct Coded
==============================================
This will look ugly (direct coded). It is ugly and messy if this is code that you would write manually. This is not the code that you would write manually 
typically. This is the code that you will get automatically generated by the SCANNER GENERATOR. You feed the regular expressions into the scanner generator
and it will generate it for you. So this is the code THAT YOU WILL NEVER HAVE TO DEBUG. FLEX probably uses a direct coded approach.
Inspite of the disadvantege of this, the big advantage here is the AVOIDANCE OF MEMORY ACCESS. 
Direct coded is only for one language, but with table driven approach, you can use it for almost any language.
From software engineering point of view, this direct coded approach is terrible when written manually by hand but it is automatically generated for us.
  

                            ============================================================
                             Compilers Lecture 12: Parsing (1): Context-Free Grammars
                            ============================================================


===========
Here's what happens
===========
Your program will be viewed by the scanner as a string of characters. The scanner divides this big string into words or tokens. The syntactic categories 
that the scanner identifies will become the symbols of the alphabet set that the parser deals with. At the parsing level, the alphabet set will consist of 
identifiers, number, = , +, - , *, if , else..
Mathematically:   {identifier, number, = , +, -, *, if , else}

Each one of the above are treated as a single symbol of the alphabet set. There are two levels: There is the scanning level, where we do the microsyntax and
we identify the words of the language. And then there is the parsing level. At the parsing level, these words become individual symbols in that language that
we will be trying to recognize using CONTEXT FREE GRAMMAR. 

============================
Parser calling the scanner
============================
The parser will call the scanner and the scanner will return a part of speech (ident, number, etc) and it will also return the actual lexeme. The parser will
call the GetNextWord() to get the next word and it's syntactic category. 

=============================
Two ways of viewing a scanner
==============================
In theory, it can work both ways (but in practice (i) is the followed one):
i) The scanner can get called one word at a time and then pass it to the scannner. It will return one word at a time.
ii) Scanner can also operate on the entire input and then just pass it as a list to the parser. 

=========================================
Treatment of floating point and integers
=========================================
When we talk about parsing, there will be little advantage in doing both integer and floating point numbers. So we will talking about integers. Because it 
will be the same concept from parsing point of view. From scanning point of view, it is different. 
All that the scanner does is that it identifies a character as a syntactic category and it passes the actual lexeme to the parser and then the parser can 
take care of it. 

=============================
What about number conversion
=============================
In principle, it doesn't belong to either scanning or parsing. But it can be done in either of those.


====================
What is a "Grammar"
====================
It is a set of rules for defining a language that recognizes or that generates a certain language. A formal language is just a set of strings. So grammar 
is a set of rules that generate all the strings that belong to a given language. If that (formal) language is REGULAR then we DO NOT need grammars because 
we can recognize that language using FA or regular expressions. Grammars are more general. And the special kind of grammars that generate Regular languages
are called "Regular Grammars". There is a subset of the CONTEXT FREE GRAMMAR  is the set of regular grammars that recognize regular languages.
Context Free Grammars are also a subset of context-sensitive grammars. Here we focus on those languages that are non regular that cannot be recognized by 
finite automatas or regular expressions. 
EXAMPLE of non regular language: 
                                 L = {a^n b^n | n>=0} 
 
================================
The "4" tuple notion of grammar
================================
G = {V, T, P, S) 
V: set of variables (non-terminals) 
T: set of terminals 
P: set of productions 
S: start variable 
 

==================
More on terminals
==================
Terminals can be viewed as set of strings that can be generated by a grammar. 

===================================================
Convention for writing terminals and non-terminals
===================================================
We write terminals in small letters and non-terminals in large letters. We can also underline terminals. 
 

===============
Derivation tree
=================
The string that gets derived will be present on the leaves of the derivation tree. The ABSTRACT SYNTAX TREE comes from the derivation tree. 

=======================================
Difference b/w AST and Derivation tree
=======================================
The difference between an AST and a parse tree is that in an AST we keep ONLY THE NODES THAT CORRESPOND TO ACTUAL OPERATIONS and we get rid of the nodes
that coresspond to grammatical symbols. You don't care about the symbols in the grammar. You care about nodes that correspond to operations like "add", 
or "multiply" etc that will be translated into actual code. Because we are building AST to generate the code. 


======================
Non ambiguous grammar
======================
If there is a single derivation tree for a given type of grammar then the grammar is said to be non-ambiguous. If there are multiple derivations then it is
ambiguous.

=============================================================
What makes a CONTEXT FREE AS OPPOSED TO CONTEXT SENSITIVE
=============================================================
In a context free grammar, all the variables that appear on the left hand side or whatever appears on the left hand side is an individual (or a single)
variable. If on the left hand side, you have a single variable and terminals then it will be CONTEXT SENSITIVE as well. We're not interested in 
context sensitive grammar in this course because programming languages can be defined in terms of context free grammars. 


============================================================
CFG as an acronym is for something else in compilers course
============================================================
It is for "control flow graph". 


==================================
Leftmost and rightmost derivation
==================================
Usually a string will have a leftmost derivation AND a rightmost derivation. Unless the grammar is very simple, there will always be a leftmost derivation
and a rightmost derivation. But THIS WILL NOTTT make the language ambiguous. A tree doesn't necessarily capture the notion of the distinguishment between
the leftmost and rightmost derivation. So for an umambiguous grammar, the parse tree will be the same for both leftmost and rightmst derivation.

================================
WHY DO WE CARE ABOUT DERIVATION?
================================
Because Parsing is all about finding a derivation for a string. So your program is a string that belongs to that programming language and the parser is 
trying to find a derivation for that. So given the rules and the productions that we will write to define the programming language. The Parser is that 
stage that looks at the rules and the program (that is a string of characters) and it tries to find a derivation for that string. So your program (even 
if it is a million line program) is one big string that should belong to that, that must belong to the language DEFINED BY THE CONTEXT FREE GRAMMAR 
OF THAT PROGRAMMING LANGUAGE

====================================
Constructing AST from Parse Tree
=====================================
After finding the derivation, the parser will construct a derivation tree for that language and then that derivation tree will become the abstract syntax
tree. 

=================================
Parsing as automated derivation
=================================
Parsing is automated derivation. It is a program that does the derivation. So there are specific algorithms for that. When we find derivation on a piece of 
paper, we are doing it by INSPECTION. The problem is that is not feasible to do a derivation through inspection for a program with potentially a million 
lines of code. so we need an algorithm to do this (to automate the derivation. Or to figure out the valid derivation for the string). The compiler will 
also need to figure out if no derivation is possible. If no derivation is possible then the compiler should report a SYNTAX ERROR> 
 

                            ==================================================================================================
                             Compilers Lecture 13: Parsing (2): Context-Free Grammars for Programming Language Constructs
                            ================================================================================================== 


============================================================
The alphabet set for typical programming language constructs
=============================================================
It will be the output of the SCANNER. It will be whatever the scanner recognizes.
======================
EXAMPLE ALPHABET SET: 
=====================
                      SIGMA = {ident, num, if, else, +, -, *...}

NOTE: The identifier, num, if, else will be considered as single symbols. But for "+", "-", "*" etc these will be considered as single symbols. So these will
      be treated as separate syntactic categories.

==================
Example 01
===================
Let us consider a line:
                        X12 = abc + def; 

Here: 
X12 = identifier 
abc = identifier
=   =  = syntactic category
+   = + syntactic category 
def = identfier 


From the above, we can write a rule like this: 
 
Assign -> ident = Expr;  <---- (i) 

Here non termminals are: Assign and Exprr since ident is a symbol at the parser level and is treated as terminal.

==============================================
Writing a basic rule for Expression using (i)
==============================================
Expr -> ident + ident


=======================================
But what is the problem with the above
=======================================
We can't do multiply or divide. So it is not generalized enough. Here is a more generalized  form of it: 

Expr -> ident OP ident 
OP   -> + | - | * | / 


========================================
Is there still a problem with the above
========================================
The above is still not general enough. It doesn't support more complex operations like more additions or subtractions. Let us try to write it in an even more
generalized format: 

                   Expr -> Expr OP ident | ident

We can use the above to parse something like: x = a + b - c


=======================================
But what is the problem with the above
=======================================
It does NOT preserve the precendence rules. So if we try to parse an expression like:  a + b * c , it will do the addition first and then it will do 
multiplication.  


============================================================
A grammar that captures the precendence rules while parsing
============================================================
This is not going to be hard. Let us consider the expression: x = a + b * c  <---- (v)
So to parse this, I have to introduce the notion of mathematical terms: 
==================
Mathematical terms
==================
From the above (v) we have the following components: 
Term --> b * c 
Term --> a 
factor --> b 
factor --> a 

The point is that the "+" or the "-" relates each term, you can have a number of factors. Now this is the key idea. The idea is that the term is a 
higher level construct that can have within it factors.
 
==============================================
Newly refined grammar with terms and factors
==============================================
Expr -> Expr + Term | Expr - Term | Term 
Term -> Term * factor | Term / Factor
Factor -> ident | num  

----
NOTE:
-----
The relationship between a Term and a Factor is the same as the relation between a Term and an Expression. Except that the operators are going to multiply
and divide.

============================================================================
We are looking at teh basic expressions that use the built in operations
============================================================================
We are not looking at function calls or exponents (which are also function calls). We are looking at the basic fundamental operations that can be performed 
on expressions.


=================================================================================
Incorporating paranthesis. Should they be accounted at the top or at the bottom?
==================================================================================
Paranthesis have HIGHER precedence and therefore they should be accounted for at the bottom. Higher precedence goes to the bottom because that is what you 
want to evaluate first. Lower precedence has a higher level in the hiearchy. The paranthesis structure is a factor. It is an alternative for the factor. 
So a factor can be an indivdual numnber, or a single identifier or paranthesis.

================================================
Revised grammar: THE CLASSIC EXPRESSION GRAMMAR
=================================================
Expr -> Expr + Term | Expr - Term | Term 
Term -> Term * Factor | Term / Factor | Factor 
Factor -> num | ident | (Expr) 


==================================================
We parsed by insepction the string "(a+b) * c - d
==================================================
WE looked at the grammar and we looked at the actual string that we wanted to parse and we tried to some matching. And we suceeded because the string was too small.
And we essentially did it by inspection. In our head, we tried a couple of possible solutions or substitutions. And when we are looking for the right
derivation, we were looking at the whole string. For example, how did we know that the first substitution that we make is "Exp - Term" and not "Exp + Term",
we knew this, because we looked at the whole string and we found a minus in between and then we said that (a+ b) * c will be one expression etc. And
how many symbols did we have to look ahead in order to figure out the right derivation? In our current example, we had to look ahead around 8 symbols
to figure out the right derivation. 

===========================================
Compilers can't afford a lot of lookaheads
============================================
And in-fact, compilers cannot afford looking six symbols ahead. In fact all the parsing algorithms that we will be learning will have just one symbol
look ahead and based on that they will decide. So in order to come up with such a parser, the grammar needs to be structured in a way that supports 
this. That supports deciding or figuring out the right substitution very early i.e by only one symbol lookahead.


=================================================
Systematically parsing the expression: a + b - c  //UNDERSTANDING THE PROBLEMS AND COMPLEXITIES WITH PARSING
=================================================
So if we are systematically doing this, then it will first go with expression. And then it is going to say that I will try the first substitution because
right now, the parser sees this a. So the parser will say: Expr + Term. So we just did 1 derivation. Then in the next step, if we are going to do the 
leftmost derivation then we should be looking for a substitution for expression. But in fact, if we are going to do this systematically then we are 
going to substitute "Expr + Term" again. And if we keep doing this systematically then it is going to do Expr + Term forever. The point is this that if 
you are doing leftmost derivation and if you have LEFT Recursion in your grammars then you will be substituting forever. So you will be in an infinite loop. 
SO LEFT recursion and leftmost derivation don't work well together. In this case, you will keep substituting forever. Lets say that we somehow find an 
escape from this and in the second derivation, we substitute Expr - Term for Expr.  But this will not lead to correct or right parsing. But this doesn't mean
that our input string is invalid because maybe one of our decisions, derivations or choices was wrong. So let us backtrack and try something else. The idea 
is that if you make a wrong decision at the top then everything based on that will be wrong. But in order to identify where the wrong substitution is, you 
will have to do a lot of backtracking and try out a lot of different possibilities.

==================================
The inefficieny with our approach
==================================
The point here is that if an actual parser does this, makes substitutions and then backtracks to the upper level and tries out different possibilites and keeps
doing it up until the very top will NOT LEAD TO VERY EFFICIENT PARSING. Because in a way, we have to traverse the entire tree. And the number of nodes in a 
tree is exponential function of the depth of the tree. 

================================================================
But is there an efficient algorithm for our particular grammmar?
=================================================================
Yes, there is a different approach to parsing that will allow us to figure out the correct substitution from the top based on ONE SYMBOL LOOKAHEAD. For this
particular grammar there is such an algorithm and this algorithm will be based on eliminating left recursion. If you eliminate left recursion, you can find 
a parsing algorithm that will not have to backtrack.

==========================
Left recursion elimination
===========================
It is the restructuring of our grammar such that we don't have left recursion, instead we have right recursion

=====================================
Example of Left Recursion Elimination
======================================
If I have a grammar with left recursion such as: 
   X -> Xa | b 
 
If I keep derivating using the above grammar, in the end, I  will have to replace a beta with the X. So what does this mean? It means that when you have 
left recursion, this will generate a Beta followed by as many alphas as you want. So this is what it generates. Beta followed by as many alphas as we want.
And this can be written in the following equivalent form: 

   X -> BX' 
   X'-> aX' | epsilon

The above is the right recursion equivalent form of the our grammar. It turns out that the kind of parsing that we are about to study, or that we will be 
studying in the next lecture, the right recursive form works very well and it will allow us to parse any string that belongs to this language by only one symbol
lookahead.



                            ============================================================= 
                             Compilers Lecture 14: Parsing (3): LL(1) Parsing, Part I
                            ============================================================== 

============================
Overview of today's lecture: Top Down Parsing
============================
Today we will talk about one parsing technique. It is called Top Down Parsing. More Specifically we will talk about LL(1) Parsing.  What Top-Down parsing 
means is that we start from the START VARIABLE. We start from the top and we work our way down until we get to the leaves.

=============
LL(1) Parsing
==============
The first L stands for scanning the input from LEFT TO RIGHT. And then SECOND L stands for LEFTMOST Derivation. And (1) here means 1 character lookahead.

==============================================
The problem with our previous form of grammar
==============================================
The first was infinite left recursion in leftmmost derivation parsing. The other problem is that even if we somehow solve this infinite loop problem, the
problem is then in identifying the right susbstitution when there are multiple rules or multiple productions that define that a certain non-terminal or 
variable. Parsing is about determining which substitution is the right substitution for the input that I am looking at. And for our previous form of grammar
we could not deterministically decide which substitution is the right substitution (which means that we will try one substitution and go with it until we 
hit a problem and realize that our substitution is right, and then we will go back and go through all the substitutions that we made and try to find or 
identify the substitution that was wrong). With this backtracking, parsing will not be efficient at all because the time will be exponential function of the 
depth or the height of the tree. 

=================
Predictive Parser
=================
We are looking for a BACKTRACK-FREE Parsing algorithm. Or predictive parser. A predictive parser is a parser that can determine based on a limited number 
of characters which substitution is the right substitution.

===============================
The technique that we talked of
===============================
Towards the end of last lecture we discussed that one technique that will work on this grammar is converting this grammar from the left recursion form to the 
right recursion. And in-fact, in today's lecture we will do this and we will understand when we convert our problematic grammar from the left recursive form
to the right recursive form, things will work out well. So this is something that people have figured out decades ago. People have found that the right 
recursive form of this grammar will work well with an LL(1) parser.


============================
Generalizing the conversion
============================
We can generalize this: 
                       X = X1a1 | X2a2|....|Xnan|b  <--- (i) 
 
we can write (i) as: 
                       X = bX'  <--------- (ii)
                       X' = a1X' | a2X' |...|anX'| epsilon


==================================================================================
Intuitively, why is the Right Recursive format going to work better with parsing?
==================================================================================
If we have a strign "baaaa" and our grammar is (ii), the reason why it is going to work better is because in (ii), it is exposing the beta (b) that must 
appear at the beginning of the string. This is making it explicitly exposed. It is obvious in this grammar that the string must start with a beta. But the
same case is not as obvious in left recursion grammar that the string that must be derived from this must start with a beta. The start is very important 
because what we are trying to do is to match an input string with a grammar given the lookahead. The first character or the first symbol in the string.


================================================================
Lets apply this general rule to the Classic Expression Grammar  
===============================================================

==================
Left Recursive form
====================
Expr -> Expr + Term | Expr - Term | Term 
Term -> Term * Factor | Term / Factor | Factor 
Factor -> num | ident | (Expr)                 


====================
Right Recursive form 
=====================
Matching it with our prototype: 
X = Expr 
alpha1 = +Term 
alpha2 = -Term 
Term   = b (Beta) 

Writing down the final form: 

Expr -> TermExpr' 
Expr'-> +TermExpr' | -TermExpr' | Epsilon
Term -> FactorTerm' 
Term' -> *FactorTerm' | /FactorTerm' | Epsilon 
Factor -> ident | num | (Expr)  //factor doesn't have left recursion, in-fact it has indirect recrusion.

//The point is that this form of the grammar looks odd but in fact it will lead to better parsing. The left recursive form is more readable though.

====================
No backtraking here
====================
We will not have to do any kind of backtracking here. It will always be obvious what the right substitution should be. 


==========================================
One minor issue with our parsing algorithm
==========================================
Let's say that we are trying to parse the string: X + 5 * Y 
Our parse tree will look something like this: 
                                                Expr 
                                        Term            Expr' 
                                 Factor     Term'   +term    Expr' 

Look at the +Term, now we know that it cannot be -Term for sure. But what about epsilon? The epsilon instead of +Term is not necessarily a bad option.
Because the "+" symbol could belong to something that comes right after an Expr'. In other words, I need to do an analysis that tells me what can come after
an Expr' in this grammar. And this is called a "FOLLOW SET". So under what conditions will epsilon be a valid substitution here? Well it will be a valid
substitution if the symbol "+" belongs to the FOLLOW SET OF Expr'.

======================
First and Follow sets
======================
The first(x) is the set of symbols/Terminals that can appear in the beginning of a string or as a first symbol in a string that is derived from x.
The follow(x) is the set of terminals that can come immediately after a string dervied from x. 

================================
COMPUTING First and Follow sets
================================
When computing the First set, it will be easier to compute the sets for the lower level terms or the lower level non-terminals or variables. So let us do
that: 

First(Factor) = {ident OR  num OR ( }
First(Term') =  {* OR / OR EPSILON} 
First(Term) = {ident OR num OR (}
First(Expr') = { +  OR - OR Epsilon) 
First (Expr) = {ident OR num OR ( } 
====================
Thought to consider
====================
NOW IS IT ALWAYS TRUE THAT IF I HAVE X -> YZ THEN THE FIRST OF X WILL BE THE FIRST OF Y? If Y is epsilon then I look at Z. If itself have an epsilon
in its first set then I should look at Z.  
Generally speaking, let's sya that X->A1,A2,A3....An. And let us say that A1 has an epsilon in its first set then I should look at the first set of 
A2. And if A2 has an epsilon in its first set then I should look at A3. Why? Because A1 and A2 may vanish and X may start with an A3.

============================
The follow of start variable
============================
The follow of start variable, IN GENERAL, is always EOF (End Of File).  Why? Because our whole string, in our current example, is an EXPRESSION. So 
Expression is our start variable, which is the biggest thing. So after root of tree, we have End of File. But in fact, if we look at the right hand 
side, we have other things that can appear in the follow set of our expression. 


====================
Computing Follow set
====================
Follow(Expr) = { eof OR ) }
Follow(Expr') = {eof OR )} //Because our whole string is an expression and our first production is saying that the Expr' may appear as the last variable
                              in the expression. So you should put everything that is in the follow sest of Expression in the follow set of Expression'.

Follow(Term) = {+ OR -} //EPISLON NEVER APPEARS IN THE FOLLOW SET. It doesn't make sense for the epsilon to appear in the follow set. Because the follow 
                       // set is about answering the question of what comes after it. And we can't say nothing i.e that nothing comes after it. If it is 
                      // the end of string then what comes after it is the end of file. So it doesn't make sense for the epsilon to appear in the follow set

Follow (Term') = {+ OR - OR eof)
Follow (Factor) = {* OR / OR + OR - OR eof OR ) } //in fact these are all the symbols 
===========================================
Taking epsilon into account in follow sets
============================================
Although we don't necessarily include the epsilon symbol in the follow sets, however we have to take it into account. If epsilon appears in one of the 
productions of the right hand side variable then our current variable that we are computing the follow set for may appear as the last string of the left 
hand side variable and thus we have to include the follow set of the left hand side variable into the follow set of the current variable being tested.
An example of this is: 

Follow(Term) = {+ OR - OR eof OR )}  //deduced from the production#1. 

Now just because epsilon was part of the production of Expr' therefore, Term could potentially be the last part of our expression and therefore we have to 
include the follow set of the Expr itno the follow set of Term.


========================
ONE LAST IMPORTANT THING
=========================
Expr' (Expression Prime) cannot be followed by a "-" or a "+", so this means that we can exclude the epsilon production for it from our grammar.


                            ======================================================================================
                             Compilers Lecture 15: Parsing (4): LL(1) Parsing, Part II: First and Follow Sets
                            ====================================================================================== 

=======================================================
Why do we need to compute the FOLLOW SET while parsing 
=======================================================


================================================
Our current situation and how parsing will lead
================================================
Right now, we are writing a grammar for which the start variable is an expression. It means that we are parsing a program that consists of a single expression.
It doesn't have multiple expresisons, assignments, if, else etc. Later on, we will be writing grammars for a real program. So our start symbol/variable will
become Program. The program is gonna be a list of statements. And each statement is one of many different kinds of statements(assignment statement, 
conditional statement, loop etc). And then, the assignment statement in particular will have an identifier, then the equal sign and then an expression. So
what we are discussing right now is part of a bigger picture for an actual programming language. At this point, we are only focusing on the expresisons. 
When we build a parser for a real program, the root of the parse tree is going to be a Program. Top down parsing is about starting at the root or at the 
top. 
 
=================================================
Having just one option at a given point in parser
=================================================
So, in this backtrack-free parser, we shouldn't have multiple options. We should only choose only one correct option (production) at a particular time. If
there are mulitple options then this parsing technique doesn't work on this particular grammar. So then the only options that we will have to fix the 
situation will be:
i) Find another parsing technique
ii) Restructure our grammar such that it doesn't have multiple options 

==========
Substituting an epsilon for Expr'   //Example string used: x + 5 * y
==========
By substituting an epsilon means that we are passing or skipping the matching. So it means that we are not doing any actual matching.  So we have an expr'
already ("x") and we are not matching the next symbol i.e plus until the next step in the derivation or in the parsing. So now deferrng the substitution or 
deferring the derivation will lead to a correct substitution only if this "+" can appear after an Expr'. Now the question is does an Expr' have in its 
follow set a "+" symbol??? So we look at its follow set after we compute it. Now it doesn't have a "+" in it. This means that deferring the matching until
a later step, we know for sure, will NOT lead to a correct derivation because this "+" doesn't belong to anything that may come after an Expr'. The only
valid option is "+TermExpr'". If we had a "+" in the follow set of Expr' then we would have had 2 valid alternatives. So Epsilon would have been a valid
alternative. Now if 2 valid alternatives were present then it means that this PARSING TECHNIQUE (LL(1)) FAILSS ON THIS GRAMMAR.

=================================
Thing to keep in mind about LL(1)
=================================
This LL(1) rule is not a parsing technique that will parse all the grammars in the world. It will only parse a subset of the grammars in the world. Sometimes
it is possible to restructure our grammar so that it is parsable by LL(1) and sometimes it is not parsable. 

=====================================
Technique of restructuring a grammar
=====================================
One technique of restructuring a grammar from non LL(1) parsable grammar to LL(1) parsable grammar is the the Left Recursion Elimination. SO by eliminating 
left recursion, you may be able parse grammar using LL(1), however this may not always be the case as Left Recursion Elimination may not be enough or 
sufficient.

================================================================
Some hint about Recursive Descrent Parsing Technique and LR(1)
================================================================
Recursive Descrent is based on LL(1). So if a Grammar is LL(1) parsable, you can build a recursive descrent parser for it. Any top-down parser can
be implemented as recursive descent. Anyting that is top-down can be implemented as recursive descent. And LL(1) is one example: the set of 
languages that are LL(1) parsable can be parsed using a recursive descrent parser. LR(1) is MORE POWERFUL THAN LL(1) but LL(1) is good for most
language constructs. 

===========================
Again, when does LL(1) fail
===========================
When we have multiple options or multiple alternatives. If we have no option at all then it means a "SYNTAX ERROR" (meaning that the string does not belong
to the language).

============================================================
What makes a Grammar LL(1) in terms of first and follow set
============================================================
For every production (and the general form of the production will be: X -> B1 | B2 |....Bn
Now if every production in the grammar has just one alternative, we know for sure that it is an LL(1) grammar (but obviously, this is not to going to be
the case in an interesting grammar: a grammar that has more than one alternative for every non-terminal). The very initial condition for LL(1) should be
that their FIRST SETs should be disjoint, but this is not sufficient. But if there is an alternative that has an epsilon in its FIRST set, then we should
look at its follow set beacuse having an epsilon means that we can skip or that we can deffer the matching and we can match later. Now will matching later
work or not will depend on the FOLLOW SETs. 

=================================
Why do we look at the FOLLOW SET
=================================
If one of the alternatives has an epsilon in it then we look at the Follow set. 

=====================
Define First+(X->Bi) 
=====================
First+(X->Bi) = {First(Bi) if Epsilon DOES NOT BELONG TO First(Bi),  
                 First(Bi) U Follow(X)  OTHERWISE}

NOW that we defined a convention, let us write down the mathematical definition for our LL(1) grammar: 
=================================
Mathematical definition for LL(1)
=================================
  "For every production : X -> B1 | B2 |...Bn 
   First+(X -> Bi) INTERSECTION First+(X -> Bj) = PHI (empty set) , for 1 <= i <= j <= n AND i != j "

//In the above, if the result is not PHI (empty set) then that means that there are multiple alternatives.


================
Example Grammar
================
S -> A|B
A -> b
B -> BACb|a
C -> AD  | Epsilon
D -> BC  | bC

Now this grammar is not LL(1) parsable. WHY? BECAUSE it has LEFT-Recursion in it. So as a first step, we must elminate Left recursion. The only production 
here that has left recursion in it is: B -> BACb | a , converting it into right recursive form gives us: 
                                                      B -> aB' 
                                                      B' -> ACbB' | Epsilon

================================
How would you compute First set
================================
Suppose we have a production: 
                              X -> A1A2A3.....An
ALGORITHM: 
                       put First(A1) - {Epsilon} into  First(X) //BUT this is not strictly true

//If we can substitute an epsilon for A1 then this means that we are not matching A1 with anything in the string. Then this means that the X may start with 
// A2. And if A2 has an epsilon then X may start with an A3. And soo on....
//So we have to keep checking until we get to an A that doesn't have an epsilon in it OR all of them have epsilons in their first sets. Now if all of them 
//have epsilon in their first sets then PUT EPSILON in the First(X).

===================================
Pseudocode for computing FIRST Set
===================================
1. First(X) = Phi (Empty Set)
2. for (i = 1 to n)                         //REPEAT OUTER LOOP UNTIL NO MORE CHANGES HAPPEN. THEN REPEAT THE INNER LOOP FOR ALL PRODUCTIONS.
3.    First(X) U = First(Ai) - {Epsilon} 
4.    if (Epsilon DOES NOT BELONG TO First(Ai) 
5.        break;
6. if (i == n + 1)              //if all the productions had epsilon in them 
7.    First(X) U = {Epsilon} 

//We have to do the above for every production. And in-fact we have to repeat the whole thing until no more changes are possible. And why repeat it? Because
//the first set of one variable may be dependent on the first set of another variable. And dependencies can be BIDIRECTIONAL in some cases. So we have to 
//keep repeating until no more changes are possible.

===================================
Pseudocode for computing Follow Set
===================================
X -> aYA1A2A3....An   <-- (i)    //Here we don't really care about what comes before Y in order to compute a follow set of a Y. 
                               //In order to compute the Follow set of variable, you have to find it somewhere on the right hand side of a production. 

for i = 1 to n 
    Follow(Y) U= First(A1) - {EPSILON} 
    if (Epsilon DOES NOT BELONG TO First(Ai) 
        break; 

if (i == n + 1) 
   Follow(Y) U= Follow(X) 

//And this whole thing above will be inside two loops. One loop that goes through all the productions and one loop that keeps repeating until no more changes
//happen.
==========================================
What to add to the follow of Y here in (i)
==========================================
We add the First(A1). If First(A1) has an EPSILON in it then I should add the First(A2) and so on...until I hit a varible that doesn't have an epsilon in its
First set. If all of the As have epsilon have EPSILON in them then in addition to adding the First sets of all of them to the Follow (Y), I also need to add
the FOLLOW(X). WHY the FOLLOW(X)? BECAUSE HAVING ALL THESE EPSILONS AFTER Y means that all of the As may vanish then Y can be the LAST SYMBOL IN AN X. And if
Y can be the last symbol in an X, we should LOGICALLY ADD EVERYTHING THAT IS IN THE FOLLOW SET OF X to the FOLLOW SET OF Y.

====================================================================================
In the very beginning, what is the SEED for Follow Computation OR Where do we start?
====================================================================================
Initially all of the FOLLOW sets are empty, except one set called the "Start variable". We look at the start variable and we add to its Follow set the 
EOF (end of file). And the logic behind it is that the large string (i.e abcd......d) and after it we have the EOF (end of file). S is the big variable, 
the top variable or the root of the tree. 

=================
First of Terminal
=================
The first of a Terminal is the Terminal itself.


============================================================================
When do we need a SECOND Round of computation of the First or the Follow Set
=============================================================================
We need to a SECOND round of computing the First or the Follow Sets if something above top depends on soomething below it. [Screenshot#2303 for reference]

===================
For the Follow Set
====================
For the Follow Set, we always start with the Start variable.

Follow (S) = {eof}                            //EOF may not be enough, so we scan for S on the right hand of each variable. However, since we don't find S on
                                             // the Right Hand Side of anything, we know that that should be the end of it.  
Follow (A) = {eof, b, a}
Follow (B) = {eof, b, FOLLOW(D) 
Follow (C) = {b, FOLLOW(D) 
Follow (D) = {b, FOLLOW(C)
Follow (B') =  {FOLLOW(B) (which is eof, b)
 
//AS YOU CAN SEE, we are DONE with the 1ST ITERATION. Here Follow(C) and Follow(D) have a Bidirectional relation. In the 2nd Iteration, we will see if
//Follow(D) has anything new then we add it to the Follow (B) because of its dependence. And then Follow (C) is fed by Follow (D) but Follow (D) does not have
//anything new. And this is it. In the 2ND iteration, there are dependencies but there isn't anything new. 

                            ============================================================
                            Compilers Lecture 16: Parsing (5): LL(1) Parsing, Part III
                            ============================================================

=================================================================
How can we, again, define LL(1) in terms of First and Follow Sets
=================================================================
If all the First sets don't have an epsilon then the intersection of First and Follow sets must be disjoint. If there is only one valid sustitution or 
expansion of the non-terminal that we are trying to expand. If there are multiple valid expansions then LL(1) will not work. We should also have this 
condition satisfied for every production of the form: 
                                                       A -> B1 | B2 |....|Bn

Then the first sets of the above should be disjoint. And if one of these productions have an epsilon in their First set then we should look at their Follow
sets.

===================
New Example Grammar
===================
S -> A | B 
A -> b
B -> aB' 
B'-> ACB' | Epsilon
C -> AD   | Epsilon 
D -> BC   | bC


====================
Computing First Sets
====================
First+(S->A) = {b} 
First+(S->B) = {a}     

Condition check: First+(S->A) INTERSECTION First(S->B) = PHI

First+(B' -> ACbB') = {b}  //because First(A) doesn't have an epsilon in it. If we have add epsilons in the entire string then we would have to look at the
                          //Follow (B') because B' could potentially be the last string in our string.

First(B'-> Epsilon) = Follow(B') = {eof b}

Now condition check: First+(B' -> ACbB') INTERSECTION WITH First+(B' -> Epsilon) = {b} != PHI  //SO WE CAN STOP HERE. This is enough to say that this grammar
                                                                                              //is not LL(1). 
===========================
Why is the above not LL(1)
===========================
There is an intersection between the Extended First set of B' -> aCbB' and the Extended First set of B' -> Epison. And thus the point is that if you find 
any intersection b/w any two alternatives then the grammar is not LL(1). Only one NON-EMPTY Intersection is enough to say that this grammar is not LL(1).

=============================================
But let us continue with the First sets still
=============================================
First+ (C -> AD) = {b} 
First+ (C -> Epsilon) =  Follow (C) = {b}

Intersection of the above two gives {b} which is again not LL(1) rule.

First+(D -> BC) = First(B) = {a} 
First+(bC) = First(b) = {b}           //The first of a terminal is the terminal itself. 

============================
Important point to remember
============================
Not all Context Free Grammars are LL(1) grammars.

================================================
The problem of determining if a Grammar is LL(1)
================================================
It is undecidable problem. Which means that you cannot come up with an algorithm that will take an arbitrary grammar and tell us if it can be transformed 
into LL(1) or not. Or that it takes a language and it tells us if that Language is LL(1) or not. So it is not always possible to even determine if it is 
LL(1) or not. Sometimes we suceed, sometimes we don't. But for many language constructs, we may transform the grammar into LL(1) grammar and we can 
successfully use the LL(1) parsing technique. 

=================================================================
Set relation between Context Free Languages and LL(1) languages
=================================================================
LL(1) is the subset of CFG. The set of languages that are parsable by LL(1) is the subset of all the languages that belong to context free languages. LR(1) 
is more general and it parses a larger set of languages. Now where are regular languages here? They are a subset of LL(1). Regular are easily parsable. You
don't even need context free grammars. You can recognize a regular language using a Finite State Automata. Context Free Grammars on the other hand are mostly
parsable by LL(1). A bigger set of context free grammars is parsable by LR(1).

===========================
New example for Follow Sets
===========================
X -> aYA1A2A3A4A5.....An

When we want to compute the Follow Set of Y, we look at A1. So if A1 has an Epsilon in it then we look at FIRST(A2). 


=============================
IMPORTANT DISTINCTION TO MAKE
=============================
When computing the Follow Set of Y, we look at A1. If A1 has an epsilon in it then we look at A2. If A2 has an epsilon in it then we look at A3. And so on. 
If A1....An all have epsilons then we look at the Folow (X) and add that to the Follow (Y).



====================
The first of Epsilon
====================
The first of epsilon is the epsilon.


==================================================================
The problem with computing the Follow Set for our above Grammar
==================================================================
The procedure that we described last time for computing the Follow sets is not efficient. Why is it not efficient? Where is it not efficient? If you don't 
have the complete Follow Set of X then you will have to go through another iteration to compute the follow set of X. Also think about A1, A2, A3, and A4. 
Ofcourse these can be terminals or non-terminals. But assuming that these can be non-terminals, I will need to compute the Follow Sets of A1, A2, A3 and A4.
In order to compute the follow set of Y, I will have to scan through A1, A2, A3...An because all of them will have epsilons. In the worst case, I will 
have to scan all of these. Then when I go to compute the follow set of A1, then I will scan A2, A3, A4..An. And then when I go to compute the follow set
of A3, I have to scan through A4, A5...An. So there is a lot of duplication here. In the book, there is a more efficient algorithm. It avoids scanning this
multiple times. It can the scan the list just ONCE by SCANNING BACKWARDS. The algorithm that we discussed last time is conceptually easier to understand,
but it is less efficient. What is more efficient is the algorithm in the book -- in one scan, it updates the Follow Sets of All non-terminals. So in this
example, if we have A1, A2 having epsilons but A3 not having an epsilon but then A4 and A5 having epsilons. In this case, what will be the Follow(A5)? It
will be Follow (x). What will be Follow (A4)? It will be First (A5) UNION Follow (X). And what is Follow(A3)? The Follow(A3) will be Follow(X) UNION 
FIRST(A5) UNION FIRST(A4). So what does this mean? It means that we keep accumulating. By the way, we have to subtract EPSILON from all of the previous 
Follow Sets. So the Algorithm in the book is based on scanning from the last symbol in the production. 

============
The Trailer
============
The trailer is initially the Follow(X) and you keep accumulating in that Trailer the First sets as you scan. But you stop accumulating when you get to
something that does not have an epsilon in it. At that point, we reset the Trailer to the First of the Variable that doesn't have an Epsilon in it. And then
you repeat the process from there.

=======================================================
Where does an Epsilon have to appear of a Non-Terminal
=======================================================
It has to appear in its First set. It doesn't make sense to have an epsilon in the Follow Set. So you cannot say that something is followed by nothing. It
has to be followed by something. And that something couldd be the EOF. So if it is not followed by any other symbol then it is followed by the EOF. It has
to be followed by a Terminal or an EOF. 

===========================================
Rewriting the "Classic Expression Grammar"
===========================================
Expr -> Expr + Term | Expr - Term | Term 
Term -> Term * Factor | Term / Factor | Factor 
Factor -> ident | num | (Expr)
//NOW WOULD LIKE TO ADD ARRAY ELEMENTS AND FUNCTION CALLS TO THIS.

================
As an example
===============
X = 2 * A[5] + 3; 

Here, the A[5] is a FACTOR!
Now look at :  3 * f(y,z,a) + 3;  //THIS, TOO, IS A FUNCTION CALL.


=========================================================================
Now let us integrate these constructs into our Classic Expression Grammar
=========================================================================
Expr -> Expr + Term | Expr - Term | Term 
Term -> Term * Factor | Term / Factor | Factor 
Factor -> ident | num | (Expr) | ident [Expr] | ident (ExprList) 
ExprList -> ExprList, Expr | Expr                                //We don't accept an Epsilon here i.e an empty list of expressions.
 
=========================================
Eliminating Left Recursion from ExprList
=========================================
Expr -> Expr + Term | Expr - Term | Term 
Term -> Term * Factor | Term / Factor | Factor 
Factor -> ident | num | (Expr) | ident [Expr] | ident (ExprList) 
ExprList -> ExprExprList' 
ExprList'-> ,ExprExprList' | Epsilon 


======================================
QUESTION: Is the above Grammar LL(1)?
======================================
No, it is not. Why not? Because if we look at Factor then it has three ALTERNATIVES FOR identifier. And thus if you take an intersection of any two of 
the threes then you'll get an identifier in that set.

================
Can this intersection be resolved?
================
Actually, yes, this is the kind of intersection that we can EASILY ELIMINATE by doing LEFT-FACTORING. Which is like Factoring in math. So the identifier
above is a common factor, so we can transform this factor into the following: 

Factor -> num | (Expr) | identFactor'               //Ident is the common factor, so we put it here and then it is followed by something and this something
                                                   // is followed by multiple alternatives and these alternatives do not have a common factor.
Factor' -> EPSILON | [EXPR] | (ExprList) 


==========================
Left Factoring GENERALITY:
==========================
IF: 
    X -> aB1 | aB2 |....|aBn |gamma1 | gamm2 |....gammaM     //THE PREFIX doesn't have to be a single symbol. 

WITH LEFT FACTORING, THE ABOVE GETS TRANSFORMED TO: 
    X -> aX' | gamma1 | gamma2 |....gammaM 
    X'-> B1  | B2 | Bn

==========================
Note about LEFT FACTORING
==========================
Left Factoring is another way of restructuring your grammar in hopes of making it LL(1) but it is not always guarnateed to work. 


=================================================
2 Tools that we studied for making Grammar LL(1)
==================================================
1. Left Recursion Elimination 
2. Left Factoring 


                            ============================================================
                            Compilers Lecture 17: Parsing (6): Recursive Descent Parsing
                            ============================================================

=========================================
What does a Recursive Descent Parser mean
==========================================
It means a Parser that implements Top-Down parsing. And it works fine when the Grammar is in LL(1) form.  And the idea is very simple and intuitive.
The idea is to write a procedure or a function for every Non-Terminal in the grammar and you just try to go through the derivation recursively in a 
top down manner.

================
Interesting Grammar
================
All interesting grammars are recursive. If a grammar doesn't have recursion in it then it is not interesting. If it doesn't have recursion then it will GENERATE
only a FINITE NUMBER OF STRINGS.

===================================================
A language that generates Finite number of strings
===================================================
It is called a regular language. Regular because you can write a regular expression which is just the OR'ing of these finite number of strings (even if it is 
a million line program).


==========
The idea
==========
The idea is that we start from the top. So we have a main function. And this main is going to look at the Expression. The start variable is the expression. So
it is going to call the function that recognizes the expression. 

=======
Pseudocode
=====

Main () {
word = GetNextWord();
res  = Expr()    //the function that parses the expression 
if (res == true && word == eof)
     ReportSuccess() 
else 
     ReportFail()      

}

Expr() { 
res = Term()
if (res ==false) 
   return false;

return ExprPrime() //we return whatever ExpressionPrime() returns
}

ExpressionPrime() {    //This will suceed 
   if (word == '+' OR word == '-') 
      word = GetNetWord() 
      res = Term() 
      if (res == false)
          return false
      return ExprPr()  
   if word == eof || word == ')'       //if an epsilon then we are not consuming a character from the input so we don't call getNextWord()
           return true;         
}

Term () { 
res = Factor(); 

if (res == false) return false; 
else return TermPrime(); 
}

TermPrime () { 
 if  (word == '*' || word == '/')
      word = GetNextWord() 
      res = Factor() 
      if (res == false) return false;
      else return TermPrime()

if (word == '+' || word == '-' || word == 'eof' || word == ')' )   //FOLLOW SET OF TermPrime 
   return true;
else 
   return false; 

}
 

Factor () { 
if (word == '(' ) 
    word = GetNextWord()    //we want to consume the next input 
    res = Expr() 
    if (res == false) return false;
    word = GetNext()
    if (word == ')' )  {
       word = GetNextWord();   //Consuming the actual word
       return true;           //Now at this point we have recognized an expression that consists of a closing paranthesis. 
     }
    else 
       return false; 

if (word == id || word == num) 
     word = GetNextWord()    //consume the word
     return true;
} 

=========================================
What does "Consuming the word" means here
==========================================
It means that we are advancing the pointer to the next character. Initially the pointer (according to my observation) will be before the first character in
the string (i.e at "null"). Also we DO NOT CONSUME a word when we are making an epsilon substitution. In-fact not consuming anything is equivalent to 
epsilon substitution.

================================================================================
What would be the right place to print the error message for the syntax error?
=================================================================================
If the rule was violated inside a factor then the right place to print this error message would be inside the Factor() method. For example : "Unexpected 
symbol '*' sign". So this is how you get syntax errors when you compile. It is not the expected symbol. The symbol is the word or the lexeme that we are 
seeing, so throughout the parsing process, we will have access to the actual lexeme (as it should be already obvious).

=============================================================
Exercise01: Draw the stack frame of the parsing of "X + 5 * Y"  //this should report success
=============================================================
=============================================================
Exercise02: Draw the stack frame of the parsing of "X +*y"  //this should report fail
=============================================================



==================================================================
Everything has to parse in order to parse the entire input string
=================================================================
So if we pass a string to the function that parses the term from a function that pareses an expression then that function (the one that parses the term) has 
to return True.


===============================
Parsing a more general program
===============================
Program -> main () {stList}       //this describes a program that consits of main and an arbitrary list of statements.
stList  ->  st Stlist | epsilon   //we are allowing a program that can have an empty body. 
st -> Assign | Cond | Loop 
Assign -> iden = Expr;
Expr -> ....//already defined above
Cond -> if(BoolExpr) {stList} else {stList} | if (BoolExpr) {stList} | if (BoolExpr) st;  //this last production allows a single statement without braces
BoolExpr -> Expr CompOP Expr
CompOP -> > | < | <= | >= | == | .....
Loop -> ..... //TO BE DONE LATER

===================================================================
Exercise01: Draw the stack frame of the parsing of "X + 5 * Y"
===================================================================


                            ============================================================
                             Compilers Lecture 18: Parsing (7): LR(1) Parsing, Part I
                            ============================================================

================
Expansions
===============
In top down parsing, we start from the root and then we keep making substitutions. But in-fact, in Top-Down, we call these substitutions "expansions" because
they increase the number of symbols. So typically, you substitute S->ABC , so most of the substitutions that you wil be making in a tree will be expansions. 
Until you get to the leaves. The leaves are the terminals. So this is top-down. 

=================================
Bottom Up Parsing and "REDUCTIONS"
==================================
In this, you start with the Terminal or you start with the string and you work your way up the tree until you get to the root of the tree. WHen we are doing
Bottom up parsing, our substitutions are going to be called "REDUCTIONS" because here we reduce. We start with a large number of symbols and then we keep
making substitutions that will reduce the number of symbols until we get to the start symbol.

=========================================
Bottom Up vs Top Down and Real Life Examples
===========================================
In general, Bottom up parsing is more powerful than Top-Down parsing. Why? Because it will have access to both the "LEFT CONTEXT" and the "RIGHT CONTEXT".
Thinking Bottom Up has been used to come up with more advanced techniques. Because Scientific Methodology, you can think of it as a combination of 
"Top Down" or "Bottom Up". Before the scientific methodology was introduced, many people did NOT believe in experiments. When you think about conducting 
experiments and then looking at the results of an experiment then you are looking at concrete data. When you making generalization based on observations or 
the results of an experiment, you are going up or BOTTOM UP. You are going from the "CONCRETE TO THE ABSTRACT". This is exactly what you do in bottom up. 
You have the concrete (WHICH IS THE LEAVES THAT HAVE THE ACTUAL STRING) and you go to the Abstract (which is the Start variable of the derivation). So we
go from the concrete to the abstract. Democracy is another example. When the people elect the President then this BOTTOM UP. Before the introduction of 
democracy, everything was TOP-DOWN. When people started introducing democratic systems, the people (the leaves of the tree) select the president and it is 
directly or indirectly (whether you select delegates that select the president) you are going BOTTOM UP.

===========================================
Which Algorithmic Technique uses Bottom Up
===========================================
It is DYNAMIC PROGRAMMING. We build solutions to the subproblems and we use solutions to the subproblems to build solutions to bigger subproblems and then 
keep doing it until you build solution to the actual problem. 

=============================================================
The algorithmic technique that does the opposite of bottom up
==============================================================
Divide and Conquer. It starts with a big problem and it divides a big problem into smaller problems. Here you are doing top-down. 

====================
Example01
====================
S -> aABe
A -> Abc | b 
B -> B -> d

//This above grammar is not LL(1) because it has a Left-Recursion on A.
//Let us see how we can parse bottom up on this for the String: abbcde

===================
Parsing bottom up
===================
Parsing bottom up means that you start with a string and then you try to find a rule in the grammar that you can use for making a reduction. You do the 
substitution. You substitute the RIGHT HAND SIDE for the left hand side. So for our example string: 
We first look at 'a'. We will read the 'a'. So now after doing that, can we find the 'a' somewhere on the right hand at the END of the RIGHT HAND SIDE OF 
a RULE?  So 'a' never appears at the end of a rule. So we know that we cannot make a reduction with an 'a'. Because there is no rule that ends with an 'a'.
So when we can't make a reduction, we shift. So this bottom up parsing is based on "Shifting" or "Reducing". SO we look for a reduction, and if we cannot
find a reduction, we shift. And also, shifting here means moving the pointer in the input. 
So now, we see a 'b'. Now, we check to see if there is a rule in the grammar that ends with a 'b'? And we do find a one that ends with a 'b', which is 
A -> b. Now we also check to see if there is a rule that ends with the suffix 'ab'? No, there is no such rule. So this is the only option that we have, which
is reducing the 'b' into A. So our String now is:
                                                   aAbcde
And now we look at the third symbol i.e 'b'. Now to the left of our pointer are the characters that we have read.   Now we look at the rules and check to see
whether there is a one that ends with an 'A'? No, there is not, so there is no opportunity for reduction and thus we shift i.e read one more symbol. Now is
there a rules that ends with a 'b'? YES, ther is a rule. But now...is 'b' a good reduction to make? Is it a good idea to reduce a 'b' to an 'A'? Under what 
conditions will this be a good reduction? Given the lookahead and given what we have.  Let us just try to do this: aAAc 
Now the question is, does 'c' ever appear in the Follow Set of A. Now the Follow of A is: Follow(A) = {b d}. Since the lookahead i.e 'c' is not in the follow
set of A, then this reduction will NOT be a valid reduction at this point. So we made this decision, based on both the lookahead which is what we have to 
the right of the pointer and the Left Context as well. 
So now we shift and now we have in the left context: 'Abc' and now we look for a rule in which the RHS ends with a 'c'. And the only rule that does that is:
A -> Abc. And in this rule, we will be reducing this whole thing to an 'A'. So now we have a the string as:  
                                                                                                              aAde
Now, we have 'd' on the right hand side of the pointer and on the l.h.s we have our Left-context.
Again, we look at what we have at the L.H.S of the pointer, and this is stored on the stack already. The LR(1) parsing will be placed in a stack and it shows
us what we have consumed. And on the R.H.S, it consists of stuff that we have not consumed yet.  So currently our stack looks like this: 

A
a 

Now, we don't have anything that ends with an 'A', which means that we cannot reduce it down to anything that ends with an 'A'. You can make a sequence of 
reductions btw. Anyways, we now shift. And then we check to see if we have anything that ends with a 'd' and yes we do. So we substitute. And our string 
becomes:
         aABe
Now check to see if any rule ends with a 'B'. Since there is not a single one then we shift. And then we check to see if there is a rule that ends with an
'e'. And actually we do, so we reduce the entire String with an 'S'.

So this is Top-Down parsing. So we started from the leaves and we worked our way up top. 

================================================
Applying this to the Classic Expression Grammar
================================================
S -> Expr 
Expr -> Expr + Term | Expr - Term | Term 
Term -> Term * factor | Term / factor | factor
factor -> num | iden | (Expr) 

//Since our Expr has multiple alternatives, we have introduced a start variable S that doesn't have multiple alternatives. And let us list all of our rules:

0. Goal -> Expr 
1. Expr -> Expr + Term 
2.          | Expr - Term 
3.          | Term 
4. Term -> Term * Factor 
5.      | Term / Factor 
6.      | Factor 
7. Factor -> ident 
8.      | num   
9.      | (Expr)

==========
Dry Run
==========
Stack                           Input                           Action         
-----                           ------                           --------
empty                        id + num * id                      Shift      //consume word and push it onto stack
id                           +num * id                          reduce by 8           //is there a rule that ends with an id? yes, we also check to see if + belongs to the follow set of factor. And actually '+' does, so this is a valid choice by rules #8. So we reduce by 8. 
Factor                       +num * id                          reduce by 6                //Is there a rule that ends with a factor? Actually yes, we have 3. But in our left context, we have no Term / Factor or Term * Factor, so therefore we are only left with a single option.
Term                         +num * id                          reduce by 3  //is there a rule that ends with a Term? Yes, and '+' is a valid follower of Expr
Expr                         +num * id                          shift           //A goal cannot  be followed by a plus, so therefore, Goal is NOT a valid substitor. 
Expr+                        num * id                           shift           //nothing ends with a plus, I cannot reduce, so I shift
Expr+num                     *id                                reduce          //by rule#7
Expr+Factor                  *id                                reduce         //by rule#6
Expr+Term                    *id                                shift      //does the multiply as a follow for expression? No, it has a follow of EOF or ')' . 
Expr+Term*                    id                                shift 
Expr+Term*id                  eof                               reduce    //by rule#8
Expr+Term*Factor              eof                               reduce   //here we have 2 possible substitutions, rule#4 and rule#6. We have to define how to systematically handle this scenario. 
Expr+Term                     eof                               reduce by rule#1         //again we have 2 possible substitutions. We will define a systematic algorithm later on 
Expr                          eof                               reduce by rule#0 
Goal 


================================================================================================================
Systematic approach to determine whether to shift or to reduce and in case of reduce which rule to use to reduce
================================================================================================================
We do this systematically by keeping a track as we are processing the symbols in the input String. It keeps a track of where we are in each rule. We will have a 
placeholder. At the beginning before we start parsing, our placeholder will be before Expr in rule#0. Because the first rule will have an expression then we
are also in rules #1, #2, #3. This is known as an "IMPLICATION". But then it applies that we are at a rule that defines a Term, so we are in rules #4,5,6 and
similarly in #7,8,9. These placeholders track our progress -- where we are in each rule. So basically for each rule in the grammar, there is a placeholder that
marks where we are. Given what we have parsed and given the lookahead.


======================
Another simple example
======================
Goal -> A 
A    -> Aa | a 
      

                            ============================================================
                             Compilers Lecture 19: Parsing (8): LR(1) Parsing, Part II
                            ============================================================

===========================================================================
What are the two Major decisons that we have to make in a Bottom up Parser
===========================================================================
Whether to shift or to reduce. 

====================================================
What does shifting means and What about Reduction? 
=====================================================
Get the next symbol from the input and put it on  the stack. So we have to systematically decide whether to shift or to reduce. But suppose that we decide to 
reduce? Is that it? No. So it is not sufficient to decide that we need to reduce. What should we specifically decide? We need to decide reduce by WHAT 
production. Because reduction can be done by multiple productions. And we should decide which production to use. 

=====================
When does LR(1) Fail?
======================
So if at some point, if the parser doesn't know whether to shift or to reduce. If both options are possible then LR(1) parsing fails on that grammar. 
Also if you try to reduce but there are multiple productions by which you can reduce and you haven't' made a clear decision then again LR(1) parsing 
fails.

============================================
Options for possibly resolving the conflict
=============================================
1. Try to restructure the grammar
2. Find other parsing algorithm


=========================================================
Can LR(1) be used to parse all Context Free languages?
========================================================
ABSOLUTELY NOT!

===============================
We will work through an example
===============================
WE will take an example now and it is probably the simplest non-trivial example that will illustrate this technique and we will see how LR(1) constructs the 
tables that will tell it whether to shift or reduce and reduce by which FLOW. Remember that when we are doing LR(1) parsing, you have your pointer somewhere
inside the String. To the left side of this pointer, you have the symbols that you have already read and to the right you have symbols that you have not 
yet read. And what you have read will be on the Stack but it will NOT NECESSARILY BE IN THE FORM OF TERMINALS. What is on the stack in LR(1) parsing? It is 
not necessarily Terminals. It may be non-terminals because we have made some reductions. Stack has a mix of terminals and non-terminals. But what we have 
NOT PROCESSED yet is a bunch of terminals (all the ones to the right). SO we have a prefix part and a suffix part. The prefix part is the left part that 
is on the stack and the suffix part is the one to the right that we have not read yet. Every reduction that we make will INTRODUCE A NON-TERMINAL.


===============
Example grammar
================
We track the progress that we are making on all productions in the grammar. Let us consider the grammar: 
0. Goal -> A
1. A -> Aa
2. A -> a 

So our start state i.e state0 is going to be "Goal" (this is going to be the first item in every initial state). And we are in the beginning of A then our
lookahead is going to be what? So in order to make this reduction from A to Goal, what should the next symbol be? EOF because this reduction is something 
that you will make ONLY once. So our lookahead will be end-of-file. So, this is our format for what we call LR(1) item.

======================
What is an LR(1) item?
======================
It looks like this: 

State: S0, Production: Goal -> . A, eof 

So, an LR(1) item is more than a production. It is a production of two things that we add, the first thing is the placeholder or the placemaker that we 
add (we place it before A). So this means that we're right at the beginning of this. And this is the lookahead i.e eof. Now when we are at the beginning 
of A, we have to look at the lower level rules to see where we can possibly be in those rules. So this first level in the state means that we are expecting
an A. And we haven't matched anything yet and we are expecting an A. If we can recognize an A then we can make a reduction and then we are done. But we 
should look at the rules that define an A. So in the next few levels we have: 

A -> .Aa, eof    //the lookahead should be the same because if we recognize an Aa then A is what should appear after an A when make this reduction and what shoul appear after an A should be an EOF()
A -> .a , eof    
//SO now,are we done? No we are not. This procedure, which we call the "CLOSURE" OF THIS STATE...we should continue doing this as long as there are non terminals with a placeholder in front it. So above, you can see that there is a placeholder in front of A, so in this case, again we should look at all definitions of A.
A -> .Aa , a   //But now, our lookahed is "A", not an end of file. It means that we are expecting an A followed by a, and this "a" is going to come from the definitions.
A -> .a , a 
//So why are we doing this? We are doing this because what come after an A may be an EOF, or it could be "a" depending on the level at which we are making reductions. 
//So this reduction (level 0) can be made at different levels and at each level, there can be a follow i.e a different symbol that follows it. So the eof lookahead must belong to the follow set of A -> .Aa or this non terminal that we are deriving. 

===========
Example
===========
Let us derive "aaa" 

            Goal 
             A
          A     a 
       A     a
       a
====================
What does this mean?
=====================
It means that this "A a" can be derived at multiple levels but what comes after A is different. So at the 2nd last level we have an "A a"
but what comes after it is going to be different. While the 3rd last level the "A a" is followed by an "eof", so there are multiple possible followers or 
multiple possible lookaheads for a single production. So let us ask ourselves when would it be possible for: A -> .a, eof?  If you just have a single A then
it would be followed by an eof. 

====================
Closure with levels
===================
           S0

level 0 -- Goal -> .A , eof 
level 1 -- A -> .Aa , eof       <--- (i)
level 1 -- A -> .a  , eof
level 2 -- A -> .Aa , a 
level 2 -- A -> .a , a

This closure tries to find al the possibilities.
Also this is our start state. Now, how can we make progress? Well, we can make progress if we see a small "a" or a big "A". If we can recognize a big A then 
we will make the following progress. 

        A    (And let us call this State S1).
//Now we will look at those productions in which the placeholder appears before an A

Goal -> Aa, eof 
A -> A.a , eof  //the point is that if we recognize an "A" in (i) then we are making progress and we will get to this point
A -> A.a , a   


===============
What to do now?
===============
Now, we should find the closure for the above. And for that we look for the placeholder and we see if the placeholder is followed by a non-terminal. DO we?
NOPE! We don't have the placeholder followed by a non-terminal. If we had a placeholder followed by a non-terminal, like A -> A.Xa then we should continue 
and we should look for the rules defining "X". Then we should put all the rules defining "X" and there could be multiple rules defining "X". Now what would
be the follow of this "X"? it will be an a. So we will list all the rules for an X followed by an A like this: 

X -> terminals&/orNon-terminals, a
X -> terminals&/orNon-terminals, a
X -> terminals&/orNon-terminals, a

Now what if we had something like this: A -> A.XY , a ? Now in this case, to continue, we should find all the rules that define "X" followed by First(Y). 

==========
Example
===========
Find the closure for A -> A.XY , a 

where First(Y) = {b, epsilon}

and 
X -> alpha
x -> beta

Then the closure of X will be: 
   X -> alpha, b 
   X -> alpha, a //BECAUSE Y may be epsilon and then it may be replaced with Follow(X).
   X -> beta, b
   X -> beta, a

//So you should cover all the rules for defining an X and then for each rule, we should cover all the possible followers of X.

===============================
Again, what are we trying to do
================================
We are trying to answer the question: What should the lookahead be in order for a REDUCTION to be VALID. 

=====================================
General rule for finding a lookahead
=====================================
If you have a rule like: X -> Beta.CGamma, a 

then in this case the closure for this will be: All the rules that define C. And for each one of these rules, you should have the lookaheads of First(Gammaa)

First (Gammaa) = First(Gamma) if EPSILON DOES NOT BELONG TO First(Gamma) 
OTHERWISE 
First (Gamma) = FIRST(GAMMA) - {Epsilon} UNION {a} 


===============================
Continuing our example from S0 (And let us call this State S2).
===============================
Now, if we get a small "a" then what should we have? 
With a small "a", we should look at the other rules. The rules that have a placeholder preceeding an 'a'. 

A -> a., eof   <--- (ii) 
A -> a. , a    <--- (iii)
 
If we reach S2 and our lookahead is either end-of-file or A then this means that we have recognized enough to make this reduciton. So when the placeholder
reaches the end of the production, it means that we have recognized enough to make up a reduction. 

Reduce (ii) by A -> a., eof Similarly reduce (iii) by A -> a., a 

==========================
So what does this tell us?
===========================
It tells us that the reduction A -> a is valid if the lookahead is either "eof" or "a". 

===================================
Now looking for a reduction from S1
===================================
Reduction means that a placeholder appears at the end of a production. And actually there is a reduction.  "Goal -> A., eof" can be reduced by "Goal -> A".

===============
The leaf state 
================
When a placeholder reaches the end of a production then that is called the "leaf state". Nothing can come after this because the placeholder cannot go any
further. It cannot go beyond this point. It has just reached the end. 


========================
State S3 (for small 'a')
========================
A -> Aa., eof <-- (v) 
A -> Aa., a   <-- (vi)

Now we have two reductions 

WE can reduce (v) by Goal -> A. And reduce (vi) by A -> Aa

========
Tracking our progress
=======
Generally speaking that when you have: 
 X -> Alpha.B, a
What does this mean? It means that what we have recognized so far i.e we have recognized an alpha. So alpha is on the stack. Beta is something that we expect 
if what we have to read next ends up being a beta then we are ready to make a reduction. So it's in the next state, we get X -> AlphaBeta., x 
Now our stack is going to have alpha and the beta in the stack. With the beta being the top of the stack.

==========================================
"dot" coressponds to the top of the stack
==========================================
The symbol to the left of our dot is the top of the stack.

===========
BTW: Using greek symbols
===========
When we use Greek symbols in our examples, they can in-fact be non-terminals and terminals.

===================================================================
Boundary between the stack and what we have to read from the input
===================================================================
This dot (.) placeholder marks the boundary between the stack and what we have to read from the input. To the left are the symbols in the stack.


================================================================
Using all of the above to construct Action Table and GoTo Table
================================================================

==============
Goto Table
==============
The Goto table is going to have the states (and we have four of them right now). So the rows are going to be states and the columns are going to be 
non-terminals. And in this grammar, the only non-terminal is an A.

========  =====
States     A (non-terminal) //here we list the states that have an outgoing edge to this non-terminal.
========  =====
S0         S1
S1         _
S2         _
S3         _

 //From the above, out of S1, S2 and S3 there is no transition on a non-terminal.
==============
Action Table
=============
The logic is that I have these states (s0, s1, s2, s3) and I need to make an action based on the lookahead. So the columns in the action table are going to
be terminals or end of files. So for example, if I am in S0 and I get an "a" then I am not yet ready to make a reduction, so I shift and goto S2.
REMEMBER: The "S" in the action table stands for "Shift".

=========   ===== ======== 
States       a     eof
=========   ===== =======
S0           S2
S1           S3     Accept by rule 0 
S2    reduce by r2  reduce by rule 2 
S3    reduce by r1  reduce by rule 1

===================
Shift-Reduce ERROR
===================
If there had been a reduction in our state S0 for a lookahead of "a" (that would mean that the placemaker or the placeholder had reched the end of some 
production that had the lookahead of "a") but at the same time we could transition to S2 for "a" then that would be called a shift reduce error because at
that point there would be two valid possibilities. 
 
==========
One valid action
==========
At every step, there should be one, unique valid action, either a shift or a reduce. And if it is a reduce, it has to be ONLY ONE reduce. I cannot have 
multiple reductions. 

====================
Reduce-Reduce ERROR
====================
When there are two valid reductions then LR(1) parsing fails. The grammar is then non-LR(1). The Reduce-Reduce error occurs when the placemaker or the 
placeholder reaches the end of multiple productions and these multiple productions have the same lookahead symbol.Example:

A -> Aa. , a 
A -> a.  , a   

The above is a REDUCE-REDUCE ERROR.

===============================
Example: Let us now Parse: "aa"
===============================
Initially we are in State0 (i.e S0), so we push that onto the Stack. Now, we go to the Action table. The action table tells us what to do. And since we 
managed to construct the action table and there is always one unique action to do, it means we always know what to do (i.e whether to shift or to reduce. And
if reduce then by which rule). Now since, I am in s0 and my lookahead is "a" then the action table tells me to Shift to S2 (i.e State#2). Now our stack has 
{S2, a, S0}. Now we are in State#2 and our lookahead is an "a" then we reduce by Rule#2 and resultantly, we will the following from the stack (S2 and a). Now 
our Stack has {S0}.Now this is a tricky part. Now since we have popped those off the stack, now our GOTO is going to depend on what remains after the popping.
So we check GoTo[S0][A]  where "A" is the non-terminal that we got after reducing by rule#2 the last time. 
So we always consult the GOTO Table when we make a reduction (and after popping off the stack). Now the GOTO of [S0][A] is S1. And as a result, we go to 
S1 and push both "A" and "S1" onto the Stack. 
Now our lookahead is still "a" and our current state is "S1", so we consult the action table and consequently we shift to S3 i.e we will push "a" onto the 
stack and we will push "S3" onto the stack. Now our lookahead is an eof, so we check ActionTable[S3][eof] and it tells us to reduce by rule#2, which we do
and consequently, we pop the last 4 items off the stack and then push A onto the stack. Then we consult Goto[S0][A] and we get S1, so we shift to S1 and 
push S1 onto the stack as well. Now our lookahead is eof, so we check ActionTable [S1][eof] then we get Accept, so consequently,  pop the last 2 items of the 
stack and push GOAL onto the stack, and since Goal is our IMPLICIT ACCEPT STATE, therefore, we accept.

==================
The Trickiest Part
==================
The trickiest part is constructing the Closure for our productions. So we look at a production and we list out all the productions of a non-terminal that appears
immediately after a placemaker or a placeholder and then we repeat the process for the First of that non-terminal.

================================
When to consult the GoTo Table
================================
This table is used after a reduction. It tells me which state to go to after a reduction. Why are the columns in a GoTo table non-terminals? Because you look
up the Non-Terminal you REDUCED TO.  Whenever we reduce, we reduce TO A NON-TERMINAL. And what state do you use when consulting to a GoTo table? You use the 
state that you get after popping off.

============================================================================
How many enteries or items will you be popping off the stack when making a reduction?
=============================================================================
It is the number of symbols in a reduction MULTIPLIED BY 2.  Why multiply by 2? Because we have to include the state themselves. 

===========================
Complicatedness of LR(1)
===========================
So LR(1) parsing involves alot of book keeping, so that is why in practice is automated. So there are tools that do this for us. This was the SIMPLEST 
EXAMPLE for LR(1) parsing and it took an entire lecture. The most trivial grammar.


                            ============================================================
                             Compilers Lecture 20: Parsing (9): LR(1) Parsing, Part III
                            ============================================================

=================
New Example
=================
Consider the following grammar: 
0. Goal -> X 
1. X -> Xab 
2. X -> ab 

//NOTE: doing LR(1) parsing for this grammar is going to be even more complicated than before because it will involve a lot of book keeping. 

====================
Canonical Collection
====================
These are the different states. And each state has a number of different LR(1) elements.
 
==============
LR(1) Element
==============
It has a rule (production), lookahead and a placeholder.
Example: X -> alpha.Beta , x   <--- (i) 

What does (i) mean? It means that what we have parsed so far is consistent with alpha. So we have parsed an alpha. So this alpha may be thousand terminals 
and non-terminals. And we expect a Beta. Remember that each state can have possibly multiple LR(1) elements in it. So each LR(1) element is a possibility. 

============================
Importance of LR(1) Element
============================
They track the state of different productions. So basically LR(1) parsing is about tracking the states of different productions and then in a given state, 
by looking at the state and the lookahead, we decide whether we are ready to make a reduction or we should shift. If both shift and reduction are possible
then it means that the GRAMMAR in its current structure or in its current format is NOT LR(1). We may be able to restructure it into LR(1) grammmar but 
we also may not be able to do that because the language may not belong to the LR(1) grammar. If there are multiple possibilities then this particular 
parsing technique fails, but if there are no valid possibilities then it means a SYNTAX ERROR.

===========================
Presence of a syntax ERROR
===========================
If there are multiple possibilities then this particular parsing technique fails, but if there are no valid possibilities then it means a SYNTAX ERROR. It 
means that there is nothing wrong with our grammar or our parsing technique. The input string has a syntax error, so the input string does not belong to 
this language. 

=================================
Parsing our above example grammar
==================================

LR(1) Items State S0:  //Below are all the possibilites in the beginning.


Goal -> .X , eof   //this was easy. This is level 0 
X    -> .Xab, eof  //this is level 1
X    -> .ab, eof   //this is also level 1
X    -> .Xab, a    //this is level 2. Also this lookahead came from production X -> .ab, eof
X    -> ab  , a   //this is level 2 
X    -> .Xab , a  //this is level 3 
.
.
....//there are infinite number of levels here because level is going to keep repeating itself for a lookahead of a. We stop at level 2, because
//after that, there are not any new productions
 
//Also remember that there can be multiple possible lookaheads for a single symbol. 
//Now we have a placehlder preceeding a non-terminal and whenever we have a placeholder preceeding a non-terminal, you have to keep going. 


LR(1) Items State S1: //This is if we transition to X. This is for Goto[S0][X] , which gives us S1 

Goal -> X. , eof  //reduce by Rule#0 
X -> X.ab , eof 
X -> X.ab, a 

//Q1: Should we continue computing the closure here? Nope, because there is no placeholder that preceedes a non-terminal here.
//Q2: Do we have any reductions in this state? YES! Goal, beacuse the placeholder has reached an end for that. 

LR(1) Items State S2: //This is if we transition to a. This is for Goto[S0][a] , which gives us S2
X -> a.b, eof 
X -> a.b, a 


LR(1) Items State S3: //This is if we transition to a. This is for Goto[S1][a] , which gives us S3
X -> Xa.b, eof 
X -> Xa.b, a 
 

LR(1) Items State S4: //This is if we transition to a. This is for Goto[S2][b] , which gives us S4
X -> ab., eof //Reduction by rule#2
X -> ab., a  //Reduction by rule#2

LR(1) Items State S5: //This is if we transition to a. This is for Goto[S3][b] , which gives us S5
X -> Xab. , eof //reduce by rule#1 
X -> Xab. , a  //reduce by rule#1

===================================
Transition graph for these states
===================================
We can draw transition graph for these states like these: 
       S0
    x     a
  S1        S2 
  a         b
  S3        S4
  b
  S5

//We can think of the above as a DFA.

================================
Goto table for the above example
===============================
SO remember that the GoTo table remembers only the non-terminals. So here, our goto table will be: 

FromState      X
-------       ---
S0             S1
S1             _ 
S2             _
S3             _
S4             _
S5             _

===================================
Action table for the above example
===================================

-----------     ----            ----          -----
From State       a               b            eof
-----------     ----            ----          -----
S0              shift to S2     synErr        _
S1              shift to s3     synErr        ACCEPT
S2              synErr          shift 2 S4
S3              synErr          shift 2 S5     _
S4              reduce by #2   reduce by #2
S5               r1                _           r1



=================================
Let us parce on a string:  aabab
=================================
Currently our stack is: [S0] //lookahead is current a, so we shift and goto state 2 
Stack becomes: [S2,a,S0] with S2 being the stack top. 
.
.
.
.
Now finally our stack looks like: [S1, X, S0] , Now we check GoTo[S0][X] , which will make our stack look like: [Accept, Goal, S0] -- which implicitly 
means accept.

==============================================================================
Simulating the String acceptance on the transition graphs (DFA like automata).
===============================================================================
Which can simulate our string on the transition graph above. So now the logic is that initially we are in S0 and because we saw an "a", we went to S2. Then
we saw "b", so we went to S4. And when we went to S4, we are ready for a reduction (we can reduce "ab" with an X). So at that point, we are replacing the
previous two transitions with the "X" transition relative to the base state (which is S0 in this case). So that is why, when we pop off, we look at the 
base state after popping off. So after we pop these off, we look at S0 because we are saying that we are replacing "ab" with an "X". So instead of making
these two transitions to an S4, we are making this one transition to "X" relative to the base -- so this is what the substitutions or the reductions 
mean.

==================================
So again, what is reduction about
==================================
Replacing potentially multiple symbols with a single symbol (which is on the left hand side of the production). And on the left hand side of the production,
we only have one symbol (because it is a context-free-language).

================
Homework problem
================
Goal -> Expr 
Expr -> Term + Expr 
Expr -> Term 
Term -> Factor * Term 
Term -> Factor 
Factor -> identifier

========
Solution (partial)
======== 
 Level 0 -- Goal -> .Expr, eof 
 Level 1 -- Expr -> .Term + Expr , eof 
 Level 1 -- Expr -> .Term, eof 
 Level 2 -- Term -> .Factor * Term, {+, eof} 
 Level 2 -- Term -> .Factor, {+,eof}
 Level 3 -- Factor -> .ident, {*, eof}


                            ============================================================
                             Compilers Lecture 21: Parsing (10): LR(1) Parsing, Part IV
                            ============================================================

===============================
Issues related to LR(1) parsing
===============================
Let us consider an example: 
St -> Assign | IF 
IF -> if (cond) Code else Code  
   -> if (cond) Code 
Code -> st | {stList}  //We should allow a single statement after the else condition 
stList -> stList st | Epsilon 

==================================
Case study of If Else statements
==================================
If else statements are interesting when it comes to parsing because the grammar (the above grammar) is ambiguous. Why is it ambiguous? 
Actually, in the above example, let us (for the sake of simplification) consider the allowance of just a single statement instead of multiple blocks of 
code for If-Else derivations. Let us now consider the following: 
                                                                    if (cond1) if (cond2) st1 else st2  <--- (i) 
Now where is the ambiguity in the above (i.e (i))?  
The ambiguity is that we are not sure whether the else statement is attached to the first if or the second if. There are two derivations for the above. 
Ambiguity means that you can come up with 2 possible derivations. (By the way, the language that we generally program with, the else statement is attached 
to the 2nd if). Now if we try to draw the parse tree for the above example, we will get more than 1 parse trees (making the grammar ambiguous). And also 
one of these derivations will consist of an else statement for the inner if and the other one of these derivations will consist of an else statement for
the outer if. 
If we are doing LR(1) parsing, at some point the placeholder will get to the the end of the st. Example: 
                                                                                                         If -> if (cond) st. else st.   <--- (ii)
                                                                                                         If -> if (cond) st.            <--- (iii)

Then it means that everything before the placeholder is already in the stack. Now by the way, what is the lookeahead? Well if we look at the very first 
production then it says that our statement can be either an assignment statement or an IF statement. Which means that an IF statement can always be followed
by an "eof" or even "else".  So, in fact, our lookeahad is going to be else and end of file. So in fact, our if statement can be followed by an EOF if it is 
a complete statement, otherwise it can beb followed by an else. Now when we construct an LR(1) table for this.
Now (ii) is going to be one of the states. It is not going to be the start state because we will get to that the placeholder gets past all the previous ones.
But that is the point where we will hit a problem because at that point, do we have a reduction? Yes, we have a reduction in both (ii) and (iii). And let 
us say that we can reduce by rule#2. BUT, do we have a shift??? YES, we also have a shift.  THe problem here is that when you get to this point and if what
you are seeing in the file is an else, you can reduce or you can shift -- and this is what we call "SHIFT-REDUCE ERROR", and this is not good. And in fact, 
if we shift, we will get one of our parse tree derivations and if we reduce then we will get another.
So, let us list out our parse trees: 
                               PARSE TREE#1: 

                                       st 
                                       if 
                                if   (cond)  st
                                             If 
                                      if (cond)   st  else st 


                               PARSE TREE#2: 

                                       st 
                                       IF 
                            if (cond)  st   else st 
                                       IF 
                                  if (cond)  st


So now getting back to (ii) and (iii). Let us say that we try to shift, which parse tree will we get? Well, if we shift then we will get the first one because
if we shift then we are waiting, we are not reducing until we get to a later state (when the placeholder will be somwehere else). And our reduction is going
to be the statement: IF -> if (cond) st1 else st2
Similarly, if we reduce then we are going to get the second parse tree. But how did we figure out what kind of parse tree will we get when we do either one
of the given operations (shift or reduce)? Because we are doing "Bottom-Up Parsing", therefore we start from the leaves and since in the second parse tree
the leaves are only upto the first statement before we reduce it to "IF" therefore, we were able to make the deduction that if we reduce we are going to 
get parse tree#2 and if we shift then we are going to get parse tree #1.
Anyways, this will give us a shift reduce error. 
 
================================
Again, what does ambiguity mean?
================================
Ambiguity means that you can come up with 2 possible derivations.

=============================
Resolving Shift-Reduce error
=============================
When you have a shift-reduce error, strictly speaking, it means that this grammar is not LR(1) grammar. But in-fact, if we can choose whether we want to 
shift or reduce when both are possible then we can proceed. So, in fact, it is a matter of choosing whether we want to shift or we want to reduce. 
And in this case, if we choose to shift, we will get the expected behavior (or the expected derivation). 

==========================
Parser Generator program
==========================
We will be using Bison to generate a parser and we have an "If Else" and if you write grammar for "If else", it will tell you that there is a shift reduce
error and the default is to favor shift instead of a reduce. In-fact, if you disable the warnings in bison, it will give you the default resolution of the 
conflict which is resolve the conflict in favor of a shift rather than a reduce.  

================
Ambiguous Grammar
=================
An ambiguous grammar is not necessarily bad. And it is not necessarily unparsable. Because all languages have "If else" and people write parsers and grammars
for those and everything works fine. 

================
New Example
=================
0. Goal -> A 
1. A -> aA 
2. A -> a 

========
State S0
========
Goal -> .A , eof 
A -> .aA, eof 
A -> .a, eof 

==========
State S1 //Transition to A 
==========
Goal -> A. , eof 

========
State S2 //ActionTable[S0][a]
======== 
A -> a.A, eof 
A -> a., eof //reduce by rule#2
A -> .aA, eof 
A -> .a, eof 

=========
State S3 //Goto[S2][A] 
========
A -> aA., eof //reduce by rule#1 

==========
State S4 //ActionTable[S2][a] 
==========
LOOP TO S3

=================================================
Important things to remember when entering states
=================================================
Whenever you enter a new state, the first question that you should ask is that is there an expansion? And the second question that you should ask after that
is that is there a reduction?? Expansion here means checking whether there is a non-terminal next to our placeholder and whether the derivations for that
non-terminal need to be listed out.

==================================
Goto table for our example grammar
==================================
     A 
S0   S1
S1   _
S2   S3
S3   _

==================================
Action table for our example grammar
==================================
     eof               a 
S0                     shift to S2  
S1   ACCEPT
S2   reduce by r2       S2 //loop
S3   reduce by r1

==================================================
Point regarding Left Recursion vs Right Recursion
==================================================
If we had 3 "a"s instead of millios a's then it would have put a million states and it would have shifted a million times. And it would have put, in fact,
2 million symbols on the stack before making the first reduction. When you apply LR(1) parsing to a RIGHT RECURSIVE grammar like the above then you will
end up with a really big stack.
LL(1) has its problems with Left-Recursion grammar. It cannot do Left recursion grammar at all. But LR(1) can do BOTH Left Recursive and Right Recursive.
But with right recursive form, it has this stack problem. It has to do a lot of shifting and it may grow the stack really big. 
Conclusively, LR(1) parsing works better with Left Recursive grammar. Because LR(1) generates totally different tables for these. 


                            ============================================================
                             Compilers Lecture 22: Syntax-Directed Translation (1)
                            ============================================================

=============================
Review from previous lectures
=============================
In earlier lectures ,we talked about the different stages of a compiler i.e the Frontend, the optimizer and the backend. We said that the frontend takes
the high level representation (the source code) and the output of the frontend is the AST (abstract syntax tree). Today, we will learn how we can construct
the AST based on our parsing techniques. 

==========================================================================
Example: We will today be parsing and generating an AST for the following:  x = a *3 + b
==========================================================================
0. Goal -> Assign 
1. Assign -> ident = Expr ; //Make an Assign Node
2. Expr -> Expr + Term      //Make an add Node
3.          | Expr- Term   //Make a subtract Node
4.         | Term          $$ = $1 
5. Term -> Term * Factor   //MAKE Multiply Node
6.        | Term / Factor  //MAKE Div Node
7.        | Facotr
8. Factor -> (Expr)    //$$ = $2 
9.        | num       // MAKE Numm Node
10.       | ident     //MAKE Identifier Node

===========================
Parse Tree for our example
===========================
            Goal 
           Assign
     ident   =    Expr             ; 
               Expr +       Term
               Term         Factor
           Term  * Factor   ident
           Factor   num     
           iden  

==========================
Conceptually: What is AST
==========================
An AST, conceptually, is this parse tree with only the operations that matter in the code generation. Or the operations that correspond to real actions. 
So we will like to keep the nodes that correspond to real actions and get rid of any nodes that correspond to grammatical substitutions -- you know 
substituting a grammatical symbol name for another. 

==================================================
Where are the actual operations in our Parse Tree?
===================================================
The "=" sign which is the assignment operator. But what about the leaves which are the identifiers and the numbers? Now we should start thinking in terms
of code generation. So in terms of code generation, this will result in generating some code because if "a" (which is an identifier) is a variable then 
this variable -- in other to use in this expression, what kind of code do we have to generate? We have to use a LOAD instruction to bring it from memory. 
So, these leaves are also interesting. 
Since our Objective is to construct a data structure that will help us in code generation, then these useful interesting nodes are the ones that we care 
about and the other nodes can be abstracted away (hence the name "Abstract" syntax tree).

======================================
Abstract Syntax Tree for our example
======================================
So our AST will look like this: 
                                Assign  
                         ident         +  
                                 *       ident
                            ident   num    
  
===================
Generation of code
===================
Load a -> r1 
Load I3 -> r2  //load immediate of 3 in another register
Mult r1, r2 -> r3
Load b -> r4 
Add r3, r4  -> r5 
Store r5 -> x 

//By the way, here we are assuming that we have an infinite number of registers. We will leave the process of mapping these virtual registers to the actual
//physical registers to the stage of Register allocation.

========================================================
What graph algorithm did we apply to generate the code?
========================================================
We applied depth first algorithm to generate this code above. What specific graph traveral technique did we take advantage of? We took the advantage of 
"Post order graph traversal". WHY? BECAUSE WE FIRST PROCESSED THE LEAVES BEFORE WE PROCESSED THE ROOT. So just with a simple POST ORDER traversal, you 
can go from an AST to ASSEMBLY CODE.

==========================
But the question now is...
==========================
The question now is how can we get from our grammar or our parse tree to the AST. The key idea is to look at the rules of this grammar and identify the 
rules that correspond to the set of operations that we care about. And whenever a reduction happens using that rule, we create a Node in a tree. So for 
rules #9 and #10, we can simply create "Num Node" and "Ident Node". But what about rule#8? Should we make a Node or not (for the paranthesis expression)?
So in order to answer, whether we should create a Node or not, so if we have something like : a * (3+b). Then we want something like: 
                 Assign 
             X           * 
                    ident     + 
                           num  ident

So, in fact, with these paranthesis expression, we did not create a new node. So this is already taking the node that coressponds to this expression. So 
what we are doing is that we are taking the root of the tree for the paranthesized expression and we are making it the right operand of our Mult. So when
we have paranthesis, we are not actually creating a new node. And remember, what to keep in mind is that this whole thing is recursive. So this process 
of parsing  the grammmar is recursive and the abstract syntax tree is recursive. So when you have a rule like this: (Expr) , this paranthesized expression 
can be just the root of a WHOLE BIG TREE. So this (Expr) can be an arbitrarily big expression. So it can have its own tree. So we are basically linking 
trees. So we are not making a new node here. 

=============================================================================================
What about the other rules in the grammar that correspond to multiply or divide instructions
=============================================================================================
When we are Saying Term goes to Factor (i.e Term -> Factor) we are not doing an operation and thus we are not creating a new node in the AST. But when we 
are saying: Term -> Term * Factor  , we are indeed creating a new multiply node.

===========================
How do we make these nodes
===========================
A node in the tree will have two children. So making this identifier node, or the number is trivial because these are leaf nodes. We can think of a node as 
an Object and this object has a type in it (for example: an identifier). And then it must have the actual name of that variable. Or what is more than the 
actual name of that variable is memory location (or the offset of that variable). So keeping the name (which is symbolic information) is not necessary. It will
be useful for sure (for example, for debugging purposes), but it is not necessary. However, keeping the offset is important. So we knowing whether the 
variable was named "x", or "y" or whatever is not important but knowing the number that was stored in those is important.
So the operations are non-leaf nodes. 

================================================
What do we need to do to create a multiply node
================================================
So since multiply is a binary operation, so it needs two operands i.e the Left operand or the right operand. So in order to do this multiply node, we need 
to have a pointer to the left operand and the right operand in order to link them together. Now how do we do this? We need a mechanism for passing variables
or passing certain attributes that are associated with the symbols in this grammar.  How do we this? we will be using the YACC notation.

======
YACC 
======
YACC is a parser generator. So it takes a grammar that we write and the actions associated with each rule (like for rule#1 we have MakeAssignNode etc) in 
the grammar and using this, it generates C code that does the parsing for you. So it does all of this LR(1) stuff like GoTo table, action table, creating 
states etc. 
YACC stands for "Yet Another Compiler Compiler" because it itself is a compiler and it generates a parser for you. And that parser becomes part of the 
compiler that you are building. 

======
Bison
======
It is a more recent version of this parser generator. 

===========================================
Notations that these parser generators use
===========================================
There is a notation that these parser generators use for each rule in the grammar. They use the "$$" for the left hand side symbol. And for all the symbols
that appear on the right hand side, they use the $ and then the number of the symbol. Remember that when you count the symbols on the right hand side, 
everthying counts (whether terminals or non-terminals).
So for example:  in rule#5: 
$$ -> $1 $2 $3 
Term -> Term * Factor        //MakeMulNode($1, $3)  //because I already know that $2 is the multiplication node. 

So why do we need this? Well we need this because in order to construct a multiply node. Suppose, we want to express the same notation for Term goes to 
Factor. How can we write this? We can do this like this: 
Term -> Factor -- $$ = $1 //this is just like a pass. I am not actually creating any new node because this is just a substitution. When you are renaming 
a factor to a term (a grammatical substitution). And similar for rule#6: 

Term -> Term / Factor  -- $$ = MakeDivide($1, $3)

================================================================
What will be these MakeMultiplyNode and MakeDivideNode be doing?
=================================================================
What is tricky about this dollar sign notation is that this attribute can take multiple types. So in our rule#5, what do you think is the right type
given what we have written so far? What is the type for dollar sign? It is going to be a POINTER TO A NODE. So our MakeMultiplyNode function will be 
like this: 

MakeMultiNode(Node *n1, Node *n2) 
 create new node with ptr n 
 n.left = n1 
 n.right = n2
 return  n  

==============================
The last node that we create
==============================
Because we are going bottom up, the last node that we are going to create is going to be the ROOT node. This is because we are starting from the leafs.
We are building our tree in a bottom up manner. And we are also implying the use of an LR(1) parser (a shift reduce parser). 
The point is that whenever the parser reduces to rule#5, call the function associated with that rule (i.e MakeMultNode($1,$3)) or execute this action.
This is the action that we are embedding in a grammar. These embedded actions are the actions that are going to build the tree. 
For rule#7, we do nothing, we are just passing. We are not creating a new node. We are taking this identifier (which is what the factor is) and making it 
this Node here (Term). 

======================================================================
The paramters need not always be $1 and $3, Example of an IF statement
======================================================================
IF -> if (cond) Code else Code
Then, ethe parse tree for this would be something like: 

        if 
Cond    Then Code     Else code 

//MakeIF(Node * cond, Node * thenCode , Node * elseCode) 

Now in order to write the action, we will need to give three parameters to our MakeIF function i.e the the condition, the then code and the else code. 
Since IF -> if  ( cond ) Code else Code 
            $1 $2 $3  $4 $5    $6   $7
 
Our MakeIf function is: MakeIF($3, $5,$7)

//So the dollar signs are not always 1 or 3.

===========
Leaf nodes
===========
They are not internal nodes. They are not linking other nodes. All internal nodes are going to need their children so they can link their children. And the 
leaves in the AST are stuff like "ident", "num" etc. Well the leaf nodes are different because here you are not linking other nodes. You are just creating a
node that should have information about this identifier or a number (as far as our example grammar goes). Hence for creating the leaf Node such as MakeNumNode
or MakeIdenNode, you infact need to pass the token or the actual lexeme as a parameter -- because these are the ones that are coming from the scanner. The
scanner has classified these as identifiers and numbers but there are actual lexemes coming along with these. 
So remember the leaf nodes are different. So this dollar sign notation or this way of passing the values from the Low-Level rules to the higher level rules 
-- so what we are doing here is that we are basically passing the values up. We are passing the values up from the lower level to the higher level rules 
in the grammar until we get to the top level rule which generates the start variable of a grammar.

=============
Conclusion
=============
Now this is how we will be building the abstract syntax tree. So the idea is that whenever you reduce, the parser is going too execute an action. When a 
parser does a reduction. 

==================
One final example
==================
Consider at some point our stack looks like the following: 
 [S3, B3...S2,B2,S1,B1, StackTop]

And we have a reduction as folllows: 
 A -> B1, B2....Bn

So we can REDUCE by A -> A1, B2,...Bn
Stacktop = Pop 2n items in the stack 
push A 
push GoTo[StackTop][A] //where StackTop is the base state. 

***NOW WE CAN EXECUTE THE ACTION ASSOCIATED WITH THIS RULE IN THE GRAMMAR. In-fact you can execute the action associated IMMEDIATELY AFTER MAKING A REDUCTION.
   AND OFCOURSE, if you shift, you don't do anything. These actions are associated with reductions and only with reductions. And ofcourse, with some of the
   reductions we are putting useless actions or the actions that are passing the information up (for example in rule#7 and rule#4) -- so those are pass type
   of actions, they will pass the information from one node in the derivation tree to the node above it. But with parser shifts there are no actions associated
   with those***


                            ============================================================
                             Compilers Lecture 23: Syntax-Directed Translation (2)
                            ============================================================

================================================================
What do we do in Syntax Directed Translation. "Embedded Actions"
================================================================
We have the grammar and in this grammar we add we call "Embedded actions" -- these are the actions that embed into our grammar. And these actions will get
executed whenever we make a reduction during parsing. 

=====================
Simulation of a parse //BASED ON LR(1) parsing 
======================

------      --------            ---------- 
Stack        String              Action
------      --------            ----------
[]          x=a*3+b             shift
[x]         =a*3+b              shift   //Should we reduce this to ident and then ident to factor? No, it will not reduce. How will it know not to reduce?
                                        //Because a Factor is never followed by an equal sign. So in this case, it will just shift
[x,=]       a*3+b               shift 
[x,=,a]     *3+b                reduce  //because a factor can be followed by a multiply. So this move is legal 
[x,=,Term]  *3+b                reduce  //beccause a Term can be followe dbya multipy 
[x,=,Term,*] 3+b                 shift 
[x,=,Term,*,3] +b                reduce 
[x,=,Term,*,Factor] +b           reduce //because an expression can be followed by a + symbol
[x,=, Exp+]      b                shift
[x,=,Exp+b]     _                reduce 
[x,=,Exp+Factor] _               reduce
[x,=,Exp + Term] _               reduce
[x,=,Exp]        _               reduce
[Assign]         _               reduce 
[Goal]           _                  _ 

===========================
Why are we doing the above?
============================
Here, derivation is a sequence of shifts and reductions. And with some of these reductions an action will take place. And with some of them, no action will
take place. The way we do this in practice is that we will write a grammar with embedded actions within it. And these embedded actions will have C code. 
So we will write Code in C. Then that is all that we will do. THe parser generator will generate an LR(1) parser for this grammar. And that L R(1) parser 
whenever it does a reduction according a certain rule, it is going to generate the code that you wrote for that reduction. You can put any kind of code 
for a certain rule if you want. It can be a line, multiple lines or a million lines of code. You have to think about the actions that you want to do 
at each reduction. You have to think about which of these reductions result into actual actions and which one of these reductions do not result into 
actual actions. In case of the ones where no actual actions are to be performed, you will just be passing values up. For ex: reducing factor to a term, 
there is no actual operation. So you will just pass the values up in that case.

========
Reminder about $$  and $num
========
$$ is always always associated with the left hand side of the symbol and the $num such as $1 or $2 is always associated with the symbol to the right. So 
make sure that you count all the symbols on the right hand side. And a symbol could be a terminal or a non-terminal.

=================================
What are we going to move to next
=================================
In a minute, we will go through another example that instead of constructing an AST will compute an estimate of the computational cost of a given 
computation. Also, we don't necessarily have to use this to construct an AST. The example that we will be going through will help us understand how this 
works and how the information is passed up.

============================
Cost of different operations
============================
Let's say that: 
Cost(Add) = 1 unit 
Cost(sub) = 1 unit
Cost(Mult) = 3 unit 
Cost(Div) = 7 unit
Cost(load) = 4 unit
Cost(store) = 3 unit
Cost(loadImmediate) = 1 

Now given these costs, we would like to write embedded actions in this grammar to compute or to estimate the computational cost of a given string (in
this case, an assignment statement).

===========
Example String
==============
The string for our example is:  x  = a * 3 + b 
==========================================
The computation will start from the leaves
===========================================
So for an identifier here, in order to compute this, we will assume that the code that will get generated is going to load 'a' from the memory. So, for 
every identifier reduced to a factor, we will have an embedded action: $$ = cost(load) //in this case, the cost of a load is 4. SO whenever there is an 
//identifier, we are assuming that the generated code will be load. Now is this assumption a valid assumption? Or does it make sense or not? But is it 
//a resonable assumption to make that whenever there is an identifier, it will  get loaded from the memory? 

==================================================================================================
Is it a reasonable assumption to make that whenever there is an identifier, it will get loaded...
==================================================================================================
Let's say that we have something like:  
                                       x = a * b + c + a + 3 * a + 7 * a 
So the code that will get generated for this is not going to "a" 4 times. It is going to load it once and then reuse. But in order to find out whether this
"a" has been loaded or not, how do you think the compiler is able to tell that? The answer is that we have to store this information in a table. We have
to store this information in some data structure. And one data structure that we can use is the "SYMBOL TABLE" -- one of the most important data structures
in a compiler. This symbol table is going to have all the symbols in it with some information. One of the pieces of information is the memory location -- 
the memory location is the location of this variable in memory, which is what we care about. And in this, you can add "isLoaded" flag indicating whether 
a variable is loaded in THE CURRENT BLOCK. If this is loaded then the cost is the cost of a load otherwise the cost is 0.


===============================================
What code do we add to make a check like this?
==============================================
if (token isLoaded) 
      $$ = 0 
else 
      $$ = cost(load)

===================
The point to pick up
====================
For now, we want an identifier to be loaded recently and in the current block of code that we are currently processing. It may have been loaded previously
in a code that is distant from the current piece of code that we are processing (so it could be too distant). The idea is that a variable will not get 
loaded all the time. 

===================================================================
If an identifier gets loaded then what happens in case of a number?
===================================================================
We can do a loadImmediate. And we can assume that the cost of a loadImmediate is 1. Here we can say: 
$$ = cost(LoadImmediate). 

==========================
Cost of a factor -> (Expr) rule
==========================
$$ = $2  

Why $2? Beause first bracket is $1, the expr is $2 and the closing bracket is $3.

====================================
Reminder: At what level do we load?
====================================
WE load at the leaf level, however that may or may not happen.

==============================
Cost of Term -> Term / Factor
===============================
So first off, it is very possible that the Term child can have a whole tree below it and this Factor on the right hand side can have a tree on its own  
below it. So whatever this Term is, we want to take its cost and the cost of a factor and then add to that, the cost of the divide. So how do we express 
this using the dollar sign notation: 
 $$ = $1 + cost(div) + $3

=====================
Cost of Expr -> Term
======================
$$ = $1 

===========================
Cost of Expr -> Expr - Term
============================
$$ = $1 + cost(sub) + $3


===============================
Cost of Assign -> ident = Expr
===============================
This "ident" is not something that we are going to load. So we will store it at a different address.  So: 
$$ = $3 + cost(Store)  

So the only thing that requires computation is the expression then we will have to assign it to store it at a given address. How will we know the address? 
We will look up the address for this identifier in the symbol table and then we will store it at that memory location. So there is no loading involved here. 

=======================
Cost for Goal -> Assign
========================
$$ = $1 


================================
Tracin all these in an example
===============================
Our example string is:            x = a * 3 + b


===========================
AST for our example string (with costs labeled)
===========================
                      Goal 
                     Assign 16
             ident    =     Expr 13          ;
                      3  8 Expr  + Term 4
                       Term 8   1  Factor 4
                  Term * Factor     ident 4
               4  Fator 3  num 1       b 4 
                4 ident
                4  a 


===========
So what is happening here
===========
We start bottom up. With this tree, what will happen, the first thing is that we will reduce an identifier to a Factor. When we do this, we calculate the 
cost of a load -- for which the cost will be "4". Then we will do a reduction from a Factor to a Term. When we do this, we are just passing the "4" up. 
So this "4" which is just the cost of a load will get passed up. So we will label the cost of Term as "4". So for each node in the tree, there is a value
associated with it. And this "$$" is that value. And these actions define the value of a node in terms of the values of its children. So this is post 
order tree traversal because we are defining the value of the current node in terms of the value of its children. So we have to process the children 
first before we can process their parent.

================================================
Why are we computing the cost of each operation?
================================================
This is an exercise at this point. From a practical point of view, it is useful. Sometimes the compiler even a high level representation of the code,
may want to estimate the number of operations needed to compute a certain expression or a certain block of code. It is useful to compute an estimate of 
evaluating a certain block of code or a certain function or whatever. 
So for example, a prefetcher in the gcc is a compiler optimization stage that adds prefetch instructions to prefetch memory locations before they are 
needed. So that when you need them, they are already in the cache. And in order to figure out the prefetching in a given loop. In order to figure out 
how early you might want to prefetch something in a certain loop iteration to use it in the next iteration. Let's say that we want to prefetch A[i+delta] 
when we are currently in A[i]. SO we want to prefetch something from the next iteration. Now how far ahead should this be? Should you fetch for iteration #2
or iteration 3 or iteration 4? This depends on the body of the loop. What you are executing in the loop and how much computation you expect to happen inside
the loop, so that you know whether to fetch 3 iterations ahead or 10 iterations or a 100 iterations ahead. Depending on how big the loop is. And in order to 
know how big the loop is, you need to come up with some estimate like the one that we came up with above. so it is not just counting. Counting instructions 
is not enough because some operations are more expensive than the others -- like a divide is more expensive than a multiply. In practice, compilers may want
to do this kind of computation. 
But we are just doing it to understand the embedded actions and the dollar sign notation. To understand how the embedded actions and the dollar sign notation
works.

========================================================================================
Right embedded actions that will count the number of divide operations in the expression
========================================================================================
So how do we modify these actions? So instead of computing an estimate of the computational cost, we will like to just count how many divides there are 
in an expression. So if you have an expression like: x = a/b + 3/4 + c * d + f/n; 
So if we have an expression like the above, we want our count to be 3 because there are 3 divides above.
So how do we modify our actions to count the number of divides in an expression? 

============
Grammar
===========
Block -> Assign        $$ = $1
       | BlockAssign   $$ = $1 + $2 //why? Because we are doing the recursion. So we are saying that when we do the recursion, the number of divides in the 
                                   //lhs block is equal to the right hand side block and the assign. So if we have 10 assignments in the right block, when 
                                  // we do the final reduction, the rhs assignment is going to be the last assignment and the block is going to have 9 assignments
                                 //in it.
Assign -> iden = Expr; $$ = $3 
Expr -> Expr + Term    $$ = $1 + $3 
       | Expr - Term   $$ = $1 + $3 
       | Term          $$ = $1 

Term -> Term * Factor   $$ = $3 + $1 
       | Term / Factor  $$ = $1 + $3 + 1 
       | Factor  $$ = $1  
Factor -> (Expr) $$ = $1   //becaues the number of divides in this factor will be equal to the number of divs in the expression.  
         | num   $$ = 0  
         | ident $$ = 0 



                            ============================================================
                             Compilers Lecture 24: Code Generation for Expressions
                            ============================================================

=================
Review until now
=================
In this lecture, we will be talking about simple code generation. So far, we have covered parsing and we know how to construct the AST. And we talked about 
how to go from the AST into ACTUAL CODE.

======
Example
=======
If we have an example like this:  a * 3 + b 
The abstract syntax tree for this will be like ths: 
        +
    *       b 
a      3

===========
To load a
==========
LoadAI rap, a -> r1 

==========
To load 3
==========
loadAI 3 -> r2

============================
Multiply operaton on a and 3
============================
mult r1,r2 -> r3
 
============
To load b
============
LoadAI rap, b -> r3

==================
Adding (a*3) and b
==================
add r3, r4 -> r5

Now in terms of code generation, we talked about this before, but now we are going to talk about this in greater detail. We will think about all the issues
that are involved in generating code for an expression like this.
In the above example, we will load a into some register r1 and we will load 3 into some register r2. Ofcourse loading "a" here is loading a variable. 
=====================================
What does "Loading a variable" mean?
======================================
Which means a memory location that consists of a base and an offset in general. The instruction that we used in the earlier lectures "LoadAI" (read as load
Address and Immediate) where the base is in a register "rap" read as "r activation record pointer".

===============================
What does LoadAI rap, @a -> r1
===============================
LoadAI is a load instruction that takes an address and it takes an immediate operand. The address is in a register. This register has a pointer to the 
activation record for the current routine that we are compiling. And this address of "a" is an immediate number that represents the offset of variable 
a. If we pass this to the emit function then rap will be src1, @a will be src2 and r1 will be dest node.

==========================
What this lecture is about
==========================
So now, today's lecture is about the implementation details of this. Not the implementation details at the coding level but algorithmatically. At coding level,
we will have to traverse the AST and generates the abstract assembly instructions.


====================
Pseudocode for this
====================
So it is a tree traversal and we have said a depth-first traversal will serve us right. So this whole process has to be recursive. 
Pesudocode is: 

Expr(node) 
{
   switch (NodeType)   //here we are thinking of a node as a data structure and this node has some fields in it like type (like operator, operand like var or a num).
  {                    //other fields like operation (like add, sub, div, mul)
    case num:         //if it is a number then all what need to do is to GENERATE a LOAD IMMEDIATE INSTRUCTION
        result = GetNextReg();
        Emit(loadI, node.val, NULL, result); 
       
    case var:
        result = GetNextReg();
        Emit(loadAI, node.base, node.offset, result);
        break;
  
   //Now for the internel nodes that are the operations. Now we need to do more than just issuing an instruction or generating an instruction
   //To generate code for multiply, what do we need to do? Before we can calculate the multiply, we have to calculate everything that is below it.
   //So we have to call the Expr(node) function recursively. This is because our operation can be a huge tree. In general, if you have an Operation then the 
  // left and the right operands can have whole subtrees below them for an arbitrary expression. So the right thing would be to make sure that each of the 
  //child tree is evaluated first. You have to go down.

 case OP: 
  t1 =  Expr (node.left)
  t2 = Expr (node.right)
   //now this means that whenever i compute an expression, I have to return the destination register. In every case, I have to return the destination register
  //so that the parent node knows this. So Expr(a) should return r1. 
  result = GetNextReg() 
  Emit(OP.operation, t1, r2, res);
  break;
}
return res;
}


//Node type
struct node {
   type(Operation, variable, num) 
   operation(add, sub, div, mult)
   value
   base 
   offset 
   left 
   right
}


//For generation of instructions 

Emit (Operation, src1, src2, dest) //if our Operation is load immediate then it will know that src1 has to be a number/integer and the dest is a register. src-2 
                                   //will not be used because for a load immediate, we only have one src which is the immediate value and our dest is a register. 


========
Terminology, which one to use? Identifier or variable? 
========
In context of parsing, we typically use identifier. In the context of code generation, it makes more sense to use the word variable.

=========
Storing the value
=========
Now our above "node" struct is a generic data type. In practice, that would be probably our base class and then we'll have derived classes from that base 
class like identifier, number etc and based on that, we will define the data type for the value. 
Now the question is, if in our abstract syntax tree the node that we currently traverse is a number node then what to do? Well we need to store that number 
value somewhere (inside the node struct). We will then pass this value to the Emit(param) function.

==========================================================================
Where should the register numbers come from for the destination values 
=========================================================================
You want to count rack of what you already allocated and so you would take that out. All what you need is a counter and remember that at this point, we are 
only doing virtual registers. So when we are doing virtual registers, we can have as many counters as we want. So all what you need is a counter. So we need
a function that gets the next register number. For examle: 
                                                             GetNextReg()


========
Storing variable
===========
All that we need is that we need this emit operation. And we need a destination. Now the base and the offset for this are going to come from where? We need
the BASE and the OFFSET. We can store the BASE and the OFFSET in our node data structure as well.

========
Register numbers
===========
These register numbers like t1, t2, t3, result etc can all be represented as integers.

=================================================================
Simulating the entire code on the above example for our given AST
=================================================================
So first of all, we are going to call the Expr(node) funciton on the root i.e the "+" operation. So our stack will be:

  [Expr(+)]

Expr(+) will switch and because it is an OS, it will recursively call itself on the left node. And the left node is going to be the multiply. So our 
stack will be: 
    [Expr(+)| Expr(*)]
Again, * is an OP, so we will do a recursion on the left subtree. 
 
    [Expr(+) | Expr(*) | Expr(a)]

Now for Expr(a), it will identify a as a variable. And it will generate this with register#1 holding the result in the variable case. This will be 
the first time that we will call GetNextRegister. And it will get us r1 and it will generate this instruction that we want. Now when Load A returns,
we will be in the case statement of the multiply (since we are tracing the recursion here). It is important to understand that this whole thing is
recursive. The R.H.S of multiply is going to be 3. So our stack will be:

  [Expr(+)| Expr(*) | Expr(3)]

When we call Expr(3), it will load immediate of 3 and it will put it in register#2. Now, we are in Expr(*). t1 returned r1 and t2 returned r2, and 
now, it is going to get the next register which will give us r3. This will generate that multiply that we want. It is going to multiply r1 that 
came from the left child (or that holds the result from the left child) and r2 that holds the result of the right child, and r3 the new register 
that we have just generated. Now the multiply will eventually return after this. So our stack will be: 

    [Expr(+)]

Now we are the second line of the add (the top level). The left child for the add return r3 that will be stored in t1. Now it will call the right 
child, so our stack will be:
      [Expr(+) | Expr(b)]
This is a load immediate instruction that will be stored and returned as r4. So our stack will be back to: 
      [Expr(+)]

Now GetnextRegister() line will be called and it will return r5 (it is just a counter). Then we will emit the instruction. 
So this is how it is going to work. 

============
Comfortableness with recursion
==========
If you are comfortable with recursion then this should be intuitive. 

====================================================================================================
Question: What if we moved "res = GetNextReg()" line before the t1 and t2 lines in case of operation
====================================================================================================
In this case, we need to ask ourselves, will the algorithm still work correctly? Yes, it will. But the register allocations will not be in the order. 
Different register numbers will be used but it will be correct. What will happen to register numbers? How different will they be? The destination is 
going to be r1. So basically, it is going to do the allocation for destination in PRE-ORDER AND NOT IN POST-ORDER, but that is fine. These are just 
register numbers. So it is going to set the desination to r1 and then it will call the left child and so on. So our labeled AST will look something like
this: 

                      +  -> stored in r1
stoerd in r2 <-   *        b -> stored in r5

stored in r3 <- a   3 -> stored in r4

==============================================================================================================================================
Question: What if we moved the emit and the result before t1 and t2, before calling expression, so if we do them in pre-order, will it work?
==============================================================================================================================================
So basically, getting a register number is not a big deal. We can do it before and after we compute the expressions below the current node. That is 
fine because it is just a number. But calculating the actual expressions below that and computing them MUST BE DONE BEFORE we can generate the code
for the current node. Because the current node is dependent on its childre.
So, the actual computation has to happen in post order. But the numbering of the registers doesn't matter. You can do it pre-order, post-order, 
any order.

=========
Lastly
=========
You may find a solution for the numbering problem, but you will NOT find a solution for the actual dependence problem.


                            =======================================================================
                             Compilers Lecture 25: Code Generation for Conditionals and Loops (1)
                            =======================================================================

===========
Intro
===========
So today is an important lecture. it is very fundamental stuff. And it is somethign that we will have to do in the next assignment. In assignment 4, you only
generate the abstract syntax tree. In assignment five, you take the abstract syntax tree and you actually generate assembly code. Last lecture we discussed 
how to generate abstract assembly for expressions. We did code generation for expressions and the point is that it is recursive. THe idea is to do it 
recursively, because an expression consists of expression within it. And the grammar for the expressions is recursively, so the generation for the code of 
expressions is recursive. 


==============
Example code
===============
1. st1 
2. if (condition) //condition may be something like register1 < register2
3.     st2 //st2 may be list of statements
4. else 
5.     st3
6. st4 


=================================================
General Abstract syntax tree for an If statement
=================================================

             if 
           /   \
       cond    stList 

 
=================================================
General Abstract syntax tree for an If-else statement
=================================================
              
                if 
           /    \     \         
       cond  stList1  stlist2 

why? Because you need something (which is stList1) to execute if the condition is true, and something else (which is stList2) to execute if the condition is
false. So your embedded action for an "if-else" would CONSTRUCT something like the above.  

=========
AST for the above example
=============
                 if/else 
            /    |     |
        cond    st2    st3

======
Now, how do we generate code for the above?
========
Now the code that we will generating will be abstract assemly. It will not be real assembly. So it is generic assembly.  That is, you know, assembly instructions
that are likely to exist on most machines and in particular RISC machines which have fewer instructions. The Abstract assembly that the compiler uses is 
usually generic, and it's something that you expect to have on most machines. Remember that some compilers such as GCC generate code for many many different
machines. And before code for a specific machine, they generate that generic assembly that is execpted not to be found on all machines, but to have some kind
of equivalent. 
The generic assembly that we will be using is "ILOC" in the textbook. ILOC stands for "Intermediate language for an optimizing compiler". And in this language
we will have some compare instructions like: 
cmp_LT rx, ry --> rz     //it takes two registers with arithmetic values andd it puts the result in rz 
cmp_LE
cmp_GT
cmp_GE
cmp_EQ
cmp_NE

So we have these compare insstructions.  Now in order for this to be useful. In order to implement a conditional, what else do we need in a addition to a 
compare instruction, in order to implement a branch. We need a condition branch. So if we put our result in rz, then if we have an instruction that will 
branch (a conditional branch) based on a register and it will go to one of the two possible addresses or labels (syntax given below).

   cr r -> L1, L2   //this will look at the result and if it is true, it will go to L1 otherwise it will go to L2. The result comes from the above compare 
                   //instructions.

=======
Jump instructions
============
jump r  //this is an indrect jump because the address to be jumped onto is stored in the register.
jumpI L //jump immediate. This is a direct jump 
=============
New example
==============
st1 
if (r1 < r2) 
   st2 
else 
   st3
st4 

============================================
Converting our example to abstract assembly
============================================
st1
cmp_LT  r1, r2 -> r3 
cbr r3 -> L1, L2        //conditional branch on r3 
.
.
.
L1 : st2
     jmpI L3 //jump immediate jumps to a label. The label is just a number or a constant. In machine code, it will just be an address in memory. 
L2: st3
l3: st4 

//These L1, L2 and L3 are labels. The assembler is going to take these labels and convert them to addresses.

==========
Basic Block -- concept
============
So the compiler will generally divide a program into basic blocks. A basic block is a straight line piece of code that doesn't have any branches. So in this
case, whenever there is a branch that will mark the end of basic block. And then we will start a new basic block. A jump instruction marks thhe end of a 
basic block. So, every branch or JUMP (whether it is a conditional or an unconditional branch) marks the end of a basic block. And EVERY LABEL marks the 
start of a new basic block. So in order to determine the basic blocks, you just scan the code. In the beginning, you are in basic block#1, whenever you 
encounter a jump or a branch then that marks as the end of the current basic block. Or if you see a label then that marks as the new beginning block. 

==================================================
Marking of the basic blocks from the above example
===================================================
 st1
 cmp_LT  r1, r2 -> r3 
 cbr r3 -> L1, L2                  //BASIC BLOCK 1
 --------------------------        
 L1 : st2                              
      jmpI L3                                    //BASIC BLOCK 2 
 --------------------------- 
 L2: st3                                        //BASIC BLOCK 3 
 ---------------------------- 
 l3: st4                                       //BASIC BLOCK 4


=============
Control flow graph (CFG) concept
============
The compiler would construct a control flow graph that will represent the control flow. Again, in this course, CFG is overloaded. It can stand for context
free grammars, and can stand for control flow graph.

=============
Control flow graph for our above example
==================
  
          Basic Block 1 
        /               \
 Basic Block 2         Basic Block 3
         \            /
          Basic Block 4

========
Importance of these concepts
==========
So these concepts are very fundamental and important. And very intuitive as well. 

=========================================================
Why does the compiler divide the program into basic blocks
==========================================================
Because, as we wil see when we discuss some of the compiler optimizations, it will make things much easier and much more logical when it comes to optimizing
code. For example, moving code or making transformations to the code within a basic block will be much easier than moving across basic blocks or making or 
applying transformations that go across basic blocks. Within the basic blocks, things are very simple and easy and it is a straight line piece of code. 
you don't have branches and you don't have control flow. WHen you start executing the basic block, everything within the basic block will get executed until 
you get to the end of the basic block. So a basic block is a straight line piece of code. 


==========
Representation of the code
===========
One typical representation of the code, after we generate the abstract syntax tree, a typical intermediate representation is to rerpresnet the progrm as a 
control flow graph and within each basic block, there is a linear list of assembly instructions.  

=============
Code generation for a while loop
=======================
For a while loop, you have code like the following:

st1 
while (r1 == r2) 
   st2 
st3 


====================================
Abstract assembly for the above code
====================================
//There are two ways of doing it

---VERSION 1-----
st1 
cmp_EQ r1, r2 -> r3         BASIC BLOCK 1 
cbr r3  -> L1, L2  
-----------------------------
L1: st2 
    cmp_EQ r1, r2 -> r3        BASIC BLOCK 2 
    cbr r3 -> L1, L2 
-----------------------------
L2: st3                        BASIC BLOCK 3
-----------------------------

----VERSION 2-----

----------------
 st1             BASIC BLOCK 1
----------------
L0: cmp_EQ r1, r2 -> r3         BASIC BLOCK 2
    cbr r3 -> L1, L2
------------------ 
L1: st2 
    jumpI L0             BASIC BLOCK 3
------------------- 
L2: st3                  BASIC BLOCK 4
--------------------


================================
Control flow graph for version 1
=================================

       [BB1]
     /      \
    /         \   
[BB2] ------> [B3]
| / \   //self loop on basic block 2
|_ |


================================
Control flow graph for Version 2
=================================

    [BB1]
      |
     \ /
 __\[BB2]
 | /  | \
 |    |  \
 |   \ /  \ 
 |--[BB3]  \> [BB4]


=========================================================
Important things to keep in mind about control flow graph
=========================================================
You have to have a cycle in your control flow graph if there is a loop. If you don't have a cycle then there is something wrong. The cycle indicates a loop
in the graph. 

======================================================
This is all that we need to know for our baby language
======================================================
This is all what you need to know in order to build our basic compiler for our baby language. These are all the features that we have. We will talk more about
boolean expressions or logical expressions, but it is something that we have previously talked about. 

==========
General condition example
====================
 st1 
while (a > b && c == d || e <= f) 
    st2

st3

=================================
Disussion on Boolean expressions
=================================
We have seen how to write grammar for a boolean expression. It is very similar to writing a grammar for an arithmetic expressions because: 
i) the && corresponds to (or maps to) in the arithmetic expressions as a MULTIPLY or a DIVIDE
ii) the || corresponds to + or -

We have actually written grammar for boolean expressions. So generating code for boolean expression is going to be similar to generating code for arithmetic 
expression. It is a similar idea.

==========================
Example boolean expression
==========================
a > b && c == d || e <= f 

====================================
AST for our above boolean expression
====================================

            (||) 
          /      \ 
                 (LE) 
      (&&)       /   \ 
     /    \     (c)  (f)
 (GT)     (EQ) 
 /  \     /   \
(a) (b)  (c)  (d)


This is something that you will do in the assignment. The same way that we wrote code for generating an arithmetic expression, the same thing applies to a 
logical expression, but you will have to take into account the different operations.

===============================
Abstract assembly for the above
===============================
loadAI rarp , @a -> r1 
loadAi rarp , @b -> r2 
cmp_GT r1, r2 -> r3 
loadAI rarp, @c -> r4 
loadAI rarp , @d -> r5 
cmp_EQ r4, r5 -> r6
and r3, r6 -> r7    //and instruction

loadAI rarp, @e -> r8 
loadAI rarp, @f -> r9 
cmp_LE r8, r9 -> r10

or r7, r10 -> r11  //or instruction 
cbr r11 -> L1, L2

==================
So what did we do 
===================
In this case, we had to generate the code for the condition, and the result of the code is in r11. So we will need to do all of the above. So basically, we
will write a recursive routine for generating code for a conditional, no matter how complicated that conditional is. And the result of that conditional 
will be stored in some register. And then you do a conditional branch on either L1 or L2. 
So we know how to write code to generate code for these basic constructs: 
i) Arithmetic expressions 
ii) logical expressions
iii) conditionals 
iv) while loops

So this is basically what we need to do in order to build a compiler.  Also, programming languages are recursive.

=========================
Visualizing a while loop
=========================

         (while)
        /      \
    (cond)     (body)




                            =======================================================================
                             Compilers Lecture 26: Code Generation for Conditionals and Loops (2)
                            =======================================================================

=======
Example
=======
st1 
while (r1 > r2) 
{
   st2 
   while (r3 <= r4) 
     st3 
   st4 
}
st5

===========================================
Compare instructions that we will be using 
===========================================
i) cmp_GT r1, r2 -> rx 
   cbr  rx, L1, Ll 2 

ii) cmp_LE

===============
Generating code
================
st1; 
L1-C: cmp r1, r2 -> r5  //r5, because we are assuming that we are using the GetNextRegister function. In the above code, you can see that the last register was r4.
      cbr r5-> L1_B , LI_O  //L1_B = loop body.  L1_O = Loop out . L1_C : loop condition 

L1-B: st2 

L2-C: cmp r3, r4 -> r6 
      cbr r6 -> L2-B, L2_O 

L2-B:  st3 
       jmpI L2-C

L2-O: st4
     jmpI L1_C
L1_O: st5


So this is an important part of the assignent. And remember that this is recursive. And you have to have a function to generate all this systematically. aND 
remember that this is based on the AST. 

     (St1)  //we have a next pointer for every node.
       |
     (While           )
       |    \        \
     (st5)  (cond)   (Body) 
                       |
                      (st2)
                       |
                      (while)
                       |   \       \ 
                     (st4)  (cond)  (body)
                                      \
                                      (st3)

In the assignment, the condition, can be an arbitrarily long and complex expression. So again, the condition can have a whole tree below it. Because it can 
be arbitrarily long and complex expression. And the body can be a statement list.  The next pointer in the while loop is a conceptually different pointer
than these pointers to what constitutes the while loop. These pointers are the pointers to the two components that make up a while loop. A while loop has a 
condition and a body. And the next pointer is a different kind of pointer. It is a pointer to what comes after it in the list of statements that it belongs
to. If you're programming this in an object oriented language (like C++) then this next pointer conceptually should not belong to the while node. It should
belong to any list element. So for any list element. But while as a while -- it has a condition and a body. How can you implement it? Well the easiest way is 
to put all of these possible pointers inside the AST node. So you can have an ASTNode and inside it, you can have everything in it. This would be the easiest
but it would not be the most elegant of structuring this. So the ASTNode can have condition, body, then, else, next. 
Ideally, an ASTNode should have ONLY the elements that belong to all nodes. And you derive from the ASTNode. You derive a WhileNode. You derive a dervied 
class for a WhileNode. And the while node will have a body annd a condition. And an IfNode can have condition, then and an else.  And an AddNode , you will
have to have expression1 (the lefthand side expression) and expression2 (second operand). So ideally you would liek to have a dervied class for each node type
that has the fields that are specific to this type. But if you put everything in one generic node then that will work (but it won't be an elegant design, though
we are writing it in C anyway).