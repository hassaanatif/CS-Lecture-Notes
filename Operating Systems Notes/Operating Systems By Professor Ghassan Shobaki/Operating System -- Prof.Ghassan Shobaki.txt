                            ============================================================
                                Operating Systems Lecture 0: Introduction and Syllabus
                            ============================================================

===================
Textbook
==================
"OS Concepts, 9th Edition by Silerschatz". There is also a 10th edition that is available. 

==================================
What is an OS and what does it do
==================================
It stands between the hardware and the software application. It is a piece of software that stands b/w the application programmar and the user and the hardware.
The user is using an application program and that application program gets services from the operating system. The applicaiton program is not only asking 
for the services from the operating system. In-fact anything that the application program needs, it is going to ask from the operating system. Or whenever
the application program needs to access a system resource, it has to go through the OS. The OS does the resource management. You can think of OS as a 
resource manager -- or it manages the resources. 

===========
Resources
============
And what are the resources that we are talking about? 
1. CPU (the main resource) 
2. Memory
3. I/O
4. Disk

======================
The importance of I/O
======================
A useful application will always need the help of I/O in one way or another. Because it will need to get some input and then it will have to give some output.
That output may be displayed on the screen or written onto the file. File access is also I/O. 


========
Process
========
The OS has a bunch of processes and a bunch of resources and these processes are requesting access to the resources and the OS is the manager that 
manages the machine and it decides which process gets which resource and for how long it is going to keep that resource.
The concept of a process is a very central concept in this course. 

================================================
Differentiating between a Process and a Program
================================================
A process is an ACTIVE ENTITY, and a program is a passive entity. So when we say MS-Word. This MS-Word is a program but when I start an instance of 
MS-Word then this is a process running. so when a program is running or executing or active then it is a process. Now if there are multiple instances 
of the same program running or active then each one of them is going to be a separate process. So if I start 3 different instances of power point 
then each one of them is going to be a process and from OS point of view, they are going to be different processes (if they are executing different 
paths within the code). 
So if you have PPT presesntation that is just in display mode that will be executing in a different path than the PPT presentation that is being developed.
Since these two processes will be different from OS's point of view therefore their access to IO will be different as well, despite being two different 
instances of the same program.

=========================
So this is what OS is...
=========================
It has all of these (a number of) proccesses that are active and in order for them to execute, they need resources. And they request access to the resources 
through the operating system. And the operating system does the management. We will see why it is a good idea to have the OS manage everything. Can you 
think of a good reason for having the OS manage access the resources as opposed to letting application programs just access the resources directly? Because
there will LOTS OF CONFLICTS! SO we need a manager. To manage all this and to resolve conflicts. And also to decide which part of the memory each program 
or each process can use. 

=============
In terms of CPU Access
==========
If I have a single CPU in the system, only one process can be active at any given point in time. You can't have multiple processes active if you have a 
single CPU. So which process should be active? Or which process should have the CPU? That is a decision that the operating system should make.

==========================
So how will the course go
==========================
We will spend most of the time studying the management of the CPU and in the last 4 weeks, we will have a couple of weeks on memory management and a couple of
weeks on storage management. THe rest of the course will be focused on different topics related to CPU management. This course focuses on the operating 
system from the inside. The design of the operating system. How it is built and what are the concepts that are used in designing operating systems. And 
what are the issues that the OS has to deal with. And what are the problems that the OS has to solve.
A big part of the course will go around answering the question "HOW". And when we say "How" in CS, the answer to the this question is usually an "algorithm".
So basically this course will be a connection of algorithms that the OS use to does its job (which is resource management). So a bunch of a algorithms for
managing the CPU. Some algorithms for managing the memory. WE will not look very closely into the IO. WE will talk about the IO when we talk about the CPU.
The DISK, we will be studying different algorithms for disk management. 
In chapter 3, we will talk about concept of proceesses and the different operations that are applied to a process. In chapter 4, we will look into a thread.
Then we will talk about process synchronization -- when you have multiple processes running in parallel, you need a mechanism for synchronizing them. It is 
a challenging task and that is why, this topic is the MOST INTELLECTUALLY TOPIC IN THIS COURSE, because you will have to think in parallel. 
In cahpter 6, we talk about CPU Scheduling. It is about deciding which process gets which resource. In this case, the resource that we refer to is called a 
"CPU". So it is basically the management of one specific resource called the "CPU". It is the operating system receiving requests to access the CPU. Because
any program needs the CPU. All proccess are requesting access to the CPU, the OS then decides which process gets the CPU and for how long (that is if you have
a single CPU). But if you have multiple CPUs then the OS decides which process gets which CPU and for how long it is going to get that. SO the OS must make 
all processes happy. It can't just give CPU for one process and forget about all other processes. The OS has to be fair. And there are DESIGN OBJECTIVES of
an OS. The OS must be fair, it should give good performance, good user experience etc. And then in chapter 7, we talk about deadlocks. Deadlock is a problem
that may occur when you have parallel programs and you are trying to synchronize them.  Then in chapter 8 and chapter 9, we will do memory managment and then
in chapter 10,11,12, and 13, we will do storage management.


==================
The term "Running"
==================
When we use the term "running" in this course. When we say that a process is running, we mean to say that is on the CPU.

==============
Thread concept
==============
The process is a bigger concept. A process may be divided into multiple threads. The thread is a lightweight process or a subprocess. 


                            ============================================================
                              Operating Systems Lecture 1: OS Introduction (Part 1)
                            ============================================================

==========
Overview
==========
This is chapter 1. This cover the intro to operating systems. 

============================
What is an operating system
============================
There are different ways to define an operating system. One definition is that it is an intermediary between a use rof a compuer and the computer hardware.
The OS executes programs and makes the computer convenient to use. Convenient for the user and the application programmer. Because the application programmar
can use many services that are provided by the OS, through the SYSTEM CALLS or APIs. And we will be talking about system calls and APIs in greater detail in 
chapter 2. And these system calls provide many services to the application programmaers. So the application programmars can make these calls instead of 
actually implementing these services. And in-fact, in many cases, an application programmar is NOT ALLOWED to access devices directly. 
Also one of the important objectives of an OS is managing hardware resources. So what are the different components of a computer system? We have a hardware. 
And we have an OS on top it. And wwe have the application programs running on top of OS. So the OS stands between the application programs and the computer
hardware. So applications program run on top of OS and use the services provided by teh OS and the OS manages access to the computer hardware (or to the 
hardware resources). So if multiple applications are requesting access to the same part of the resource then the OS will manage that (or it will resolve
the conflict in a good way). So most of this course is learning about how the OS will resolve these conflicts among the application programs when they 
request access to the resources. 

=======================
The compiler and the OS
=======================
The compiler uses the OS which is true. But in fact this is not always true. It is true that the compiler uses the operating system but it is also true that
the operating system uses the compiler. So it goes both ways. So the relationship between the compilers and the OS goes both ways.

========================================================
So the question is, when does the OS use the compiler? 
========================================================
If we are talking about a language like Java that has a JIT compiler then yes. But here the Operating System invokes the JUST IN TIMEcompiler. It does not
use it. It is not that the OS itself needs the compiler. It is starting the JIT compiler for that particular programming language. 
But when does the OS need a compiler? When does an application need a compiler? When it is compiled. If it is written in a compiled language then the 
application is compiled by the software vendor (who makes the software). If it is not an open source software all that you get is the binary that the
software vendor has generated using a compiler. Now the question is? In which language are Operating Systems are written in? Most are written in C and 
C++. So if the OS itself is written in a high level language then the OS itself needs a compiler to compile its code. SO the OS doesn't need the compiler when
it is running, but the people who make the OS need the compiler to compile the OS source code into MACHINE CODE. So what you load on the machine is the 
OS machine code that has been generated by a compiler. So basically the OS machine code has been generated by a compiler.
=======================
Compilation of Windows
=======================
Windows is a closed source OS, so who compiles it? MS does. So what you get is the binary. You don't need a compiler on your machine. They need the compiler
on their machine when they develop the OS and they just give you the binary which is the machine code. So a CD with MS windows on it has the binary (executable
) written onto it. 

============
Compilation of Linux
============
But when you get an open source operating system (like Linux) then you actually get the actual source code of that operating system. And usually it is the 
system administrator who compiles the operating system source code. So even if it is an open source OS, you may get the binary, but another option is getting
the source code of an OS and compile it yourself. 

=============
The design of OS
============
The design of an OS is going to depend on the nature of the system that this OS is going to run on (or is going to manage. Because the OS manages the system).
WE have different kinds of systems like: 
- Mainframes
- Servers
- Workstations 
- Handheld devices
- Embedded devices

And each one of these has its own specific needs and requirements, which reflect on the OS as design objectives. 

===============================================
Design objectives example of a server and a mobile 
==============================================
So an OS on a server has different design objectives to ensure fairness among users. It has multiple users and making all users happy is an important design objective of an OS that runs on a server
but if it is an OS that is running on a mobile device then that mobile device is going to be run by one user and there are other important issues when it
comes to designing an OS for a mobile system. Issues such as making good use of poor or limited resources on mobile device. And issues like the battery life.
We would like to minimize the usage of energy so that the battery can serve for a longer time. So the point is that different systems have different 
objectives.

=============================
Embedded Systems requirement
=============================
Embedded systems have their own specific requirements. What is an embedded system? It is an embedded computer. It is part of another system like a car, 
refrigator, medical device etc. So those computers that control other machines or devices are called "embedded systems". And obviously, they are everywhere.
They are in every machines or devices that we use. And obviously, in these small embedded computers, the requirements are different. These are basically
controlling the machine, they are not interacting with the user. So we do not have the notion of user interface. It is basicallly machine interface. They
are basically interfacing with a machine and controlling the machine. So they have different nature and different requirements.

==================================================
OS as a Resource Allocator  and a Control Program
==================================================
So we can think of the OS as a RESOURCE ALLOCATOR or a as a control program. A resource allocator because it manages all the resources and it decides between
conflicting requests for efficient and fair resource use.  As a control program, it controls the execution of programs to prevent errors and improper use
of the computer (protection).
So OS as a manager resolves conficts and provides protection. A very simple example of protection is memory protection (this is something that we will be 
studying in greater detail in the Memory Management chapter), but we all know that the OS has the job of allocating memory. So if I have processes like: 
P1, P2 etc. 
Then P1 has a certain chunk of memory allocated to it then P2 has a different chunk of memory allocated to it. And then there is the OS kernel. Now what if 
P1 attempts to access a memory location in P2 then this will definitely cause a problem. It may override some data or code in P2. And what is even worse
is that if the P1 attempts to write the Kernel itself then it is something that we should not allow. And it is the job of OS to protect (memory protection)
the system from buggy application. So P1 is buggy and is trying to access memory that is not allocated to it (the memory that belongs to other user processes
or to the Kernel) then we use should protect the system from this. And how is this protection implemented? It is implemented with HARDWARE SUPPORT.

========================================================================================
How does HARDWARE SUPPORT work -- Exceptions and Interrupt and Interrupt Service Routine
=========================================================================================
This is something that we will be studying later in the memory management part. An exception will be generated and the hardware will be trapped to the 
OS. How will that happen? Because when an execption gets generated that will involve an ISR (Interrupt Service Routine). An exception is an interrupt. It is
one kind of interrupt. And for each interrupt, there is an interrupt service routine that services the interrupt. And that Interrupt Service Routine belongs
to the OS. So whenver something wrong happens, if the hardware supports protection then it is going to give control to the OS kernel. It will call the manager
and the manager will resolve the conflict. 
So in our example, P1 is trying to access memory location allocated to P2, what will the manager do in this example? It will just terminate process#1. The 
process#1 is not a well behaving citizen in the community. And this will prevent the system from crashing. 

==========
DOS 
==========
DOS does not support this kind of memory protection. So DOS is an OS that did not provide protection and if your application is trying to access memory 
that does not belong to it then that may lead to seeing all kinds of bad behavior like seeing all kinds of garbage on the screen or even SYSTEM CRASH.
So on DOS, if you do, in C, something like: 
       int *ptr; 
       *ptr = 5;
The above example may crash the system. WHY? Because you could be rewriting a different piece of memory. Our pointer is not initialized which means that can
be pointing to anywhere. We are not initializing the pointer, we are not making it point to a valid memory address, so it is pointing to a random address,
and that random address could be somewhere in the Kernel itself. And if we rewriting 5 to some random place in the Kernel then we are overwriting the Kernel
code and then the system crashes. At that point, you might have to reboot your machine. 


=======================
So what is the Kernel? 
=======================
It is the main program of an OS. Or the central piece or the essential piece of an OS. The Kernel is the operating system that is always loaded in memory
that may load other OS components or modules.

==================================
All the modern OS
==================================
All the modern OS support protection and they do not allow this. But remember that you need hardware support. And the earlier version of Intel did not have 
this support for protection. SO support for protection remember that when that exception gets generated, the control is giving to the Kernel and the Kernel
handles the situation by terminating the process. 
Nowadays, Operating Systems ship with many many different pieces of software which we call system programs but some OS have programs that are not system
programs like games and calculators etc that ship with the OS. Clearly these are not part of the Kernel. These have nothing to do with the main functionality
of the OS. So these are things that you get as part of the package. But the essential part is the Kernel. It is always in memory. 

=====================================================
What does "Running" mean. And what does "ACTIVE" mean
=====================================================
In-fact running at all times on the computer, this is not strictly true, because we will define running at some point. So running would mean "has the CPU".
Suppose that you have a single CPU, only one program can be running at a given point in time. So initially, it is the Kernel that is running. Then the Kernel
is going to give the CPU to some other process. So now when the process#1 is running then process#1 gets the CPU. The kernel is not running now. The term
running in this course means "running on the CPU" or "has the CPU". The Kernel is not running, but it is active. It's loaded in memory and it's waiting for
an interrupt to happen as we will explain in detail in the next lecture.

========================================================
The desirablity for minimization of Kernel Intervention
========================================================
It is desirable to minimize the time that the Kernel spends on the CPU. We would like the CPU to serve the user program. We would like the 99.9% of the CPU
time to be given to application programs and not to the Kernel. 
So you know the Kernel is the manager and that manager manages or controls the system with minimal intervention. 

==========
Bootstrap
===========
So the bootstrap program is the program that is iny any machine or any hardware. The Bootstrap program is stored in the "read-only-memory" or the firmware
(aka EPROM).
And it will be the very first program that will run when you boot the system. When you boot the system, the bootstrap program runs and what does that 
bootstrap program do? It will do some initializations to initialize certain things in the system and then it will load the Kernel and once it loads the 
Kernel, the kernel now is in control and we will see next time, what is the mechanism that the Kernel will use to give the CPU to the processes and then
get it back. The Kernel has a mechansim to get the CPU back after giving it to some process. And it does this by setting up a "TIMED INTERRUPT" and this 
timed interrupt that will give the CPU to a process for a limited period of time (NOT FOR FOREVER). That is how the Kernel stays in control, by setting a 
limit on the time that is given to each process on the CPU. 

                            ===============================================================================
                            Operating Systems Lecture 2: OS Introduction (Part 2): CPU, I/O and Interrupts
                            ===============================================================================

==============
Lecture Intro
==============
Most of today's lecture is going to be about the interrupts and how the interrupts are used to drive and operate the system. So last time, we started 
talking about the I/O devices. Each I/O device, like a keyboard has a controller that controls that device and each device has a "LOCAL BUFFER" for 
storing the data that that device will use or that device will consume.

==========
Diff b/w a device controller and a device driver
========
The driver is a software and the controller is a hardware. So the device controller is the electronic part of the device. It is an electronic circuit that 
controls the device. It can be viewed as an "INTERFACE" between the device (which could be a mechanical device like a mouse, or a keyboard) but it has an 
electronic piece that interfaces with the computer. So that electronic piece is the device controller. 
And the device driver is a piece of software that belongs to the OS and it talks to that device controller. So it is between the  operating system and the 
device controller. 

======================
Example of a keyboard 
======================
Now suppose the device is a keyboard. It is going to write the data into some local buffer. Now the data is written into the local buffer by the I/O device 
itself. But at some point, we need to move that data from the local buffer to the main memory. And in particular we need to move it from the local buffer
to the memory location that belongs to the process that requested that I/O service. So then this data will be transferred from the local buffer to the 
main memory (the address space of a certain process). 

=======================
Who does this transfer?
========================
This transfer is done by the CPU and not by the I/O device, and it is something that the OS does.

=============================================================================
How does the OS (or Kernel) know that an I/O device has completed a certain request?
=============================================================================
The answer is that an I/O device will generate an interrupt to notify the OS that it has completed a certain request.

========================================
Timing Diagram and talk on CPU Scheduler
========================================
When the system starts, the Kernel (after the bootstrap program) is running. Then the Kernel will select a process to get the CPU. So how does it select 
a process? It invokes what we call a "CPU scheduler" (a very important part of OS). For now we only need to understand what the CPU scheduler does. And 
what it does is select one process to get the CPU next. How does it select that? It selects it based on a certain scheduling algorithm. And we will not
study scheduling algorithms until we get to chapter 6. So in chapter 6, we will be studying different scheduling algos and policies. So we will understand
the "how" later on. For now, we need to understand the "what". What does the CPU Scheduler does? It selects a process to get the CPU. So according to 
certain policies or algorithms to get the CPU. Suppose, it selects Process#1 then our timing diagram looks like this: 
 
               CPU [[Kernel] [P1]]

So process#1 is now running on the CPU. Typically a process will not be doing CPU only. It will need to do I/O at some point, because it needs to get a 
user response from the keyboard or the mouse, or it needs to write something to the screen or it needs to access a file. So it is hard to imagine a process
that doesn't require I/O. So it will request I/O from the Kernel. And that request is made via a SYSTEM CALL. So that process that is running on the CPU
is going to make a system call. Obviously, a system call means that you are calling the OS (or the Kernel) to do something or to perform a service for you.
And one of the most important services is the I/O. Accessing the I/O or making an I/O request. The SYSTEM CALL is implemented via an INTERRUPT. So basically
this process will make a system call via an interrupt. Now the Kernel is in charge again. Now, something to keep in mind is that whenever an interrupt is 
generated, the control is given to the Kernel. 

==========================
Interrupt service routine
===========================
When an interrupt is generated, what will be invoked is an "Interrupt Service Routine". For each interrupt, there is an interrupt service routine that 
services that interrupt.

=================
Interrupt Vector
=================
It is basically a look up table. That is indexed by the INTERRUPT NUMBER. And for each interrupt number, there is a corresponding ISR (Interrupt Service 
Routine) that services that interrupt.  

=====================
Interrupts and Kernel
=====================
And these interrupts belong to the Kernel. Whenever an ISR is invoked, you are basically giving control to the Kernel.

===============================================
What will the Kernel do when an ISR is invoked?
===============================================
It will satisfy that I/O request by sending the request to the corresponding I/O device. Now to send a request to the corresponding I/O device. See the 
timing diagram now at this point: 
        CPU [[Kernel][P1][Kernel]]
        I/O               []

What is happening above? This is the I/O device, at some point the request will be sent to the I/O device. Now the I/O device may or may not be available 
immediately to process that request because that I/O device typically has a Queue of requests. So there may be other requests that are currently in the 
queue or that are waiting to get serviced. So the I/O device has a queue. The queue may have other requests in it or it may be empty. 
So assuming that the queue is empty (as depicted in the above timing diagram) then that means that there are no other requests and thus this request will 
get processed by the I/O device at that point.
        CPU [[Kernel][P1][Kernel]]
        I/O               [P1]

So now the I/O device is servicing P1, then now the Kernel is done (it has sent that request to the I/O device). Now the I/O device is going to take some
time to process that request, and we expect I/O device to be SLOW (or to take some time). So during this time, the Kernel is NOT JUST GOING TO WAIT. It is 
not going to waste system resources waiting for this I/O request to get completed. So what do you think the Kernel will do at this point? It cannot give the 
CPU to P1 again because it WILL NOT WORK. Why will it not work? 

===========================================================================================================
Is a certain process P1 is being serviced by the I/O, can the Kernel give the CPU to P1 during this time?
===========================================================================================================
It cannot do that becaues it will not work. What is an I/O? An I/O is like reading from the keyboard. Your process#1 is reading from the keyboard. 
         
      read x 
      y = x+5;
      --- 
      ---
I cannot go and execute things that are after this read operation because I am reading x. And then I am using x. So until I read "x", I cannot execute 
y =  x+5;
And in-fact, Operating Systems never re-order the instructions in a given process. The OS is not in the business of reordering instructions within any 
given process. The OS just executes a program in-order.

====================================================
So what does the Kernel do during this waiting time?
=====================================================
The kernel will have to find another process to give the CPU to. Usually there are multiple processes. So it will find a process like P2 and it will give
it the CPU. 

        CPU[[Kernel][P1][Kernel][P2]]
        I/O               [P1]

Now what is interesting here is that if we take a snapshot at this point, we will see that multiple processes are making progress. We are using two resources.
P2 is making progress on the CPU. P1 is making progress on the I/O device. Now what is the state of each process? With the operating system terminology, P2
in this case is in the running state. So running means that you are on the CPU. So P2 is running. And P1 is waiting. 

Process   State
P1        waiting
P2        running 

Now even though P1 is waiting, it is actually making progress but according to the terminology that we will be using in this course, this process is waiting
because it is not on the CPU and it is basically waiting for the I/O device to complete processing this request. But just the point here is that if we say
that a process is waiting it doesn't mean that it is wasting time. It could be that it is getting serviced by the I/O device. 

        CPU[[Kernel][P1][Kernel][P2]]
        I/O               [P1]

So basically this above timing diagram is one form of concurrency between the CPU and the I/O device. Now what will happen next? Eventually the I/O device
will complete processing this request.

====================================================================
What will happen when the I/O device completes processing a request?
====================================================================
It must notify the kernel that it has completed processing a request. And it must do that by generating an INTERRUPT. And whenver an interrupt gets generated
who gets control? The KERNEL. So for simplicity's sake, we are assuming that P2 is the process that has kept CPU for this long, but this is not always true.
The CPU may have been given to some other process from P2 during this time. But anyways, for simplicity's sake, we are assuming that P2 has kept the CPU for
this long.
        CPU[[Kernel][P1][Kernel][P2][Kernel]]
        I/O               [P1      ]

Anyways, now that the Kernel gets the CPU after the interrupt is generated by the I/O, what is the logical thing that the Kernel should do in this case? It 
will do scheduling BUT BEFORE THAT it will have TO COPY THE DATA FROM THE I/O device to the main memory. It is basically the I/O device saying: "OK I am 
done with this request. I am ready. I have the data in my local buffer, so come and pick it up".  So you have to pick the data up from I/O device's local 
buffer and put it in main memory and put it in the location that belongs to the process that is requesting that I/O. That is the job that the OS will have 
to do. It will have to make the data available to the process that requested the I/O. Then after doing this, it will mark PROCESS#1 as READY. Ready means 
"READY TO TAKE THE CPU".

=========================================================================================================
What happens after when the data is put into the location of process requesting I/O inside main memory?? 
=========================================================================================================
That certain process (in our case Process#1 or P1) will be marked as "READY" by the Kernel. Ready means "READY TO TAKE THE CPU". Who decides whether it will
get it or not? The scheduler. So being ready means that it will be one of the candidates that the scheduler will consider for giving the CPU next to. Also,
the CPU is not necessarily given back to P1 after all this because who gets the CPU is based on the scheduling policy. And based on the scheduling policy
P1 could be a lower priority process. P1 may or MAY NOT get the CPU. It may get the CPU or may not get the CPU. It all depends on the scheduling algorithm
that is being used. 
       
==========================================
Now that the I/O has completed...what now?
==========================================
Now that P1 is labeled "READY". Now based on certain scheduling algorithm's criteria that we will be studying later in chapter 6, it will decide what to do
next.

==================================================================
What if there are multiple processes requesting the same resource?
==================================================================
The I/O device has a queue, which means that multiple requests can be sent to the I/O device. It will queue them. And it will service them one at a time, in
order. When it completes one of them, it will generate an INTERRUPT. So there is an interrupt for each request. So when the I/O device notifies the Kernel
that it has completed a request (it is just one specific request that belongs to one specific process) then there may be other requests in the queue and they
will be serviced one request at a time.

========================================================================
What happens according to the diagram if P2 decides to access the I/O?
========================================================================
Then it will make a system call. So now there will be a different diagram. At some point, there will be another I/O request. So the Kernel will be in-charge
and it will send a request to the I/O device. Now if it sends the request to the same I/O device as P1 or a different one can make a difference. If it sends
it to the same I/O device, what do you think will happen? It will be put on the Queue, because that I/O device is already busy servicing P1 and it is atleast
not ready now. 
But suppose that it sends it to another I/O device and if that I/O device happens to be available (meaning its queue is empty) then maybe that request for
P2 may get serviced immedaiately.

       CPU[[Kernel][P1][Kernel][P2][Kernel]]
        I/O               [P1             ]
        I/O#2                      [P2...

And then whichever request completes first, it is going to generate an interrupt. So if P1 started first then it is going to notify the Kernel that is done.


================================================================
If both P1 and P2 are getting serviced, what will the Kernel do?
================================================================
It will find another process to give the CPU to. So it may give the CPU to process#3.


=========
Is CPU ever in-active?
=======
When the Kernel doesn't find a process that is ready then the Kernel will keep the CPU. This is known as the "Idle state". This is not something that we want.
In general, we would like to minimize Kernel intervention. The CPU is to service the processes and the Kernel is the manager. And that manager should 
minimize the time that it spends on the CPU. It is not useful time. The Kernel time, from an application's point of view, is not useful time. So the Kernel
should manage the system with minimal CPU time. 


==========================
Two of types of Interrupts
==========================
1. Interrupts that are used by the I/O device to notify the system that it has completed processing an I/O request
2. Interrupts that implement System Calls

=====================================
Other types of Interrupts: Exceptions
=====================================
In addition to the above two, there are other kind of interrupts such as the "Exceptions". So exceptions are generated for different reasons. Exceptions
may be generated if a process does something wrong or we do an invalid operation such as:
1. Divide by zero. So this will generate an exception. 
2. If a process runs into STACK OVERFLOW. 
3. A floating point overflow. 
4. It executes an invalid operation.
5. Out of bound memory access (we won't fully understand how these type of exceptions are caught until we get to the memory management chapter -- which is
   chapter 8 and chapter 9). 

When an exception is generated, Kernel gets the control and the Kernel will then terminate that process in order to protect the system.

                            ===============================================================================
                             Operating Systems Lecture 3: OS Introduction (Part 3): Computer System Review
                            ===============================================================================
====================
Storage structres -- FROM FASTEST TO SLOWEST
====================
- Register
- Cache 
- Main memory
- Solid-state disk 
- Hard disk 
- Optical disk 
- Magnetic tapes

In this course we will talk about memory management and then we will talk about disk management later. 


=========
Caching
=========
It is storing a subset of the data that we have in a slow and a large device. And what you store in the cache is what you expect to need more frequently. 
Caching happens at multiple levels in a computer system. It even happens in the web browsers, where you cache the web pages that the user accesses most
frequently. You make it more easily accessible by putting them in a more faster storage device or by putting them in an easy access place.  
Cache management is an important design problem. Cache size and replacement policy. 


============
Multi processor systems
=============
We will talk about multiprocessor scheduling when we get to scheduling (which is chapter 6: CPU Scheduling). Most systems uses a single general purpose 
processor. Most systems have special purpose processors as well (such as multiprocessor). Multiprocessor systems growing in use and importance. 
Multiprocessor systems are also known as "parallel systems" or "TIGHTLY COUPLED systems".  Advantages include: 
1. Increased throughput 
2. Economy of sacle
3. Increased reliablity - graceful degradation or fault tolerance.

=========================
Types of multiprocessors
=========================
1. Assymetric multiprocessing -- each processor is assigned a specific task. 
2. Symmetric multiprocessing -- each proessor performs all tasks.

================================
Classical Multiprocessor system 
================================
 
   ------      ------        ------
    CPU0        CPU1          CPU 3
   ------      ------        -------
    -registers  -registers   -registers    
    -cache      -cache       -cache

       |           |            |
     --------------------------------
                MEMORY
     ---------------------------------

So here is a classical example of a multiprocessor system where we have 3 CPUs and each CPU has its own register and its own cache but they are all sharing
the same memory. And this can be done on a same chip on a multicore system.

        ==================================
      |   CPU Core0      CPU Core1       |
      |    registers      registers      |
      |     cache         cache          |
      |                                  |
      ------------------------------------
                   |
         -------------------
         |     Memory      |
         -------------------


So above, you have one chip that has two CPUs on it. Core0 and Core1 sharing same memory.


                            ========================================================================================
                            Operating Systems Lecture 4: OS Introduction (Part 4): Multiprogramming vs Time Sharing
                            ========================================================================================

==========================================
Intro: High Level view at Multiprogramming
===========================================
This is an important OS concept. In both cases, the OS has multiple processes to handle. In a multiprogramming system, the OS is going to give the CPU to
one process. And it will wait until that process requests IO. Or has to go into the waiting state for other reasons (we will be covering these other 
reasons later on, but for now the only reason that we have covered is requesting IO). So if a process goes into the waiting state, the OS is going to 
give the CPU to another process, that is MULTIPROGRAMMING. 


======================================
Intro: High Level view at Timesharing 
======================================
Time sharing means that the Kernel is not going to wait for a process to request I/O or go into the waiting state. The Kernel will keep switching processes,
even if a process doesn't need to wait, the OS is GOING TO TAKE THE CPU FROM IT and it is going to give it to another process.

================================================
Detailed info on multiprogramming (Batch system)
================================================
It is needed for efficiency. 
- Single user cannot keep CPU and I/O devices busy at all times 
- Multiprogramming organizes jobs (code and data) so CPU always has one to execute.
- A subset of total jobs in system is kept in memory. 
- One job selected and run via job scheduling 
- When it has to wait (for I/O for example) OS switches to another job.

================================================
Detailed info on TimeSharing (multitasking) 
================================================
- It is a logical extension in which CPU switches jobs so frequently that users can interact with each job while it is runnijng, creating interactive 
computing. 
- Response timem should be < 1 second 
- Each user has at least one program executing in memory => process
- If severeal jobs ready to run at the same time => CPU scheduling 
- If processes don't fit in memory, swapping moves them in and out to run 
- Virtual memory allows execution of processes not completely in memory.

=====================================================
Timing diagram for Multiprogramming and Time sharing
=====================================================
Multiprogramming: 
     CPU [[Kernel][P1     ][K][P2.....]]
   
TimeSharing: 
     CPU[[Kernel][p1][Kernel][P2][Kerne][P1][Kernel][P2]]

So time sharing is switching between processes. We may have many processes and we don't know which process gets the CPU next. Which process gets the CPU 
next is a scheduling decision.

==================================================================================
Why would a CPU do Time-sharing? Or why would it keep switching between processes?
===================================================================================
It is about "user experience". So the user here will feel that multiple processes are making progress at the same time. So if we just assume that all of 
these processes belong to the same computer. Like you have your MS-Word open, and you have your browser active and at the same time you have your email
active and your email receives messages and you get notified when you are typing. So there are multiple processes. So when this is happening, the user is 
getting the feel that multiple processes are making progress. Even though, strictly speaking, only one of them will be running (or be active on the CPU) at
any given time but the user will get the feel or the illusion that multiple processes are running at the same time. So this is MORE RESPONSIVE. This system
is going to be more responsive to the user. 

===================================================
But saying that TimeSharing is "faster" is NOT true
===================================================
This is not faster in fact. So when you say that which one is faster? So you have to define "faster" here. Faster is not well defined here. So I can argue
that TimeSharing is faster from a user point of view. But I can argue that multiprogramming is faster. How? What is the argument? If you ask what is the 
total time needed to execute a given set of processes? So the total time needed to finish them, assuming that you can always overlap CPU with I/O. Assuming
that I/O doesn't take a long time. Asumming that when P1 goes to I/O and P2 gets the CPU and when P2 requests I/O again, the I/O for P1 is done. Then 
assuming that the I/O deson't take a long time here all processes will finish faster.  Why? Because we have spent less time on the Kernel. Or in other
words, Kernel has spent less time on the CPU. You can think of Kernel time as wasted time. So assuming that the I/O is fast enough then we will need 
less time i.e the total time to execute both processes is going to be less. 

============================================
Thinking of "fast" in terms of "Throughput"
============================================
What's the total number of processes that you can complete executing within a given period of time. If I give you one minute, what is the total number  of
processes that you can finish in that time. Obivously, in multiprogramming you will be able to complete more processes. Again, assuming that the I/O is
fast enough of the I/O can always be overlapped with CPU.

============
Conclusively
============
From throughput point of view, Multiprogramming is better. From user responsiveness point of view, Time sharing is better. That is why the word "faster" is 
not welll defined.

=====================================
User interaction on multiprogramming
=====================================
This is definitely less interactive and if this process does not request the I/O for a long time then the user will feel that the SYSTEM IS SLOW.

====================================================================
Big Question: How does the OS implement Time Sharing? Timed Interrupt
=====================================================================
The OS needs a mechanism for setting a limit on the TIME that process#1 gets. So initially, Kernel has the CPU. Now when it gives the CPU to process#1, 
it needs a mechanism to get the CPU back after a specific or a certain time period that is determined by the Kernel. So the Kernel wants to say that, "OK, 
I am going to give the CPU to process#1 for 10 milliseconds and after 10 milliseconds, I, the Kernel, want the CPU back". So how does the Kernel implement
this? 
This is implemented using a TIMED INTERRUPT. Remember that whenever there is an interrupt, the Kernel gets control. So, the Kernel is going to set up a 
timed interrupt, an interrupt that gets triggered after a certain period of time that is determined by the Kernel. So the Kernel is going to set a 
certain counter. Of-course, it is going to be in binary. So the Kernel is going to put 10 milliseconds (in binary) inside the counter and then it will
get decremented with the SYSTEM CLOCK. So whenever the clock ticks that amount of time, that period is going to get decremented from what is in the 
counter and then eventually that counter will reach 0. 

============================================
What happens when the counter reaches zero?
============================================
When the counter reaches zero then an interrupt gets generated. And when that interrupt gets generated, the CPU is in control again. So this is 
extremely important. Otherwise, the Kernel will not have control. So this is a very important mechansim. This will allow the Kernel to determine
how much time each process spends on the CPU. And obviously, this ability to set up this timed interrupt (or setting the value of this counter), obviously
this previllege should be given only to the Kernel. A user process should not be allowed to touch that timed interrupt. And that is why, this requires
"Hardware Support" by supporting "DUAL-MODE OF OPERATION". So the hardware must have a way of distinguishing between a Kernel and a user process. And that
requires hardware support and this is implemented using the dual mode hardware feature.

===========================
Dual Mode Hardware Feature
===========================
This dual mode hardware feature is implemented by a mod bit. So there is a certain bit that indicates whether we are Kernel mode or in user mode. If 
we are in Kernel mode then the Kernel is allowed to execute some PREVILLEGED INSTRUCTIONS. Among these instructions includes the instuction that set up
this timed interrupt. If you are in user mode then you cannot execute the previleged instructions.

==============
Interrupts and timers
==============
Timer prevents infinite loop / process hogging resources
- Timer is set to interrupt the computer after some time period 
- Keep a counter that is decremented by the physical clock 
- Operating system sets the counter (previleged instruction) 
- When counter reaches zero, generate an interrupt
- Set up before scheduling process to regain control or terminate program that exceeds alloted time.

====================================================
All kinds of interrupts that we have studied so far
====================================================
1. Exceptions 
2. I/O completion 
3. Timed interrupt 
4. System call (they can be made for many different reasons. Including the normal termination of a process. For example: exit() is a system call).

Also "process completition" is a special case of a more general kind of interrupt. 

===================
More on dual mode
=================
Dual mode operation allows OS to protect itself and other system components 
- user mode and kernel mode
- Mode bit provided by hardware 
  - Provides ability to distinguish between user code and kernel code 
  - Some instructions designated as previleged, only executable in kernel mode
  - System call changes mode to kernel, return resets it to user 
- Lack of Hardware-supported dual mode can cause serious problems. Example: DOS on Intel 8088 
- Some modern machiens support multiple modes (e.g third mode for virtual machine manager).

===================================
Transition from user to kernel mode
===================================

------------
user process
-------------
|  User Process executing -> calls system call                   return from system call        user mode (mode bit = 1)    
|                                           |                      / \
___________________________________________\|/______________________|
------                                trap mode bit = 0             |
Kernel                                            \> execute system call                            kernel mode (mode bit = 0)
------

=========================
WHat is happening above?
==========================
We have a user process that is executing in user mode. When it makes a system call then the mod bit is changed to 0 and now we are in Kernel mode. The 
kernel is executing. When the Kernel is done, it is going to set the mode bit back to 1. So basically whenever the Kernel gives the CPU to a process, it 
must set the mode bit to 1 to indicate that it is the user that is running on the system, so the hardware does not allow a user process to execute 
previleged instructions (THIS IS A KEY IDEA).


                            ========================================================================================
                             Operating Systems Lecture 5: OS Introduction (Part 5): The OS is Interrupt Driven
                            ========================================================================================

=========================
An example of exceptions 
=========================
When you try to run a program that is compiled for an ARM machine on an intel processor. So you are trying to execute an invalid binary for a particular
machine, so that will get caught as an invalid operation. Now when these exceptions occur, what happens? The OS gets control and it terminates the 
process that is trying to do something wrong and that provides protection.
Also, in particular, if a process is trying to do an "out of bound memory access" then it may crash the whole system.


=================
Killing a process
==================
It is legal for an operating system to kill a process when an exception gets generated. Becuase killing the process will not allow it to crash the system.

====================
Time quantum/quanta
====================
That period of time that the OS gives to a process on the CPU is called a "time quantum". A process will get enough time quanta to complete but how long
is this time quantum is determined by the OS. In timed interrupt, we switch between the processes.

=========
Are timed interrupts common?
=======
Yes, they happen very frequently because they will allow the OS to switch between processes. Otherwise one process may keep the CPU forever. In a typical
system that we use everyday, there are multiple processes running at the same time. And that is time sharing and without time interrupt the OS will not
be able to implement that. 

========
What is the purpose of switching between the processes?
======
1. User responsivenss 
2. Fairness to the users. Because these different processes may belong to the same user or to the different user. Think of a server. An operating system
   on a server and multiple users are using that server. Like if you are using an athena machine and 10 other students are using athena at the same time\
   so in that case, if it is not heavily overloaded, you will not feel any lateness. You will feel like the machine is all yours.  You will feel like 
   you are the only user on the machine, but you are not the only user on the machine. There are other users and the OS is just switching between the 
   users so frequently that each user gets the illusion that the machine is all theirs. So this is the point in doing this kind of switching.


=======================================================
Are timed Interrupt good for terminating infinite loops??
=======================================================
So if we have a program that does this: while (1) {---}
It doesn't have a break in it and it will loop forever. Now what does a timed interrupt do in this case, how does a timed interrupt interact with this. 
Will the timed interrupt terminate the loop? 
Suppose that a time quantum that is given to each process is 10 miliseconds. So the system will give this above process 10 miliseconds. And we can 
assume that during the first 10 miliseconds given to our above process the system is going to execute part of the loop. Then the OS is going to 
switch to another process. And it may switch to 10 or 12 other processes before it gives the CPU back to this process. And now this process is going to 
process the other portion of the above while loop. So if in this example, we assume that the body of the loop take 20 miliseconds, the process is going 
to take 2 time quantum. But then in the next time around, it will get another time quantum and it will execute the first portion of the loop again. 
SO the point is that the OS are not even aware of INFINITE LOOPS. OSs don't have a mechanism of detecting infinite loops and the operating system doesn't
care if a program is in an infinite loop because as long as the program is behaving well, the OS doesn't care. THere are programs that run for days, 
or even weeks or months. YOu can start your web browser today and you leave it running for weeks. It is typical. For OS point of view, the program is 
still running and the operating system is not going to terminate it as long as it is behaving well. 
So the OS is not like a human being that may get sick or tired of a program and say that this program has been running for a long time and i am going 
to kill it. As long as the process is behaving like a well behaved citizen and it is not doing anything wrong, the OS will keep giving it time quantum
forever. But now, the point here is that when the OS keeps switching between processes and our process will be kept stuck in an infinite loop, whether 
for a good reason or a bad reaosn, it doesn't matter. Other processes are making progress as well. And in fact whne we get to scheduling, we will see
how an OS will try to give more CPU time to processes that are more active or interactive. A process that is doing more activity or interacting with the
user is going to get more attention from the OS and it is going to get more CPU time and we will see that when we get to CPU scheduling. 


===========
Summary
===========
The OS doesn't detect an infinite. It doesn't prevent an infinite loop. But what it does is prevent the slowing of whole system or killing the systme. Other
processes are making progress as well.

================
System interrupt
================
It is a way for an appication program to ask the OS for a certain service that only the OS provides. All the services that only the OS are allowed to 
implement because they invoke access to certain device that only the OS is allowed to access or perform certain previlliged instructions. Anything of 
that sort, a user program is not allowed to do directly. It must go through the OS. And it does that via system call. System call is like making a function
call.  But it is a special kind of function call where the implementation of that function call is in the Kernel.So it is not implemented a user level
library, it is implemented by the OS. And usually system calls are implemented by interrupts. 
Some machiens have a system called "instruction" but it does the same thing basically (whether it is an interrupt or it is a system call instruction that 
the user program executes, either way, control will be given to the OS. It is basically the application program saying, "Now I need the OS to do this for
me, so now the operating system should get control.


                            ========================================================================================
                             Operating Systems Lecture 6: OS Introduction (Part 6): The OS is a Resource Manager
                            ========================================================================================

=========
Intro
=========
So now we will go quickly through the different kinds of resource management that operating systems do. Why? 

=============================
Process Management Activities 
=============================
The operating system is responsible for the following activities in connection with process management: 
- Creating and deleting both user and system processes  -- this is something that we will be discussing in chapter 3 
- Suspending and resuming processes -- chapter 3
- Providing mechanisms for process  synhronization -- chapter 5
- Providing mechnisms for process communication -- chapter 3
- Providing mechanisms for deadlock handling  -- discussed in chapter 7

These are the acitivities that the OS does.  


=============
Memory management
=================
To execute a program, all (or part) of the instructions must be in memory 
- All (or part) of the data that is needed by the program must in the memory 
- Memory management determines what is in memory and when 
- Memory management activities: 
  - Keeping track of whic parts of the memory are currently being used and by whom 
  - Deciding which processes (or parts) and data to move into and out of memory 
  - Allocating and deallocating memory space as needed 

We will be spending 2 weeks on memory management trying to understand how an OS manages memory. How it allocates memory to user processes and how it manages
that. 

=================
Storage management
==================
How the OS manages the storage device -- how it manages the disk, and how it provies the file system interface. If you have a text file with line 1, line 2
and line 3, it will look like a nice text file with some human readable lines on it. But on the device, it may not look like this. It may not be one 
contiguous piece of data. It could be stored on the device in different pieces or different blocks of data. And maybe one block could be somewhere in the 
middlle oe line#1. And the next block coud be physically stored in a totally different place. But to you, when you look at the file it will look like a nice
text file which is contiguous and human readable and everything. 

=======================================
Performanc of various levels of storage
=======================================
There are different levels of storage in the system.

=====                                 ======               ======               ======               =====               ======
Level                                   1                    2                    3                    4                   5
======                                ======               ======               ======               =====               ======
Name                                  register              cache               main memory         solid state disk     magnetic disk
Typical size                          < 1 KB                < 16mb               <64GB              < 1TB                < 10TB
Implementation technology             custom memory with    on-chip or off-chip  CMOS SRAM          flash memory         magnetic disk
                                      multiple ports CMOS   CMOS SRAM
Access time (ns)                      0.25 - 0.5            0.5 - 2.5            80 - 250           25,000 - 50,000      5,000,000
Bandwidth (MB/sec)                    20,000 - 100,000      5,000 - 10,000       1,000 - 5000       500                  20 - 150
Managed by                            compiler              hardware              OS                OS                   OS
Backed by                             cache                 main memory           disk              disk                 disk or tape

Movement between levels of storage hierarchy can be explicit or implicit.
Registers can be accessed typically in the fraction of a nanosecond. And when we say that, what are we implying about the speed of the processor? Or in
other words what clock speed corresponds to one nanosecond. 
 1 nanosescond = 1GHz  
      Also
 1GHz = 10^9Hz in one second 
 which equivalent to 
 10^(-9)s /cycle 
 which is: 
 1 ns/cycle (or one nanosecond per cycle).

This means that if a processor does 10^9 cycles each second then this means that each cycle is one nanosecond. 


===========
Access to register
===============
Usually access to register is done in one cycle. You cannot access anything in less than one cycle, so this is saying that  the cycle ranges from 0.25 to
0.5 nanoseconds. So 0.25 nanoseconds here corresponds to 4GHz, we do it like this: 

 0.25 nanoseconds = 1GHz 
 25/100 = 1GHz
 100Ghz/25 = # of gigahertz
 answer : 4GHz

So if each cycle is a quarter of a nanosecond then the clock speed is 4 gigahertz. So, you can access the register in one cycle. 
===============
Access to cache, main memory and sold state, and magnetic 
================
Then you can access the cache in a couple of cycles (usually 2 or 3 cycles) but if it is L2 or l3 cache then that may take longer. So if it takes 25 
nanseconds then on a 4 gigahertz process will be a 100 cycles and that is probably an L3 cache. Access to main memory can take hundreds of cycles. Solid 
state can take tens of thousands and a magnetic disk can take million of nanoseconds or millions of gigahertz. So they vary in terms of speed. It is good
to have a good estimate of how fast each device is. And of course we will, when we get to disk management, we will talk more about this. 


The OS is responsible for managing main memory and disk. That is why, in this course, we have spent two weeks on memory management and two weeks on 
disk management. But we will not spend any time on register management and cache management becuase cash management is done by the hardware and register
management is done by the compiler. 

========
Cache management (by hardware)
==============
We understand that the OS manages the hardware. Anyways, deciding what to put in the cache (or updating the cache) is the job of the hardware. Also, what 
is the main decision that is being made in the cache management besides the architecture of the cache -- how much cache we have at each level, and the 
hardware architecture of the cache itself. There is a very important decision that the hardware must make. What is it? It has to prioritize what data 
it needs to store in the cache versus having to store it in other things. And how does it prioritize this? Does it go through main memory and say that this 
is higher priority piece of data I'm going to put it in the cache. Does it like scan main memory and choose what to put in the cache? It might be knowing
the frequency that it is used or knowing how recently it has been used? Does it do an analysis? Like does it sound like something a hardware would do? 
Let's think realistically...what can a hardware do. It is not going to scan the main memory and say that this is an important piece of memory and put it 
in the cache. What does the hardware do? It does this IMPLICITLY. It does this without actually scanning main memory. 

============ 9:11
How does caching work
=============
Hardware cannot do sophisticated analyses of data. It is just that the hardware will start with an empty cache. Nothing is in the cache. It is a cold start.
Then when a process starts loading or accessing memory location, whenever a memory location is accessed, the hardware is going to check the cache. If it is 
in the cache, it will be used. If it is not in the cache, the hardware is going to load it from the main memory into cache. Now as long as there is room 
in the cache, there is no issue. The hardware is just goint to load it from the main memory into the cache. But we have an issue, when? When the cache is 
full. So that is when the hardware will have to make a decision. It will have to decide what is in the cache. Now the cache is full and I need to load. 
Now what is the solution? There is only one solution. I will have to victimize one of these blocks or lines of cache in the cache and I have to override 
them with what I am loading. So, now I need some kind of replacement policy. So this replacement policy -- so do we remember one good cash replacement 
policy?  

===========
LRU policy
===========
LRU stands for "Least recently used". This is review material. But the point is that the hardware decides the replacement policy and the operating system
has nothing to do with it.


========
Register management (by compiler) -- (also discussing how the overlapping of multiple variable's live ranges affect the register allocation)
==============
If you have a program that defines a thousand variable:

int x1; int x2; ..... int 10000;

There are 10,000 vvariables in our program. Now when the compiler translates a program written in a high level language into machine code, the machine 
code doesn't understand variables. The whole notion of a variable is not there in the hardware. What the hardware knows are registesr and memory locations.
So any piece of data should be in a CPU register or in the memory. The hardware doesn't understand the concept of a variable. So the compiler's job is to 
map these program variables into memory locations or registesr. And what is the preferred device? Registers, because they are faster. So hopefully, the 
compiler can map all these variables into registers. Now, it has a limited number of registers. Like for example, 32 registers on a hardware. So the compiler
has 10,000 variables annd 32 registers. So the compiler will have to findd a good way to map these. It may even be able to map 10,000 registers onto 
32 registers because not all variables overlap. If two variabes overlap in their live ranges then they cannot be mapped into the same register. But if they
do not overlap (like you define a variable and then you are done with it and then you start another variable).  Then these two variables can be placed 
in the same register.
So solving this problem of mapping program variables into CPU registers or mapping as many variables into as many CPU registers as possible is the job of the 
compiler, in what we call "REGISTER ALLOCATION".
So this is what we mean by saying that the compiler controls the registers. So register allocation is not an OS topic. 


==============
I/O Subsystem
=============
We will be talking about I/O and how the Operating System tries to overlap CPU execution with I/O requests but we unfortunately in this course, we will not
get to cover the internals of I/O devices. But we will understand basically the interface of I/O devices. Or we will basically understand how the OS 
interacts with an I/O device. And in-fact, we already have a reasonable understanding of how an OS interacts with an I/O device but we will not be studying 
the I/O subsystem itself.  

=======
A bit more info on I/O subsystem 
=========
- One purpose of OS is to hide peculiarities of hardware devices from the user. 
- I/O subsystem is responsible for: 
  - General device-driver interface 
  - Drivers for specific hardware devices 
  - Memory managment of I/O 

======================
Kernel Data Structures
======================
It is very important to keep in mind that the OS is just a computer program. It is a special kind of program.  It is a complex program. It is a program that 
controls the whole machine, but it is a program. And what is a program? A program is basically a combination of algorithms and data structures that are 
expressed using some programming language.  That is what alll programs are. And the more complex a program is, the more complex the algorithms and the
data structures that the program uses will be. So more complex programs use more complex algorithms and data structures. And since the Operating Systme is 
a very complex program and since it does many different and many challenging resource management tasks, it will be using many sophisticated algorithms and 
data structures. Ofcourse this depends on the OS itself. But there may be a certain data structure that one operating system implements as a binary search
tree while another operating system implements as a hashtable. So different OSs vary in the way they implement certain functionaities or algorithms. They 
use different algorithms and data structures. Most of the typical data structures that OS is going to use is going to be like: BSTs, Hashtables, Lists,bitmap,
and such.

==========
Linux Data structures
=========
Linux data structures are defined in: include files <linux/list.h> , <linux/kfifo.h> , <linux/rbtree.h>

===========
Open source operating systems
===========
These are made available in source-code format rather than just binary closed-source
- Counter to the "copy protection" and "Digital Rights Management (DRM)" movement 
- Started by "Free Software Foundation (FSF)", which has "copyleft" GNU Public License (GPL)
- Exampless include GNU/Linux and BSD UNI (including core of Mac OS X), Open Solaris and many more
- Good for academic purposes (education and research).

We just need to be aware that some OS are open source. In theory, we can modify the OS (if we have the expertise of modifying an OS kernel). If it is a 
closed source OS then you just get the OS executable. Libraries, dynamic linked libraries etc are all executable.

                                    ==========================================
                                     Operating Systems Lecture 7: OS Services
                                    ==========================================

===========
Intro
=========
In this chapter, we will take a look at the inside of an OS and how it is structure. But before we do that, we will take a greater look at the details that 
the OS provides.

So here we have an operating system sitting on top of a hardware. Standing between the user and the hardware. And between the user programs and the hardware.
So these are the services that the OS does. So it does program execution, I/O, file systems, process communication, resource allocation, accounting,
keeping track of resource usage. We will talk about many of these in greater details about these later in the corresponding chapters. After this, you have
error detection and protection and security. 

=================================================================
Now how do user programs access these services via system calls? 
=================================================================
A user program will issue a system call to access a certain OS service. So these services include user interface. We already know CLI and GUI. Now here, 
we assume that we are familiar with the command line and the with the script in "CSC60". This assumes that we are comfortable using CLI and doing some 
scripting as well. 

=======================
Example of a CLI system
========================
A vax system with a command line interface is an example of a CLI based sysetm. 


==================
Program execution
==================
Program execution is one of the main functionalities of an OS. So when you click on an icon, you are basically asking the OS to execute that program. On
the command line, you just type the program name and then hit enter. By doing this, you are asking the command line to execute that program. 

============
I/O operations
==============
These are controlled and they are implemented by the Operating systems and access to the I/O devices is limited to the Operating Systems. We are not allowing
user programs to access I/O programs directly. Why? Because of the following reason(s): 
i) Easier implementation for application. The application will be easier to write and to implement. To hide the details of the I/O devices from the 
   application programs. When an application program needs to read from the keyboard, the application program doesn't need to know about the specific 
   details of that particular I/O device. It will just say read. And the OS will handle all the low level and hardware level details. 
ii) To divide access between processes that may need to use a particular I/O service. To resolve conflicts in other words. If we allow user programs to 
    access I/O devices directly then there will be lots of conflicts. What if two processes are trying to access the same I/O device at the same time? 
    Then there will be conflicts. They may be overwriting each other. And so if we go through the operating system, the operating system is going to  
    manage this and it will resolve conflicts in a way that ensure correct functionality. 

============================================================================================================================================
Is the fact that OS is the manager as opposed to implementing some kind of actual device itself as the manager an implementation detail?
============================================================================================================================================
NO! This is a policy. It is a policy of giving control to the OS. Keeping the OS in control and allowing the OS to resolve conflicts because otherwise 
application programs will be creating lots of conflicts; they will be conflicting with each other when they are trying to access the devices. It is a 
very important policy that will keep the OS in control and ensure that the system will work correctly.

===================================
Operating system Servicess (Cont.)
===================================
The different services that OS offers, it would include File Systems manipulation. Communications, process communication, we will be studying different 
methods for inter process communication includingg shared memory and message parsing.  Shared memory will be shared in detail and we will be using it in
Assignment#1. We will be doing that through the Operating System. Message passing will be left for the networking class. Error detection is very important 
for the OS to be aware of all the erorrs that may occur in the system whether they are user errors or hardware errors or device errors. THe OS should be 
aware of these and take the right action.
Resource allocation, accounting -- keeping track of how much of each resource a process has used (especially the CPU, and we will see this in the CPU
scheduling). 

=======================
Detailed notes Operating system Servicess (Cont.)
========================
One set of operating system services provides functions that are help to the user (Cont.):

- File-system manipulation : the file system is of particular interest. Programs need to read and write files and directories, create and delete them, search
                             them, list file information, permission management. 

- Communications : Processes may exchange information, on the same computer or between computers over a network. 
                     - Comunications may be via shared memory or through message passing (packets moved by the OS). 

- Error detection: OS needs to be constantly aware of possible errors. 
                   - May occur in the CPU and memory hardware, in I/O devices, in user program
                   - For each type of error, OS should take the appropriate action to ensure correct and consistent computing.
                   - Debugging facilities can greatly enhance the user's and programmer's abilities to efficiently use the system.


==========================
Anoter set of OS functions
==========================
Another set of OS functions exists for ensuring the efficient operation of the system itsellf via resource sharingg: 
  - Resource allocation : when multiple users or multiple jobs running concurrently, resources must be allocated to each of them. 
  - Accounting : to keep track of which users use how much and what kinds of computer resources.
  - Protection and security: The owners of infomration stored in a mutiuser or networked computer system may want to control use of that information, 
                             concurrent processes should not interfere with each other. 
                 Protection: Involves ensuring that all access to system resources is controlled. 
                 Security  : Security of the system from outsiders requires user authentication, extends to defending external I/O devices from invalid 
                             access attempts.



                                    ==========================================
                                    Operating Systems Lecture 8: System Calls
                                    ==========================================

====================================
A view of operating system services
====================================

  -----------------------------------------------------------
           User and other system programs 
  -----------------------------------------------------------
           GUI | Batch | Command Line 
          ----------------------------
           |    User Interfaces      |
  ------------------------------------------------------
  |               System calls                         |
  ------------------------------------------------------
  
  -------------------------------------------------------------
  |  Program execution | I/O operations | File systems | communication | resource allocation | accounting 
  |  error detection   | protection andd security 
  | 
  |                                            Services
  ---------------------------------------------------------------------------------------------------------
                                               Operating system
  ---------------------------------------------------------------------------------------------------------
  |                                             hardware                                                  |
  ---------------------------------------------------------------------------------------------------------



=================================
This above picture and some recap
=================================
The OS stands between the hardware and the user programs and the OS implements multiple services and the user programs or the application programs have 
access to these services through system calls. Some of the services are the services that the user interacts with. Like the user interface, program 
execution, I/O operations, file system manipulation, communications like message passing shared memory (in this course, shared memory is something that 
we will covering in asignment#1).


=================
User interfaces
=================
In terms of user interface, there is a command line interface and GUI. In the earlier days, only CLI existed. Then some system started to offer GUI with
all the features like windows, menus and mouse and clicks and even touch screens. In terms of CLI, in this course, we will have to do some command line
interaction with the system. 
CSC60 (Introduction to systems programming using UNIX) is a prerequisite for this course. So we are expected to have done some command line interaction.
And some script programming. Some scripting languages and the make  files, and things ike that. So, we have to do some of this in the programming 
assignment.

=============
Details on CLI
=============
CLI or command interpreter allows direct command entry
  - Sometimes implemented in kernel, sometimes as special programs (Windows and Linux)
  - Sometimes multiple flavors implemented - shells, e.g., C-Shell, Korn Shell, Bourne Shell
  - Primarily fetches a command from user and executes it. 
  - Sometimes commands are built in, sometimes they are just names of programs (used in Unix)
     - If the latter, adding new features doesn't require shell modification

=========
Details on GUI
========
User-friendly desktop metaphor interface 
  - Usually mouse, keyboard, and monitor 
  - Icons represent files, programs, actions, etc
  - Various mouse buttons over objects in the interface cause various actions (provide information, options, execute function, open directory (known as 
    folder)
  - Invented at Xerox PARC (1973)

Many systems now include both CLI and GUI interfaces: 
  - Microsoft Windows is GUI with CLI "command" shell.
  - Apple Mac OS X is "Aqua" UI interface with UNIX kernel underneath and shells available.
  - Unix and Linux have CLI with optional GUI interfaces (CDE, KDE, GNOME) 


=============
System calls
=============
So a system call is just a function (or a function call). But it is a special kind of function call. But what is it that separates it from a regular 
function call? What happens when you make a system call that doesn't happen when you make a regular function call? INTERRUPTION! It will stop the 
process and handover the CPU to the kernel. System calls as we said are implemented as interrupts. And when an application program makes a system 
call, the control of the CPU will get transfered to the Kernel.
So a system call is a special kind of function call that causes this change in the possession of the CPU. Now the Kernel is in possession of the CPU
as opposed to that process. And now we are in Kernel mode. The mode is going to change from user mode to Kernel model. So making a system call will
result in this change, which means the Kernel is now in charge and the kernel is going to execute that function (the implementation of that function). 
It will perform that function and it will return the results to the application programs.

====================
Making system calls -- APIs 
==================== 
Now usually we do not use system calls directly. Usually system calls are wrapped by/with "application program interfaces" (APIs). These system calls
are wrapped with APIs for two different reasons:
i) Convenience 
ii) Portability 

It is more convenient to use APIs because APIs are more user friendly and easier to use. And in many cases, they have  fewer parameters that you have
to pass to them. An API is implemented by making a system call. So it is just a wrapper. Think of an API as a wrapper of one or more system calls. 
So an API may be implemented using multiple system calls. You may package multiple system calls with one API that requests multiple services. 

=======================
The ease of using APIs
=======================
APIs are usually more user friendly and easier to use because a system call may have many parameters. Like a system call may have 10 parameters, and 
some of these parameters may not be interesting for the user because usually all users are going to set the parameters to the default values. There are
certain default values that are 99% of the time are the settings that you want. So, an API will just set these parameter values to those commonly used
defaults. And then an API will only require from the user specifying 3 or 4 parameters, essentially acting as a wrapper. So that is one thing. 

===========================
Portability of APIs - POSIX
=========================== 
The other thing is PORTABILITY. We have some standard APIs like POSIX.  In assignments, we will be using the standard POSIX API. And POSIX stands for (given
the P means portable) Portable Operating System Interface. Which means that it is a standard interface that can be implemented on multiple systems. POSIX is
indeed implemented on multiple systems. Like Linux Systems, Unix Systems, different kinds of Linux systems, and it is also implemented on the MAC systems. 
And, it might also be implemented on the windows. There might be recent implementations for the windows for the POSIX API.
So this is a standardization; it's one common API that is implemented on multiple systems. So we will use it in our programming assignments. In theory, 
if we write a program on POSIX using MAC and we run it on a Unix system then it should run. But we don't live in a perfect world. It, sometimes, doesn't 
work. There are sometimes compatibility issues. Sometimes there are some differences that will cause the programs (that uses the POSIX API) to function
correctly on one platform but to have a problem on another platform. So that is why, for the programming assignments, we have to make sure that our 
programs function correcltly in our standard ecs environment. So we need to make sure to standardize things. 
But in theory, if you use the POSIX APIs, it should work on any system that implements POSIX. This is true 99% of the time (maybe), but because implementations
are not perfect, or because there are some subtle differences between the different implementations, maybe 1% or 0.1% of the time, there will be a problem.


===========
Java API
==========
In addition to the POSIX API, there is the Java API. Java is a language that is implemented on many different platforms. In java, there are lots of 
functions that we call that use the system that we know are implemented using the system calls. Anything that involves an I/O or a file access, that is 
implemented using system calls. 

=====================================
So finally, when to use a SYSTEM CALL
=====================================
Anything that we know that cannot be done without OS interaction or OS intervention, we know that it must be implemented using a system call. And ther eis 
also the windows API. 

=========================================
Details on System calls (from the slides)
=========================================
- Programming interface to the services provided by the OS 
- Typicaly written in a high level language (C or C++)
- Mostly accessed by program via a high level Application Programming Interface (API) rather than direct system call use. Why? 
  - Portability 
  - More convenient, as they hide system-call details 

- Three most common APIs are Win32 API for Windows, POSIX API for POSIX-based systems including virtually all versions of UNIX, Linux , and MAC OS X), and
  Java API for the Java Virtual Machine (JVM)

Note that the system-call names used throughout this text are generic.

===============
Example of System Calls: 17:20
===============
                   
            #include<unistd.h>
            ssize_t    read(int fd, void *buf, size_t count)  
            _______    ____ _______________________________
            return     func           parameters
             value     name

This is one example of a read function call that is supported on UNIX system if we are programming in C. Under Linux, we will use this read. Well it is one
of the options that are available to us . It is not the only options available to us. There is another called formatted read, written as "fread". So there
are multiple functions available to you, but they are all implemented using system calls because you cannot read from a file without going through the OS.
So this read takes 3 parameters: 

Param         :  Meaning
int fd        :  file descriptor 
void *buf     :  pointer to a buffer (that you as an application programmer is RESPONSIBLE for ALLOCATING)
size_t count  :  size of the buffer

Return value   : meaning
--------------------------
ssize_t        : the api returns the size of the data that has been written to the buffer.

===================
Allocating a buffer (void *buf parameter)
===================
When you are dynamically allocating a buffer, you are making a system call. And then you pass the size of tht buffer. So if you allocate a buffer of size 
1k, you want to pass this size to the system so that the system only writes to 1k and it doesn't go beyond what has been allocated. So programmar has a
lot of responsbility like allocating the right size. Everything is explicit. There is less automation. 

========================================================
When passing a file descriptor? Where does it come from?
========================================================
How does it know to open the file that you want to read from? How will it know to read the file that you want to read from. So this file descriptor is 
specifying the file. When you read, you read from a specific file. How will the system know which file to read from? Remember that you are only passing 
a file descriptor, and you are not passing a file name. 
Logically, where should that file descriptor come from? File descriptor is just a number (an Integer) for that file. But where will that come from? 
The answer is that before you read, you open the file. And with open you pass among other things a file name. And then the "open(param)" returns the file 
number or an id for that file that you can use in subsequent references for the same file. And the subsequent references are the reads and the writers. 
Then you close it after you are done. So this is just an example. 

                        fd  = open("name",....)   //this is how we retrieve the file descriptor 

=====================
Details on Example of Standard API 
============================
As an example of a standard API, consider the read() function that is available in UNIX and Linu systems. The API for this function is obtained from the 
main page by invoking the command: 
          main read

on the command line. A description of the API is presented above. 
A program that uses the read() function MUST INCLUDE unistd.h header file, asthis file defines the ssize_t and size_t data types (among other things). The 
parameters passed to read() are as follows: 
 - int fd : the file descriptor to be read 
 - void *buf : a buffer where the data will be read into 
 - size_t count : the maximum number of bytes to be read into the buffer 

On a sucessful read, the number of bytes read is return. A return value of 0 indicates end of file. Ifa n error occurs, read() return -1.


=====================
Implementing system calls
===================
When implementing system calls, the system will have something called a "system call interface" which is the interface between the application programs and 
the actual system calls. The system call interface will just have a table because each system call will have a number. So usually the implementation is a 
table indexed by the system call number and for each system call number there is an entry for the corresponding function within the system that implements
that. So all of these are in Kernel mode. 
IMPORTANT: System calls are implemented in Kernel mode, but the call, which is the system call or the API, is issued in user mode.

==========================================
Diagram for the system call implementation
==========================================
                -----------------
               | User application|
                -----------------
            open() |         / \ 
                  \ /         |
user mode    -------------------------------------
------------|      system call interface          |
Kernel mode  -------------------------------------
              |        ---                     /|\                 
              |-----> | . |                    ---------------- |                                     
                      | . |                                     |
                      |---|               open()                |
                     i|---| ----------->    implementation      |
                      |---|                   of open()         |
                      -----                    system call      |
                                               .                | 
                                               .                |
                                               return ----------|


========================================================
Parameter passing -- how do we pass parameters to system call
========================================================
There are three (atleast 3) different methods.

==================================
Solution 1: Passing the parameters in registers
==================================
One method is passing the parameters in registers. But what is the problem with this? The number of registers is limited, so it limits the number of 
parameters. Usually not all registers are available for parameter passing, only a SUBSET of the registers are available for parameter passing and that is
very limited. And so in order to solve this problem, people came up with OTHER SOLUTIONS.  

====================================================
Solution 2: Puttign all the parameters in a memory block
====================================================
So you put all the parameters (like parameter1, param2, ....param n) in a block and then you pass the address of the block in a register (in one register).

==========================================================================
Solution 2: Pushing parameters on the stack and having the system pop them
=========================================================================
So it is one standard method for passing parameters: the caller pushes and the callee does a pop. And in this context, the callee is the Operating System. 


========================================
Details on System Call Parameter Passing
========================================
- Often, more information is required than simply identity of desired system call. 
  - Exact type and amount of information vary according to OS and call 
- Three general methods used to pass parameters to the OS 
  - Simplest: pass the parameters in registers 
    - In some cases, may be more parameters than registers 
  - Parameters stored in a block or table, in memory, and address of block passed as a parameter in register. 
    - This approach taken by Linux and Solaris
  - Parameters placed, or pushed, onto the stack by the program and poppded off the stack by the operating system
  - Block and stack methods do not limit the numnber of length of parameters being passed.

======================
Types of System Calls
======================
We will go through these examples of system calls and we will just point out that some of these will be used in our assignments.

=======
Process control
=========
 - create process, terminate process
 - end, abort
 - load, execute
 - get process attributes, set process attributes 
 - wait for time 
 - wait event, signal event
 - allocate and free memory 
 - Dump memory if error 
 - Debugger for determining bugs, single step execution 
 - Locks for managing access to shared data between processes.

==============
File management
===============
We will be using file management in the assignments. 
- Create file, delete file
- open, close file 
- read, write, reposition
- get and set file attributes

============
Device management
=============
- request device, release device
- read, write, reposition 
- get device attributes, set device attributes 
- logicaly attach and detach devices

========
Information maintenance
==========
- get time or date, set time or date
- get system data, set system data 
- get and set process, file or device attributes

===========
Communications
============
- create, delete communication connection
- send, receive messages if message passing model to host name or process name
   - From client to server 
- Shared-memory model create and gain access to memory regions
- transfer status information 
- attach and detach remote devices

===========
Protection
===========
- Control access to resources
- Get and set permissions
- Alllow and deny user access

=========================================
Examples of Windows and Unix system calls
==========================================
There are some differences in the implementations for windows system calls and unix system calls. So for example, there is a difference betewen how we 
create a process using fork() in Unix than how we create a process using CreateProcess() in windows.
Waiting, so we will be using wait() on linux in the assignments. As you can see the system call names on windows are longer and more descriptive than the 
ones on Linux. So we will be using some of these (not all of them) in the assignments.

                                         =========================================
                                          Table for Windows and Unix System calls
                                         ==========================================
 
                             Windows                  Unix 
                          ---------------            --------          
Process Control           CreateProcess()             fork()
                          ExitProcess()               exit() 
                         WaitForSingleObject()        wait() 

File manipulation        CreateFile()                 open() 
                         ReadFile()                   read() 
                         WriteFile()                  write() 
                         CloseHandle()                close()

Device                   SetConsoleMode()             ioctl()
Manipulation             ReadConsole()                read() 
                         WriteConsole()               write() 


Information             GetCurrentProcessID()       getpid()
Maintenance             SetTimer()                  alarm()
                        Sleep()                     sleep()


Communication          CreatePipe()                   pipe() 
                       CreateFileMapping()            shmget() 
                       MapViewOfFile()                mmap()

Protection             SetFileSecurity()              chmod() 
                       InitializeSecurityDescriptor() umask() 
                       SetSecurityDescriptorGroup()   chown()

==========================
Standard C Library Example
==========================
Below is a diagram to remind you that even if you are using a Standard Library function that accesses I/O devices like the printf("") library function in C.
That printf() library function, because it must go through the I/O device, it must go through the OS. So it is implemented as a system call because the 
standard C library will translate it to the right system call that writes to the screen. So just the idea here is that some of the standard library functions
that you use in the language that you are programming in (whether it is C or C++ or Java or Python or whatever) they are implemented as system calls.

================
Diagram of Standard C library Example
===============
    ____________________________
  | #include<stdio.h>
  | int main() 
  |  {
  |   .
  |   .
|-------printf("Greetings"); <-------|  
| |.                                 |
| |.                                 | 
| |return 0;                         |
| | }                                | 
| |______________________________    |
|                                 ___|
\/                               |   
 user mode     ------------------|--
------------  | standard C library |----------
 kernel model  --------------------
                |              / \
                | write()       |
               \ / _____________|_______ 
                  | write() system call|
                   --------------------

===================
System Programs
===============
They are programs that belong to the Operating System BUT ARE NOT PART OF THE KERNEL. So they are like extensions of the operating system. They run in user 
mode and not in Kernel mode. But system programs may involve many system calls. So a system program may be issuing many system calls. So whenever it makes
a system call, it is switching to the OS and the OS is running the system in System Mode, but system programs typically run in usermode. So examples of 
system programs are: 
- System programs for file modification like Text editors. A text editor is a system program. Every OS has atleast one text editor. Some OS have multiple text
  editors. Like a notepad on Windows and Vi or Pico or Emacs on Unix. So these text editors are not part of the Kernel, but they are system programs. And 
  usually they make lots of system calls. So it is part of the system, but it is not part of the Kernel, but it makes lots of system calls. 
- Status information are the programs that give you information about the status of the system. 
- Programming language support like compilers, assemblers, debuggers, interpreters, linkers are all system programs but these are not part of the Kernel. 
  And usually there is a compiler with an Operating System. Like UNIX systems are distributed using the gcc compiler, but that compiler is not part of the 
  Kernel. 

These are the examples of the system programs that the user interacts directly with and in fact the user loads. So when you use a text editor, you actually
load it. You actually click on that text editor if you are using a gui or you invoke it at the command line as a user. 

=============================================
System programs that the user does not launch
=============================================
But there are system programs that the user does not launch, they are launched by the system itself. They may launched at boot time. And then when they 
complete tasks, they will terminate. And there are some background system programs that the system creates and the they keep running. As long as the 
system is active, these programs are running in the background. Examples are programs that keep checking on the disk or checking on memory. These are running
in the background and they are performing system services. So the user did not create them. They got created by the System but they are not part of the Kernel.
So when you see a Task manager in your windows, you will see a bunch of processes or a bunch of programs that you did not create as a user. So these got 
created by the system.

========
Daemons
=======
Background services are sometimes called "Daemons". 


=============================================
Details on System Programs -- From the slides
==============================================
System programs provide a convenient environment for program development and execution. They can be divided into:  
 - File manipulation
 - Status information such as date, time, memory available, etc.
 - Programming language support
 - Program loading and execution
 - Communications
 - Backkground services

Most users' view of the operating system is defined by the system programs, not the actual system calls.
System programs provide a convenient environment for program development and execution. Some of them are simply interfaces to system calls; others are 
considerably more complex.

============
File management
==============
Create, delete, copy, rename, print, dump, list, and generally manipulate files and directories

===========
Status information
==========
- Some ask the system for info - date, time, amount of available memory, disk space, number of users. 
- Others provide detailed performance, logging, and debugging information 
- Typically, these programs format and print the output to the terminal or other output devices
- Some systems implement a registry -- used to store and retrieve configuration information.

=========
File modification
=============
- Text editors to create and modify files
- Special commands to search contents of files or perform transformations of the text.

============
Programming-language support
============
Compilers, assembler, debuggers, and interpreters sometimes provided.

========
Program loading and execution
========
Absolute loaders, reloctable loaders, linkage editors, and overlay-loaders, debugging system for higher level and machine languag.

==========
Communications
===========
Provide the mechanism for creating virtual connections among processes , users and computer systems. 
 - Allows users to send messages to one another's screens, browse web pages, send electronic mail messages, log in remotely, transfer files from one machine
   to another.

=========
Background serivces
============
Launch at boot time
 - Some for system startup, then terminate
 - Some from system boot to shutdown 

- Provide facilities like disk checking error logging, printing 
- Run in user context not kernel context
- Known as services, subsystems, daemons.

                                    ===========================================================
                                     Operating Systems Lecture 9: OS Design and Implementation
                                    ===========================================================

========================
Idea of designing an OS
========================
So now designing an Operating System is just like designing any other system. There is no perfect way of designing a system. And there is no widely accepted
method of designing a system. Different people have different design philosophies and approaches and in fact in this course we will be studying different 
approaches and different methods for designing an operating system, and none of them is perfect. That is why, they are interesting because we would like 
to understand the strengths and weaknesses of each design method; the advantages and the disadvantages. There is no prefect solution to designing OS or 
to designing software systems in general. As we will see when we get to different design methods or structures of operating systems, one operating system
may use multiple approaches because operating systems evoke over time. So in earlier versions of an OS, the designers may use a certain design approach
or philosophy and then they realize that they will have to change it or to use or may combine it with other design approaches. Why? Because, it is not 
working well, and because there are some new hardware features that the system will have to handle correctly properly. Like multi-core system for example.
If you design your operating system and it works fine for a single core, now when the hardware evolves and the system has multiple cores, your system is 
no longer working properly, so you need to extend it or modify the design such that it works well on the new kind of HARDWARE.

=================================================
Two kinds of considerations when designing an OS 
=================================================
There are considerations from user point of view and considerations from developer's point of view. 

=======================
From user point of view
========================
User want the OS to be convenient, fast, reliable. That is what the user cares about. 

===============================
From developer's point of view
===============================
User doesn't care about these things. Extensibility (how easy it is to extend an OS and modify the algorithms of the OS), flexibility, debug ability. All of
these are SYSTEM GOALS.

===========================================
Distinguishing between policy and mechanism
============================================

==========
Policy
==========
Policy pertains to "WHAT". 

========
Mechanism
=========
It pertains to "How". 

=======
Example //Time quantum and Time slice. Timed Interrupt
=======
A timed interrupt is a MECHANISM for accomplishing time sharing and ensuring the Kernel to be in control. It allows multiple processes to make progress 
concurrently. Now when the Kernel sets the counter to 10 miliseconds (which is what we call the time QUANTUM or TIME SLICE) that the system is givingg to 
a certain process is a POLICY. And this policy is going to depend on the SCHEDULING ALGORITHM. And when we get to scheduling, we will see how long this 
time quantum is may vary. And in-fact, not all processes may get the same time quantum. So some processes may get longer time slices (depending on the behavior
of that process maybe) than other processes depending on the scheduling algorithm. So deciding how long that time quantum is, that is a policy, while the 
mechanism for implementing this policy is the timed interrupt -- which requires hardware support.

============
Flexibility of the mechansim
=========
The point is that you want your mechanism (that implements a certain policy) to be flexible enough to allowing it to chnage the policy. So, in order to 
achieve that, you don't want to hard code anything or to be limited. You want it to be as flexible as possible. So that you, as an OS designer, can change
the policy. Because policies change and as we said that OSs evolve and they have to support different hardware and different environments. So that is why 
flexibility is desirable. 

=======================
Implementation of an OS
=======================
Earlier Operating Systems were written in Machine Code and assembly code, but modern operating systems are written in C and C++. At some time, operating 
systems were written in other languages as well. The systems that we use (like Linux and Windows) are written in C. We use: 
Linux, Windows, Mac OS 

All of the above are written in C. The operating systems that we use are mostly written in C. Which means that the main body of the operating system is
written in C. But some of the system programs for example, they are separate programs that may be written in other languages. And some of the system 
programs are not as performance critical as the Kernel itself. The kernel itself is very performance critical, so you have to write it in a powerful 
language, while some of the system programs are not performance critical, so you may write them in less powerful languages, like Scripting languages. For 
example: Perl and Python. 

In the Kernel itself (or the OS itself) some certain performance critical sections may be written in assembly. Basically the code is handwritten in assembly
as opposed to being written in C. Now, earlier Operating systems were written in machine language. Modern OS are written in high level languages. Now the 
question, why? So we need to understand the advantages of writing an OS in a high level language. And there are at least 3 advantages of writing an OS 
in a high level language as opposed to writing it in machine code: 
1) Software Engineering advantages including abstraction, readability, programmer productivity, debug ability, reusability 
2) Portability. Example: DOS operating system.  It was written in the Intel x86 assembly, which means that this OS could be run only on Intel processors. 
   Comparing this with Linux that is written in C but can be run on Intel x86, on SPARC andd IBM PowerPC (totally different hardware architectures). You 
   can run the same OS on these different architecture because it is written in C. And you just compile the OS for different machines. So you use a compiler
   that compiles for Intel and a compiler that compiles for SPARC and a one that compiles for PowwerPC.
3) Compiler optimizations. When you write a code in a high level language, you are relying on the compiler to optimize the code. And sometimes, compilers
   do a better job at optimizing performance, especially for a larger program. If you are talking about a few lines of code and you are an expert assembly
   programmer then your optimization will be really good. But what about thousands of lines of program? Tens of thousands of lines of code? Even if you 
   are an expert aseembly programmer can you beat the compiler at optimizing larger pieces of code? In general, the human brain cannot handle larger pieces 
   of code, while the compiler uses algorithms that can optimize the code. While you as a human can optimize small piece of code.  


================
Details on Implementation
===============
- Much variation
  - Earlier OSes in assembly language
  - Then system programming languages like Algol, PL/1 
  - Now C, C++
- Actually usually a mix of languages 
  - Lowest levels in assembly 
  - Main body in C 
  - Systems programs in C, C++, scripting languages like PERL, Python 

- Linux and Windows Kernels are mostly written in C
- High level language is easier to port to other hardware.
  - Linux is written in C and is available on Intel x86, Oracle SPARC and IBM PowerPC 
- MS-DOS and early versions of Windows were written in Intel x86 Assembly and ran only on Intel x86 HW

                                    ===========================================================
                                          Operating Systems Lecture 10: OS Structures
                                    ===========================================================

=============
Lecture Intro
==============
We will be discussing the MOST IMPORTANT PART of this chapter, which is the actual implementation of an OS. So today, we will be discssuing different 
approaches to structured OS. 

=========
Last  discussion
========
We talked about advantages of writing an OS in a high level language as opposed to the assembly code. 

===============================================================================
When does writing code in Assembly make sense as opposed to high level language
===============================================================================
If the section of code that we are writing is performance critical. That is one condition. The other cocndition is that the section that we are writing is
small in size. In that case, the assembly programmer is capable of writing very efficient code for that section. 

=========================
Compiler is an automation
=========================
The compiler is an automation and automation always works better when you have bigger data to process.

===========
What language is Kernel written in
============
The kernels for most of the systems that we use are written in C. So we use Windows, Linux, both are written in C. Android which is based on Linux is written
in C. Mac OS (Apple's OS) is written in C. That is why, in this course, we must do some system programming in C. 


===============
Operating system structure
============
We will be discussing 4 kinds of structures. 
1. A simple monolithic structure (or no kind of structure basically).
2. Layered 
3. Microkernel 
4. Modules 


=======
Details on OS Structure
=========
GEneral-purpose OS is a very large program and hence there are various ways to structure it: >
1. Simple structure (Monolithic), e.g., MS-DOS, original Unix 
2. Layered - an abstraction 
3. Microkernel, e.g., Mach
4. Modules 
5. Hybrid Systems 

==============
Monolithic
============
It just means that the OS is in one piece. And it is not divided into modules and it is not structured. One example is the MS-DOS OS. Which is a relatively 
small operating system that offered limited features. And the earlier versions of Linux (i.e UNIX) were to a great extent monolithic. Whereas, as we will 
see later reall operating systems use a combination of approaches.

==========================
Structure of an MS-DOS OS
==========================
MS-DOS is written to provide the most functionality in the least space. 
- Not divided into modules
- Although MS-DOS has some structure, its interfaces and levels of functionality are not well separated. 
- Application programs can access BIOS to write directly to the display and disk drives. 

          ---------------------------------
           Application program
          -----------------------------------
               |                           |
              \ /                          |  
          ----------------------------     | 
           resident system program         |
          ----------------------------     |
               |                 |         | 
              \ /                |         |
          ---------------------- |         | 
           MS-DOS device drivers |         | 
          ---------------------- |         |
               |                 |         |
              \ /               \ /       \ /
          ---------------------------------------
           ROM BIOS device drivers
          ---------------------------------------
           

==================================================
All operating systems are hybrid operating systems
==================================================
So we will see all Operating Systems are hybrid operating systems -- so they use a combination of approaches that we will be discussing here. So there is
no pure OS that uses a single approach. Why? Because OSs evolve. They start off by designing an OS in a specific way and then they realize that they have
a performance problem and in order to fix that performance problem, they take a different approach. 

========================
OSs based on another OS
========================
Sometimes the operating systems are based on another operating system. Like android is based on Linux, so it is not a new OS that was  written from scratch. 
So you are building it on an existing Operating systems. But the components that you are adding may take a different approach. So the way operating systems
were developed in reality, made them all hybrid -- mixed, using a mixed of approaches.

=============================
Limited structuring -- UNIX
=============================
Unix -- limited by hardware functionality. The original UNIX operating system had limited structuring. The UNIX OS consists of two seperable parts. 
 - System programs 
 - The Kernel 
   - Consists of everything below the system-call interface and above the physical hardware 
   - Provides the file system, CPU scheduling, memory management, and other operating-system functions; a large number of fucntions for one level 

- Difficult to implement and maintain 
- Performance advantage due to little overhead in system-call interface and communication within the kernel.

=====================================
Looking at a classical Unix structure
=====================================
 Beyond simple but not fully layered 
  
    -----------------------------------------------
                  (the user)
    -----------------------------------------------
                 shells and commands 
                 compilers and interpreters 
                 system libraries 
    ------------------------------------------------ 
 l       --System call interface to the kernel--    
 e    - Signals terminal        file system                   CPU scheduling 
 n      handling              swapping block I/O system       page replacement 
 r    - character I/O         disk and tape drivers           demand paging 
 e     system terminal                                        virtual memory
 k     drivers    
     -----------------------------------------------------------------------------------------------------------------
                         --Kernel interface to the hardware--
      Terminals and controllers terminals  |    device controllers, disk and tapes | Memory controllers physical memory
    --------------------------------------------------------------------------------------------------------------------

================================================
Description on traditional UNIX System structure
================================================
There isn't much structure. We have a big kernel that has very much everything in it. There is an interface to the hardware and interface to the user. And 
between these there are all the services that the Kernel provides. So this is monolithic in the sense that there is a big kernel that has everyhting and
it is not well structured. Now we will understand what this means when we look at the other approaches that are more structured. 


================
Layered Approach
================
In this approach, the Kernel is organized in layers. So there is a hiearchy of layers. And there is layer 0, which is the hardware. And on top of that, there
is layer 1 which is the lowest layer in the system, and this kind of approach is an abstraction or it is an ideal approach that polynomial operating system
implement exactly -- you may see this kind of approach in some operating systems, but it is not going to be that the whole system is designed using this way. 
This is a hiearchial apprach. 
So if you have multiple layers, and if you are in the third last layer (let's call it layer N) and there is a service in our layer N, then this service can call
services in layer N-1. So it cannot talk to the layers above it. And it cannot even tallk to the layers that are below to it (that are not next to it). So 
you are basically allowed to make function calls in layer N-1 only.  So let's say that you are in layer N+1 and you have to talk to layer N-1 then you have 
to go through layer N. 

===============================================================================================
What are the advantages and disadvantages of Layerd Approach relative to the monolith approach?
===============================================================================================
1. Performance factor is a disadvantage in the layered approach relative to the monolith approach. Because in a layered approach, if one layer needs to talk
to a layer that is much below it then it has to go through a whole bunch of other layers which takes up more time. So that is an OVERHEAD. Going through that
hiearchy or having to go through the hiearchy will  involve an OVERHEAD which will degrade performance. So from performance point of view, monolithic works 
better than layered.
2. Challenge in designing a layered system is that sometimes it is not clear where to put a certain service, or given two services x and y, sometimes it is not
   clear whether x should be on top of y, or whether y should be on top of x. A concrete example is that of a CPU scheduler and a disk driver.

=================
The word "simple"
=================
The word simple is not very religious. It could be interpreted both ways. You can argue that layered is simpler. But you can also argue that monolithic is 
simpler because there is no structure. In-fact when you say simple, monolithic is simple because it has no structure. Layered is not simple but it is easier
for development and debugging. When you are debugging this, you will have multiple layers and you can debug each layer separately. So basically, this approach
has all the good ideas that you learn in SOFTWARE ENGINEERING. So from software engineering point of view, this is a top down design and it has all the good
things that you learn in software engineering classes. While, a monolithic kernel, from software engeering point of view is terrible. It is not modular. It is
not divided into pieces, but from performance point of view, it is good because there is minimal overhead. So basically layered approach is a cleaner design.

=========================================================================================
Challenge discussion: CPU Scheduler and Disk Driver (or the backing store driver) example
=========================================================================================
A CPU Scheduler and the back store (or the disk driver/manager ) is a good example to demonstrate a challenge that we face in a layered architecture. So
on the surface, a disk driver appears to be on top of a scheduler because the disk driver uses the scheduler. So the scheduler is at a lower level becuase
the scheduler schedules all processes in the system including the disk driver or the disk manager. So disk driver needs or uses the scheduler, and the 
scheduler is at a lower level. This is what appears on the surface, but if you think deeply about this and the needs of the schedudler, the scheduler may 
need the disk driver if there are too many processes in the memory and the scheduler needs to swap some of them with the disk (or in other words if it needs
to put some of the processes on the disk) because there isn't enough memory, in this case, the CPU scheduler will be invoking the disk driver. So in this case
the interaction goes in both directions. When the interaction between two layers goes in both directions, it will not be easy to decide which one should be 
placed on top. 

=============================
Solution to the above problem
==============================
One solution would be to put them in the same layer. Or put them both at the same level. But if we keep doing this, what will we end up with? We will end 
up with a monolithic structure by keeping everything at the same level. So this is just a challenge in designing a layered approach. And I (instructor) 
think that it is a challenge that we all face when trying to do a top-down design.

==================
Microkernel System -- idea of message passing here.
===================
We have a small kernel that has only the essential features and all other services are placed in separated processes that are run in user mode. Now this (in 
the slide presented) is just an example, so don't think that all microkernel systems are gonna put these services inside the kernel and the rest outside the
kernel. This is a design decision and different designers may make different decisions and may choose to put different services in different places. But the 
idea is that the designers of the system are going to identify what they think are essential services. And only those essential services are put in the 
Kernel, while all other services are run as separate processes in user mode. Now when these separate processes that are running in user mode cannot talk to
each other directly (they have to talk to the kernel), so if you have an application program which is running in the user mode, if it needs to make a 
request to another service, it cannot talk to it directly. It will communicate with the Kernel through messages and then the Kernel will communicate with that
service and the kernel will then return the result back to that application program. And this is done via MESSAGE PASSING. 

============================================================
From performance point of view, is Microkernel good or bad?
============================================================
It is bad. There is an obvious overhead in those messages that are exchanged between different processes. So there is the message passing overhead. Now 
this leads to the logical question: (given below)

======================================================================================
what are the advantages of microkernel approach and what made people come up with it?
======================================================================================
1. Things are easier to design. WE do not have the hiearchy that we had in the layered approach, so different services can talk to each other without going
through the Hiearchy but they have to go through the Kernel. So this is one advantage. 
2. You can upgrade parts like the File System or the device driver without updating the Kernel. Updating the Kernel here means "recompiling the kernel". 
   Remember that the Kernel here is written in a high level language. And if the designers of the system make a change to this service then they only need
   to recompile that particular service. They need not to recompile the Kernel. So this is a BIG ADVANTAGE. The kernel is small and it has only the essential
   features, so recompiling the kernel will happen less frequently.
3. The question is what will happen if one of these services fail? If this service fails, it's not crash the machine because the Kernel is still intact in 
   the machine.  So if one service fails, the kernel will not be affected and the system will still be stable and running. So smaller kernel is more stable 
   because if anything goes wrong in any of these services then the whole system will not crash. 
4. Portability advantage. When you have the system divided into smaller pieces, it will be more easier to port to another platform because there are parts 
   that are platform-independent and parts that are platform-dependent. And only those parts that are platform dependent need to be changed when you port
   the system to another platform.

======
One thing to note
=========
In all the OS structures, the user process will ALWAYS have to talk to the Kernel. It is never directly going to talk to another process.

=======
Minimizing the code running in previlleged mode
======
While there is a security advantage here, it is always more secure to minimize the code that is running in previlleged mode. So the code that is running in
previlleged mode or Kernel mode is minimal and there is a security advantage.

==========================
Diagram of a micro-kernel
==========================

   ------------       --------------       ---------------   -
  | Application|     |  File System |     | Device driver |   | user mode
  |  Program   |      --------------       ---------------   -
   ------------         /|\      /|\        /|\     
       / \               |        |          |
    ___ |________________|________|__________|__________________
   |    |       messages |        | messages |                  |
   |    |_ _ _ _ _ _ _ _ |        |_ _ _ _ _ |                  | _ 
   |                                                            |  |
   |   _________________    ___________    __________           |  |
   |  | Interprocess   |   | Memory    |  |  CPU     |          |  | KERNEL MODE
   |  |  communication |   |management |  |Scheduling|          |  |
   |  |________________|   |___________|  |__________|          |__|
   |                MICROKERNEL                                 |
   |_______/ \__________________________________/ \_____________|
            |                                    |
   _________|____________________________________|_______________
  |        \/               Hardware             \/              | 
  |______________________________________________________________|           

==============
Kernel Modules
==============
There is a module approach or a loadable module approach. Now the challenge here is to understand the difference between the loadable module and the 
microkernel. In the loadable module -- it is very similar to the Microkernel in the sense that there is a small kernel that has the essential features, and 
all other services now are in modules. So other services are in modules that are loaded by the Kernel. But the difference is these services (that are not 
in the kernel) they are not loaded as separate processes, they are loaded as modules that are added to the kernel. So they are loaded as Dynamic Link 
Libraries for example.  One way of loading the services is as dynamic link libraries and sticking them to the Kernel. So, in memory, the memory image would 
look like this: 

   ============
   Micro Kernel
   ============
  _____________
  | 
  |------------
  |  service#1 
  |------------
  |  
  |------------ -
  | Kernel       |<---- Kernel Address space  
  |------------ -
  |  
  |------------
  |  service#2
  |------------
  |  
  |------------
  |  service#3
  |------------
  |  

So above, inside the microkernel, we have a small kernel. And then we have other services as separate processes. Like Service#1, Service#2 and Service#3. 
Each one of these is going to be a separate process and each will have their own address space. 

   ===============
   Loadable Module
   ===============
  _____________
  | 
  |----------- --|
  | Kernel       |
  |------------  |
  | Service#1    |
  |------------  |  <---- Kernel Address space  
  | Service#2    |
  |------------  |
  | Service#3    |
  |----------- --|
  |
  | 

 Now, with the loadable module, you have your small kernel, but then you load other services and you stick them to the kernels. So they are kernel extensions.
 In the microkernel, we had multiple address spaces. So this is the difference between the two. 


   ===============
     Monolithic 
   ===============
  _____________
  | 
  |----------- --|
  | Kernel       |
  |              |
  |              |
  |              |  <---- Kernel Address space  
  |              |
  |              |
  |              |
  |--------------|

So it is one big Kernel. Everything is loaded at once. 

    ===========
      Layered
    ===========
  _____________
  | 
  |----------- --|
  | Kernel       |
  |              |
  |              |
  |              |  <---- Kernel Address space  
  |              |
  |              |
  |              |
  |--------------|


The layer looks just like monolithic in memory. Why? Where is the difference between Layered and monolithic? The difference between the two is not in the 
memory image. So, in memory, monolithic and layered are going to look the same. The difference between the two is in the source code. Because layered would
have the hiearchy (so difference source code). So once it gets compiled into an executable or a binary, that binary is going to be a big binary like the 
monolithic. So, in the binary, you will not see the difference.  

==================================================================
Advantages of loadable module compared to Layered and microkernel
==================================================================
1. Memory is an advantage  It is more portable, so you can just plug and play the modules that you'll need for a specific device. And you save memory space by
   not loading up some huge kernel. So memory is a clear advantage. 
2. For loadable modules, you will not have to recompile the kernel if you change one of the modules. So if these are loaded as modules then they are compiled
   separately. And when you load them, you know them as libraries.  And you change service1, you will NOT have to actually recompile the Kernel. 

=========================
Solaris Modular Approach
=========================
   
    device and bus drivers          scheduling classses       file systems 
                          \         |                       /   
                           ---------------------------------
 miscallaneous modules -   |       CORE SOLARIS KERNEL      | - loadable system calls 
                           ----------------------------------
                                |                 |
                          STREAMS               Executable 
                          modules                formats 


=================================================
Detailed description on Modules (from the slides)
=================================================
Kernel provides core services; other services are provided as loadable kernel modules (boot time or run time). 
  - Each core component is separate 
  - Each talks to others over known interfaces
  - Each is loadable as needed within the kernel
- Overall, similar to layered but more flexible, beacuse any module can call any other module (no hierarchy)
- Also similar to microkernel, but more efficient, because modules do not need to invoke message passing 
- Perhaps, the BEST CURRENT METHODOLOGY FOR OS DESIGN.
- Common design in MODERN LINUX SYSTEMS (Solaris, Linux, Mac OS) and Windows.

=============== 
Hybrid systems
===============
No operating system implements only one core structure or takes a single approach. All systems use (or real systems use) mixed approaches. For example, 
Linux and Solaris are monolithic to a great extent because the OS runs in a single address space, but there are loadable modules. By the way, the loadable 
module approach is probably, now in the modern systems, the BEST, but it is not the perfect approach. If you think about the advantages and the disadvantages
compared to other approaches. 
Windows is a good example. It started off as a microkernel because Windows NT was based on the bus (or subsystem?), which was a microkernel. Then they realized
that they had a performance problem. And when they realized that, they fixed that problem by moving more services back into the Kernel, or making the Kernel
bigger. Which means that the Kernel became more monolithic. So now, the Windows Kernel is more monolithic because it is big, but it is still not purely 
monolithic, because it still has some of the microkernel features that they were in the earlier versions. And it even supports loadable modules. So you have a
mix of microkernel, loadable and monolithic because the kernel is very big. And ofcourse, it's a big for performance.


The Apple's MacOS is based on the MacOS that was developed by Carnegie Mello in the 80s but at this time Apple's Mac is not based solely on the MacOS, there is
also the BSD UNIX, as well. So it is a mix of two systems. So it is obviously a hybrid system. And the Kernel is based on two different systems and it's  a
relatively big kernel -- so to a great extent, it is monolithic. But it also supports Kernel extension, so these are loadable modules. So, so far all the 
systems that we have looked at have loadable modules. Like Windows, Linux and Solaris. 

The Apple's iOS for mobile devices (Iphone and ipad) is based on their MacOS. So the core OS is the same as the OS for laptops, but they added some layers
to support the mobile environment, including core services like cloud computing and databases, media services (like graphics, audio, video) and the 
Cocoa touch (which is the user interface for the Apple mobile devices). So it is based on the MacOS, but we will see that they run on the different CPUs. The 
MacOS is run on Intel, while the mobile uses ARM processors. 

======
Android
=======
The android OS is based on a Linux Kernel, and on top of that, they added libraries and runtime systems to support the mobile environment.  Among these 
libraries we have:
1. SQLite : A lightweight version of the SQL database
2. Libc   : It's a lightweight version of the C library 
3. Dalvik Virtual machine for java (because Java is the main language for application development on Android). 

================================
Detailed notes on Hybrid System
================================
Most modern operating systems do not adopt one pure model. 
 - Hybrid combines multiple approaches to address performance, security, usability needs 
 - Linux and Solaris are monolithic, becuase the OS runs in a single address space (for performance) but modular for dynamic loading of some 
   functionalities. 
 - Windows is mostly monolithic (for performance), but with microkernel features such as providing support for different subsysems (persoanlities)
   in user mode; it also supports loadable kernel modules. 

- Apple Mac OS X  is hybrid, layered, Aqua UI plus COCOA programming environment. 
  - Below is Kernel consisting of Mach microkernel and BSD Unix parts, plus I/O kit and dynamically loadable modules (called KERNEL EXTENSIONS). 

==================
Mac OS X Structure
===================

 ------------------------------------------------
  graphical user interface 
                           Aqua
 -------------------------------------------------
 
 --------------------------------------------
  Application environments and services 
       [java]   [cocoa]  [quicktime] [BSD] 
 --------------------------------------------
                                       \|/
----------------------|-------------------------
 kernel environment   |   BSD
                      |   
    Mach              |-------------------------|
                                                |
------------------------------------------------|
   I/O kit             |   Kernel Extensions    |
-------------------------------------------------

===================
More details on iOS 
====================
Apple mobile OS for iPhone, iPad 
  - Structured on Mac OS X , added functionality 
  - Does not run OS X applications nativelly 
    - Also run on different CPU architecture (ARM vs. Intel) 
  - Cocoa Touch : Objective-C API for developing mobile apps 
  - Media services layer for graphics, audio, video 
  - Core services provides cloud computing, databases 
  - Core operating system, based on Mac OS X kernel

===============================
Detailed information on Android
================================
Developed by Open Handset Alliance (mostly Google)
  - Open Source
  - Runs on a variety of mobile platforms 
Similar stack to iOS 
Based on Linux Kernel but modified 
  - Provides process, memory, device-driver management 
  - Adds power management 
Runtime environment includes core set of libraries and Dalvik virtual machine
  - Apps developed in Java plus Android API 
     - Java class files compiled to java bytecode then translated to executable that runs in Dalvik VM 
Libraries include frameworks for webbrowser (webkit), database (SQLite), multimedia, smaller libc.

===================
Android Architecture
====================
    --------------------------------
    |  Application Framework       |
    --------------------------------
    _______________________________________      __________________________
   |             Libraries                 |    |     Android runtime      |
   |                                       |     ---------------------------
   | [SQLite]           [openGL]           |    | [Core libraries]         |
   | [surface manager] [media framework]   |    | [Dalvik virtual machine] |
   | [webkit]          [libc]              |     ---------------------------
   |_______________________________________|
     _______________________________________________________________________
    |                             Linux Kernel                              |
     -----------------------------------------------------------------------
====== 
Conclusion
=======
So, as you can see that all systems are hybrid and that is mainly because of the way that they evolve. 



                                    =====================================================================================
                                     Operating Systems Lecture 11: Processes (Part 1): Basic Concepts and Process States
                                    =====================================================================================

=======
Intro 
=======
This is where we start Chapter#3 of the book. We have already defined ap rocess and we have already said that a process is a program in execution and a 
process is an active entity while the program is a passive entity and if we have multiple instances of the same program or same application running then 
each of them will be a separate process and the OS will not even know that they are instances of the same appication because they may be behaving 
differently (or may be doing different things).

================================
Detailed note on Process Concept
================================
An operating system executess a variety of programs 
  - Batch system (multiprogramm) - jobs 
  - Time-shared (multitasking) systems - processes or tasks 
Textook uses the terms "job" and "processes" almost interchangebly. 
Process : program in execution; unit of work in modern time-sharing systems.
System is a collection of OS processes and user processes. 
Program is passive entity stored on disk (executable file), process is active 
 - Program ecomes process when executable file gets loaded into memory. 
Execution of program started via GUI mouse click or name entry on command line
One program can be several processes 
  - Consider multiple users executing the same program or one user executing multiple instances of the same program.


==================
Process in memory
==================
The program code, also called "text section" 
Data section containing global variables 
Stack containing temporary data 
 - Function parameters, local variables, return values, return addresses 
Heap containing memory dynamically allocated during run time.


===========================
Memory layout of a process
===========================
        __________________
    max |     Stack      |
        |________________|
        |       |        |  
        |      \ /       |
        |                |
        |                |
        |       / \      |
        |________|_______|
        |                |
        |     heap       |
        |________________|
        |     data       |
        |________________|
        |     text       |
     0  |________________|


There is the text section, which is the code section. But what is in the Data, Heap and the Stack?  In data, we have global and static variables. 
On the stack, we have local variables, function parameters, return value, return address and the data that is needed to implement a function call.
Then what is on the heap? On the heap, we have dynamically allocated variables.

==============
Process State(s)
==============
These are extremely important concepts here. When a process is first created and admitted to the system, the system is gonna put it in the "READY STATE", 
which means that it is ready to take the CPU as soon as the OS decides to give it the CPU. So it is like "I am ready". But other processes are ready, 
and there are a limited number of CPUs (possibly one CPU, or 2 or 3 CPUs), but there are 10 or 20 processes in the system.
So the scheduler will be selecting one process for each CPU. So there will be one process that will be run on that CPU. And when the process is selected by 
the scheduler, the state of that process will be changed from "READY" to "RUNNING".  So the process is running. When a process is running, different things
can happen. One possible event is just normal termination. If a process just terminates or comes to its end, it will be in the terminated state. A process 
may request I/O. If a process requests I/O then the system will put in the WAITING STATE. So it will be in the waiting state until that I/O request or until
the device completes processing that I/O request. Then the point is when the device completes the I/O request, the system is not just gonna give it the 
CPU. It will put it in the READY STATE. You NEVER go from waiting to running. You have to be in ready state. So you will be waiting in line, you will be in
that ready queue and then the scheduler will pick one process for each CPU to be in that running state. 

===========
What if an Interrupt happens (or more specifically a timed interrupt happens).
===========================
Now suppose that a process is running then there is an interrupt that put the process in the ready state, what kind of interrupt will that be? That wil be 
a timed interrupt. So if it is a timmed interrupt, you now, the time quantum that the system gave to the process expire then the timed interrupt will get 
triggered and that will put the process in the ready state and then the kernel will take over.


==================
Point to pick up
==================
Point to pick up is that when a process is waiting and the I/O completes,, it is in the ready state, it doesn't directly go into the running state.  Also when
a process is waiting for the CPU, we DO NOT CALL IT waiting, we call it "READY". Even though, in English language we can say that the process is waiting for 
the CPU, but the technical term that is used to describe the state of that process is READY. So when you see "Ready", it means that it is in the READY QUEUE
and it is waiting for the CPU.

==========================
An exception is generated
==========================
when an exception (for an invalid operation) gets generated (which obviously means that the process is running, otherwise an exception would not be generated),
what kind of transition will take place? Which state will it transition? It will go into the TERMINATE state.

============
Going from Running to waiting 
===========
So when a process requests I/O, it is making a system call. So when it makes a system call, the Kernel takes over. And whenn the Kernel takes over, it will
send the request to the I/O device and it will mark the process as "waiting". So it will set the state of the process to waiting. And the process will stay 
waiting until the I/O device completes processing the request and notifies the OPERATING SYSTEM, how does the I/O device notify the system that the process 
has completed? Through generating an interrupt.



                                    =====================================================================================
                                     Operating Systems Lecture 12: Processes (Part 2): Process States and Transitions
                                    =====================================================================================

======================
Example from last time
======================
So, the most important point is that everything goes to the READY QUEUE. When a process is ready to take the CPU, it goes into the READY QUEUE. And, if a 
process gets to the CPU, if it is in the ready queue and the scheduler decides to give the CPU to that process then that pprocess will become running.
Remember: 

  Running = On the CPU 
  Ready   = Ready to take the CPU (but in real life, waiting). 
  Waiting = waiting for an I/O device or an event. 

Now when a process requests I/O or needs to wait for an event then the system is going to put it in the waiting state.  THe process will be in that state,
until that event occurs or until I/O completes. A process will shift to the Ready state after this. A process never gets the CPU directly. This is the main
point here. It has to go to the ready queue, because other processes may be interested in the CPU as well. And it is the scheduler which decides which process 
gets the CPU or which process gets which CPU in terms of multiple CPUs. 

===================
Transition from Running to Termination
================
It could be for a good reason, which is normal termination. A process just completed. Or it could be for a bad reason i.e a process does a bad operation.

===========
When the system is started and the very first process needs the CPU
===========
In that case, yes, that process wil have to go through the READY QUEUE. That is how the system has been designed. Everything has to go through the READY
QUEUE.
When a system launch then it is going to start a bunch of processes (system processes). It is not going to start just one process. Normally, you know, 
if you look at your task manager on Windows, you see a bunch of processes that were launched by the system itslef. 

=====================
Process control Block
======================
So this a data structure in wich the system will track all the information that it needs to track for each process. You have: 
1) THe process state: what is the current state of a process. 
2) Program counter: Which is the address of the next instruction for this process to execute. 
3) CPU register: Values of the CPU registers. 
4) Scheduling Information: Like the priority of a process. And the infromation that the scheduler needs to track for a process. 
5) Memory management infromatioN
6) Accounting information: How much time the process has spent on the CPU. And how much the time the process has spent in total. For example, if a process 
                           has been active for 10 milliseconds, or 10 seconds. But during these 10 seconds, it wasn't on the CPU all this time. It was on the
                           CPU only for part of this 10 miliseconds. So the system compares all this. 
                           Tracking or accounting information is important for scheduling because scheduling decisions are based on the process's past 
                           behavior. So the behavior of the process can be characterized by keeping track of how much CPU time the task spends or how  
                           frequently the task requests I/O, and alll of these process behavior things.
I/O Status information   : Features the I/O devices that the process is currently use. List of open files. Etc. 


=========================================================================================================
Why does the operating system need to store the values of the CPU registers in the process control block?
=========================================================================================================
When a process gets the CPU then ALL REGISTERS are available to that process. 
Anyways, the answer to the above question is: Because when the process is taken away from the CPU, it might need to go back to what it was doing in the CPU 
before, so it needs to reload those values back into the register. 
For each process, in the process control block that belongs to that process...the process control block is an Object. Each process has an Object (or an 
instance of that class which is the process control block) in which the system must store all these values, so that when process#1 gets interrupted (and 
it will get interrupted by the timed interrupt for sure), so when it loses possession of the CPU, the system needs to store these values, so that when 
process#1 gets the CPU again in the future, it will pick up from where it left off. So, this is a very important piece of information that the system needs 
to store in the process control block. 



===============
Detailed note on Process Control Block (PCB)
============
Information associated with each process (Also called TASK CONTROL BLOCK). 
- Process state: running, waiting etc. 
- Program counter: Address of next instruction to execute. 
- CPU Registers - contents of all CPU registers : general purpose, stack reg,...
- CPU Scheduling information : priority, scheduling queue pointers.
- Memory-management information : memory allocated to the process. 
- Accounting information :  CPU time used, clock time elapsed since start time limits.
- I/O status information: I/O devices allocated to process, list of open files

   ____________________________
  |       Process State        |
  |____________________________|
  |       Process number       |
  |____________________________|
  |       Program counter      |
  |____________________________|
  |                            |
  |          Registers         |
  |____________________________|      
  |      memory limits         |
  |____________________________|
  |       List of open files   |
  |____________________________|
  |                            |
  |     ..............         |


===============================
Process Representation in Linux
================================
This slide gives us a feeling of what a real operating system looks like, or reminds us that a real operating system is a program. It is a program that
is written in C. This is Linux code, and this is what process control block looks like on Linux. It is a STRUCT. It uses a C struct that is called a "TASK
STRUCT". It has the following attributes:

Represented by the C structure -- task_struct 

pid t_pid; //process identifier 
long state;  //state of the process
unsigned int time_slice //scheduling information 
struct task_struct *parent;  //this process's parent 
struct list_head children; //this process's children
struct files_struct *files; //list of open files 
struct mm_struct *mm; //address space of this process


Comments: So each process has a unique parent, list of children ( a pprocess can have multiple children), list of open files, and a pointer to another data 
structure that tracks memory management. So memory management is one of hte complicated modules in an operating system.
And for memory management purposes, the memory management has its own data structures. And in the main data structure for a process, we have a pointer to 
that data structure that tracks memory management. 

===================================================
Diagram that shows transitions that take place in OS
=================================================
So, firstly we have P0 executing on the CPU, then it gets interrupted for one of many possible reasons (one reason is timed interrupt, another reason is 
requesting the I/O). For whatever reason, the Kernel gets the CPU and the very first that the Kernel does is: 
SAVING THE STATE OF THE P0.

============================
What does saving a state means
==============================
Saving the values of the register and the program counter (which points to the next instruction to be executed in that process). SO that when this process
gets the CPU again, the system knows fromm which instruction should it continue from. 
And rememer that when the Kernel gets the CPU, the Kernel itself is a program, so it will do some work. So when the Kernel gets the CPU, it will be 
modifying these registesr (like register A to 5 , and register B to 12 etc). So that is why the kernel needs to store these values before the Kernel starts
modifying these values.

===============
Selecting the process to execute next
===============
Then the Kernel will select, y invoking the schedudler, the process to excecute next. Let us call this as P1. Now, system before giving the CPU to P1 is going
to RELOAD THE STATE. It is going to set the values of the registers to the values in the PROCESS CONTROL BLOCK that belongs to P1.  

===========
Where will the PCB physically be? 
===========
It will be in memory. But it will be specifically in the KERNEL ADDRESS SPACE because the PCB is a Kernel data structure that will be physicially inside the 
Kernel address space in memory. 

===========================
Now when reloading the state
=============================
So then, we will reload the state so that you are clear on where the information will be flowing or moving. So in this case, when I say taht the state of 
the process#0, the information of or the data is getting moved from CPU to memory. The values that are on the CPU are saved into the memory. Here, in 
process#1 we are loading into the CPU. We are loading from memory into the CPU, the values of the register. 

==============
Now at some point P1 loses the possesion of the CPU
=================
For one of the different reaosn that we explained ofcourse.Either because it requests I/O or it because it gets interrupted by the timed interrupted. In 
this case, the Kernel will do the same thing. It will save the contents of the CPU registers into PCB belonging to P1. And then the Kernel will do its work. 
And then it will select a process to get the CPU next. And assuming that the process it is going to give CPU to next is P0, the Kernel is going to reload 
the state of the process#0. So now, the transfer here, the data goes froom memory into CPU. From the CPU (which is in memory) into the CPU. 

================
Visualizing this
=================
OK so it looks like this. We have a memory and the Kernel will have its own address space. And in the kernel address space there will be the Kernel code and 
the Kernel data structure (or Kernel data). Among these data there will be: PCB0, PCB1, PCB2 (for different processes) and about many many other
data structure that the kernel will keep in memory. 

======================================================================================================================================
During this time, the Kernel will be doing something. During this "Save and reload" series of events, what will the kernel be doing? 
======================================================================================================================================
So, we are looking for a couple of important things that the Kernel be doing: 
i) It will definitely be invoking the scheduler.  BEcaues in order to decide which process gets the CPU, it must invoke the scheduler. 
ii) Besides scheduling, the mod bit will get flipped from user mode to Kernel mode. When the Kernel first gets the posession of the CPU, it wil change the 
    mod bit to Kernel mode, then the Kernel gives the CPU to another process. Also, when the Kernel gives the CPU to another process, it will change it to 
    user mode. And it is extremely important to do this. If the Kernel doesn't change it to user mode, then it will e giving the user, the Kernel privilege.
iii) Changing the states of the processes. What will the state of P0 change to in our example/ Now first, the state of the P0 was Running then the Kernel 
     will change its state to something else. So if the cause of P1 interrupt was requesting I/O the state of the P1 will be waiting, because it will be    
     waiting for the I/O or waiting for a system service. But if was a timed interrupt (or if the process got interrupted by the Kernel) then the state of 
     P0 will be changed to ready. The system just interrupted it, so that it can give the CPU to another process. And this TIME IS CALLED IDLE. Even though, 
    idle is not the best word to use in this case, because something useful is happening but from user point of view, the CPU now is in possession of Kernel. 
    And this is something that we would like to minimize. And in a good system, or if the system is running efficiently then the 99.9% of the time, the CPU 
    should be in possession of a user process. The kernel should do its job with minimal CPu time. So this is idle in the sense that the Kernel is running on 
    the CPU and none of the user processes are running on the CPU during this time.


==========
Diagram
========
See the end of the video lecture and you will see the diagram.


                                    =================================================================================================
                                    Operating Systems Lecture 13: Processes (Part 3): System Queues, CPU-Bound vs I/O-Bound Processes
                                    ==================================================================================================

===========
Intro -- Threads
===========
We'll talk briefly about threads but there is a whole chaptper about threads (which is chapter 4). In-fact, all general purpose OSs support threads and what 
a thread is a lightweight process. So a process may be divided into threads and that is something that the application designer can do. It is a design 
decision to design an application such that it is multithreaded.  Now when an application is multi-threaded, the OS needs to track the behavior of each thread 
separately. 

==============
Threads
=============
So far, process has a single thread of execution. A thread is a lightweight process. 
Consider having multiple program counters per process: 
 - Multiple locations can execute at once
   - Multple threads of control -> threads 
Must ten have storage for thread details, multiple program counters in PC. 
Details on next chapter (Chapter 4).


So most of the information in the process control block (probably except processID) must be kept on pair of thread basis. So each thread will have its own 
program counter, it will have its own CPU registers, accounting information etc. So most of the information in a process control block is thread specific or
tracked on a per-thread basis. 

==================
Process Scheduling 
==================
We already know about pprocess scheduling and we know very well what a schedudler is. We still don't know what algos OS uses for schedudling. So the scheduler
is part of the OS that selects the next process for each CPU.

==================
Concept of a Queue
==================
In OS there are many queues.
Job queue: list of all processes in the system. This is where all processes will reside. 

Ready queue: Here, the system will maintain the processes that are ready to take the CPU. 

Device queue: Also, each I/O  device will have its own queue where it will keep the processses that are waiting for that particular device. In-fact, not 
only devices have queues, but as we will see later events have queues as well. Events like messages or semaphores will have their own queues. So, in an OS, 
there are a lot of QUEUES and a process will be moving from one queue to another.

=========
Detailed info -- Queues and scheduling 
=================================
- Maximize CPU use, quickly switch processes onto CPU for time sharing. 
- Process scheduler selects among availale processes for next execution on CPU 
- Maintains scheduling queues of processes 
  - Job queue -- list of all processes in the system 
  - Ready queue -- processes residing in main memory, ready to execute. 
     - Data structure depends on the scheduling algorithm
  - Device queues -- processes waiting for an I/O 
     - Each device has its own queue 
  - Processes migrate among various queues.  

=========================================================
Illustration : Ready queue and various I/O device queues
=========================================================

      queue header               
                                 
ready   [head] -> PCB7 -> PCB2 <--|
queue   [tail]--------------------|


magnetic tape unity 0 [] //empty queue 

magnetic tape unit 1 [] //emtpy queue

disk unit 0 -> PCB3 -> PCB14 -> PCB6 

terminal unit 0 [PCB5]      //just one element in the queue.



So we first have a Ready queue. There will always be a ready queue. There is that queue for the CPU -- the processes that are ready to take the CPU. And 
in this case, we have a couple of other queues -- a couple of magnetic tape queues. Here we are emphasizing the point that each device has its own queue. 
And we have a queue for disk 0 and we have a queue for terminal 0.
Processes 7 and 2 are ready. 
Now when disk unit 0 completes processing the request for PCB3, it will generate an interrupt and the Kernel will take over. And then one of the things that 
the Kernel will be doing is: 
i) Dequeing PCB3 from the device queue 
ii) Enqueuing PCB3 into the ready queue

After doing so, it will not give PCB3 the CPU directly (depending on which other processes are in the ready queue and the scheduling algorithm).

====================================
Representation of Process Scheduling
=====================================
Queuing diagram represents queues, resources, flows.


------->[Ready queue] -------------------------> (CPU) ----->            
  |----/\                                           |____
  |______(I/O)<---[I/O queue] <-----[I/O request]<--|
  |__________________________[time slice expired]<--|
  |_________(child executes)<---[fork a child]<-----|
  |______(interrupt occurs)<-[wait for interrupt]<--|


A process starts off in the ready queue. Then at some point, it will get dispatched by the scheduler and it will get the CPU. Then when the process gets the
CPU, many things may happen such as:
i) Request I/O -- when it request I/O it will be in the queue for that I/O device. Then when that completes, the system is going to put that process in 
                  the ready queue.

ii) Wait for an interrupt -- If the process gets interrupted by the system then the process will be placed in that ready queue.
iii) Fork a child -- If a process creates another process, by forking a child for example, so that child tht will get created will not just get the CPU.  So 
                     think about it, you have just one CPU and a process creates another process. It will be impossible for the both to be on the CPU. Because
                     the CPU can have only process at a time. SO when a process creates an another process, the child process will put on the READY queue. 
                     And then the child and the parent will be competing for the CPU (if there is a single CPU). If there are multiple CPUs, the system may 
                     put each process on a different CPU. And they will be executing in parallel.  Later on we will be making a distinction between two 
                     process executing in parallel and two processes executing concurrently. It is an IMPORTANT CONCEPT that will be exxplained in chapter 4.


Now if a process is waiting for something (like an event like a semaphore or a message) then it will be waiting until that event occurs then it will be waiting until that event occurs and it will 
be an interrupt will trigger that and then the process will be placed in the READY queue.


===============
More on schedulers
=================
An operating system may have different kinds (or levels) of schedulers. But the main scheduler that we focus on (and that is always there) is the short term
scheduler or the CPU scheduler. It is that scheduler that looks at the ready queue (and the ready queue is in memory). The ready queue is a Kernel data structure.
 Whether or not it is in the cache, we don't know. But this short term scheduler looks like at the ready queue and according to a certain algorithm, picks 
a process for each CPU.

===============
Long term scheduler
===================
Now some systems may have a long term scheduler that will decide or select a subset of processes to be in the ready queue. 
Now why would the system put a subset of the processes in the ready queue? Maybe because of the memory limitations. 
If memory is limited then this system then the system may not be able to put all processes to keep all processes in the ready queue. So long term schedulers is something that many systems do not
have. It selects which processes should go into the ready queue. 

=======
Our focus
========
Our focus will be on the short term scheduler or the CPU scheduler.


===========
Classification of processes
==================
Now processes can be described as either IO-bound or CPU-BOUND. So this is an important concept that we need to understand. The brief description is that a 
CPU bound process is a process that spends most of its time doing CPU. While an IO bound process is a process that does more IO than CPU.

==========
CPU Burst
==========
The amount of time the process uses the processor efore it is no longer ready. 

============
Explanation
============
P1 [          CPU         ][IO][      CPU     ]
This is the typical behavior of a process. It has a CPU burst. then it requests I/O then after the I/O it has another CPU burst. 

P2 [CPU][I/O][CPU][I/O][CPU....]
And in the other process, we have the following behavior.  When we say I/O, you know here we are describing the behavior of that process. But how long will
that I/O take is not under the process's control. It depends on the system or on the availability of that I/O device and on the speed of that device and how
many other processes are competing for that device, because if more processes are interested in that device then this I/O time will be longer, because the
process will spending more time in the queue for that I/O device. 

=======
Questions from above illustration
========
Q: Is process 1 CPU Bound or I/O bound? 
A: It is CPU bound because it does more CPU. Also process 2 is I/O bound because it does a lot of I/O. 

=================================================
 What kind of program will be doing I/O bound? 
=================================================
In other words, when you look at an I/O bound process, what does that tell you about the behavior or the nature of that program? How would you describe the 
behavior of a program that does that? Well, it is constantly looking for user input which means it is likely to be an interactive program. A program that 
interacts a lot with the user and it will be doing lots of I/O and spending a small amount of time processsing user requests and then requesting more input
from the user. 
What kind of program is CPU Bound? it is a program that does lots of calculations. Like a scientific program. Or image processing progarm. It does a lot of
computation. If it is on the GPU, the GPU is another story BTW. A sceintific program will have charactersitics like this. 

===========
Why do we need to know this in the context of OS?
==========
IN fact when we get to scheduling, we will see how the scheduler will treat these two kind of programs differently. At this point, we need to understand that 
for good system performance, we should have a mix of IO bound and CPU. Or in other words, what will happen if all the process in the system were CPU bound? 
Well the answer is that the queues are going to get very backed up. The queues depends on what kind of processes that we have. So if all the processes in 
the system were like P1 then our Ready Queue is going to get full. There will be lots of processes that are waiting for the CPU because they all wnat to do
CPU, and what will the I/O device queues look like. They will be empty. Because the CRITICAL RESOURCE in this case that the processes are competing for and 
what to spend most of their time on will be the CPU. So the processes will spend most of their time waiting for that resource that is in high demand, which
in this case is the CPU. So big ready queues, and empty device queues. Now with P2, it will be the opposite. So the processes will be spending most of their
time in the I/O device queus, and the ready queue will empty for long periods of time because all processes are not ready. They are waiting on some I/O 
device. 

=============
Resource utilization
=========
Now in terms of resource utilization, if all processes are like P1 then we will not utilize resources properly because they will all be waiting for the CPU. 
The CPU will be over utilized, but all other resources will be under utilized. In P2, the processes will be spending lots of time waiting for I/O. But if 
we have a mix of the two, all the resources will be utilized, and the scheduler will always find some process in the ready queue to give the CPU to. While 
other processes are getting serviced by I/O. So, while some processes are gettign service by IO, another processs or other processes (depending on how many
CPUs you have) will be running on the CPU or disk. 

============================================================================================
Will the OS be able to classify a process as CPU bound or I/O bound? Does the OS know this?
============================================================================================
Yes, it does.  See, an OS does not comprehend processes. It doesn't know what a process is doing. It doesn't have the notion of understanding the semantics
of a process. An OS does not know. But it knows the behavior. And what behavior is how frequently does a process I/O. How much CPU time does that process
demand. And this is something that you can track. You can track the history of the process, the behavior of the process. How often it requests I/O? What was
the length of the last "n"CPU bursts of a process? Or how much the last any times the process was given the CPU? Did it use its entire time quantum. Or did 
it request I/O before its time quantum expires? 

                                    ==========================================================================
                                       Operating Systems Lecture 14: Processes (Part 4): Context Switching
                                    ===========================================================================
===============
One thing to note
==================
Just want to point it out that the scheduler will be giving more time to I/O bound processes. We will understand that when we get to scheduling. 

====================================
Detailed info on Schedulers Types
====================================

=====================
Short term schedulers
=====================
This is also known as a CPU scheduler. It selects which processes should be executed next and allocates CPU. 
- Sometimes the only scheduler in a system (Unix and Windows) 
- Short-term scheduler is invoked frequently (milliseconds) => (must be FAST)

===================
Long term scheduler
===================
It is also known as a job scheduler. It selects which processes should be brought into the ready queue (loaded into memory)
 - Long term scheduler is invoked infrequently (seconds, mintes) => (may be slow)
 - The long term scheduler controls the degree of multipgoramming (number of processes in memory).
 
- Processes can be described as either:
  - I/O Bound process -- sppends more time doing I/O than computations, many short CPU bursts 
  - CPU-bound process -- spends more time doing computations; few very long CPU burts 
- Long-term scheduler strives for a good process mix to achieve balance.

=====================
Medium term scheduler
=====================
Some systems may have this scheduler. And this may takke some processes off the ready queue and store them on the disk. Why? There are two possible reasons:
i) Memory is full, and it is trying to free up some memory. Remember! THe ready queue is an OS data structure that is in memory. 
ii) Acheiving a better balance between CPU Bound processes and I/O bound processes. In this case, the medium term scheduler will take one process after it 
    completes its time quantum, it will swap it out (or in other words, put it on disk). And then it will be a partially executed process. And then later on,
    it will swap it in.

==================
Context switching 
==================
Context switching is, the workk the kernel has to ddo when it switches context. And what is switching context? It is CHANGING THE PROCESS THAT IS RUNNING 
ON THE CPU. So the work that the kernel has to do: 
After the time quantum of a process expires, the Kernel takes over, then gives the CPU to process#2. But the work that the kernel does here includes context
switching which is SAVING the state of the process that lost the CPU and restoring the state of the process that is going to get the CPU. So it will:
- Save the values of the registers
- Save the program counter (so that the system knows where that particular process left off, so it can start it from that point later on). 
 
This is what we call context switching. Now the time that is taken in context switching depends on two factors: 
i) Design of the operating system itself.;
ii) Hardware


i) Design of OS 
   So there are a lot of updates that the system will have to make when it does context switching. IT will have to update many system or kernel data structures.
   What data structures? That depends on how the system is designed. That is why the time for context switching is dependent on the design of the OS. 

ii) Hardware 
    Because what do you do in Context Switching? So we have memory, and we have CPU, it has some registers (like A, B etc), now in the Kernel address space 
    we have process control blocks (like PCB1, PCB2,...), so if our P#1 is losing the CPU, so we need to store the vaalues of the CPU registers into the PCB
    for process#1 and we need to load the values of the CPU registers from the PCB of P#2. So there will be copying of data from CPU to memory and from memory
    to CPU. And obviously, the speed of this will depend on the speed of the memory system. Faster memory system will give us faster transfer of data between
    memory and CPU.

=============
Other factors
=============
Some instructions sets have single instructions that allows you to copy all registers with one instruction. And another insturciton that reloads the values of
all registers from memory to CPU. So if such a instruction set exists then that will increase the speed of context switching. So that is why, context 
switching is hardware dependent. 

=========================
Time of context switching
=========================
In general, context switching takes a few milliseconds. But milliseconds is such a long time. When you say a few milliseconds, how many machine cycles are 
these? How many machine cycles in one milliseconds? Assuming that the machine is 1GHz, each cycle will be 1 nanoseconds. So if it is one gigahertz machine
then one cycle is 1ns. Which means that one millisecond is a MILLION CYCLES. So it is important to have a sense of how long or how short these numbers are. 
So the time taken is not short. We are talking about millions of machine cycles. All the machines these days have the clock speeds in gigahertz, which means
that the machine cycle is in nanosecond, or in the fraction of a nanosecond.

===================================
Detailed info on context switching
===================================
- When CPU switches t another process, the system must save the state of the current process and load the saved state for the new process via a context 
switch.
- Context of a process represented in the PCB. 
- Context-switch time is overhead; the system does no useful work while switching.
- Time depends on OS complexity and design (e.g. memory management techniques). 
- Time depends on hardware support. 
  - Memory speed, number of registers, existence of special instructions, such as a single instruction to load and store all registers 
  - Some hardwawre provides multiple sets of registers per CPU 
- Typical switching time is a few milliseconds.


==================
Multitasking in mobile systems
===================
The idea is that on mobile systems, there are some resource limiations including battery life and the amount of memory. A mobile system has limited memory, 
and battery life is important and because of this the system cannot afford running too many processes in the background.
Foreground is the process that is on the display and you're interacting with it. When you are interacting with one process, there are multiple processes running
in the background. Like your browser and your email. The email server is running in the background and it is receiving messages, or your whatsapp for example. 
When you are not on whatsapp or when you are running some other application then an application like whatsapp that is receiving messages is doing that in 
the background. So not all processes can be run in the background. It can be a process that uses limited resources. So, on each system, like IOS or android,
it has its own limitations for background processes.

============
Detailed info on Multitasking in mobile sytems
=======================================
Some mobile systems (e.g., early version of iOS) allow only one user process to run, others suspend. 
iOS 4 allows:
  - Single foregroundd process - controlled via user interface 
  - Multiple background processes - in memory, running, but not on tthe display, and with limits. 
     - Limits include short task, receiving notifcation of events, specific long-running tasks like audio playback. 
     - Limitation du to battery life and memory usage concerns 

- Android runs foreground and background with fewer limits. 
   - Background process uses a SERVICE to perform tasks 
   - Service has no user interface, little memory usage.



                                    =======================================================================================
                                      Operating Systems Lecture 15: Processes (Part 5): Process Creation and Termination
                                    =======================================================================================


=========
Intro
=========
So in a unix system, each process has a pair. So when the system starts, it is going to start a root process that is going to call "init", 
that is going to be the parent of all processes. And then all other processes are going to be children or grand children of this process. So if you login,
then the login is a process that is the child of the init process. And if you start in a CMD or a shell then that shell becomes the child of a login process,
because the login reads the shell. And if you start an application in the commandline like "emacs" then that application is a child of the shell process.
So each process is a child of another process. And the concept of creating a process is important. 


===========================================
How are processes created in a Unix system
===========================================
So we have the "fork" system call that creates a new process. So we have a process at some point that executes fork. So when it executes fork, or when it 
makes the fork system call, it will start a child process, and then both the children and the parent are running concurrently. And we need to understand 
the concept of these two processes running concurrently. 

===========
Running concurrently
==============
Running concurrently means that they are both active and they are both competing for the CPU. So if the system has a single CPU then it would be impossible
for the parent and the child to be running at the same time. So at any point in time, there is only one process running on the CPU. So the system may be 
doing something like this. 
So first parent gets the CPU then Kernel, then Child then Process#x then Kernel then Child then Kernel then Parent. And we are assuming here that this is on
a single CPU>

================================================
How long is the time quanta/quantum going to be
================================================
As we will see, this time quantum for each process will be in: 10 milliseconds to 100 miliseconds range. Which meanns that this is going to be a small fraction
of a second. Which means that multiple context switches will happen in one second which means that it is something that we will not feel. 

================
In assignment#1
================
We will have a parent and a child. And the parent will be creating a child using the fork system call. In our assignment the parent is going to be a prorducer
and the child is going to be a consumer. The producer will generate some random numbers and write those numbers in a shared memory block. And the consumer 
will be reading these random numbers from that shared memory block. So we will have a consumer and a producer communicating through the shared memory block.


==============
Understanding this method for creating a child
===============
The point is that once the system makes a fork system call, then we will have two processes running concurrently. Now running concurrently doesn't mean that
they are running at the same time. It could mean they are competing for the CPU and switching between different processes. But they will be both active at 
the same time. 

=========
C program forking separate process
===============
The return value for the process will magically be 0 for the child, and it will return a number greater than zero for the parent.  

======================================
Understanding this method of creating a chart
=====================================
When a program makes fork() system call, now the fork system call returns 2 processes that are running concurrently. Now running concurrently doesn't mean
that they are running at the same time. It could mean that they are competing for the CPU. But they will be both active at the same time. Now this fork will
magically return 0 for the child. And it will return number greater than zero to the parent. So one way of illustrating this:
  ___________
 |
 |
P|  A
 |  B 
 |  C
 |-----
 |
 |______
 | A 
C| B
 | C 
 |_____


We have the address space of the parent. And we have ABCD in it. The system is going to create a totally new address space for the child once we fork. And
what will it put in it? It will put a copy of the parent's code. So it is basically cloning. So fork() does cloning. So it clones the parent. It is executing
the same code. Now what if you want a child to run a different program (which is what we do most of the time). We don't want the child to be running the same
code that the parent is running. We want it to run a different program. So that is where we use the "exec()" system call. 

========
exec() system call
========
This system call is going to load any program that you want, into the child's address space. So you just specify the path to that executable. Like this;
execlp("/bin/ls", "Is", NULL);

And then the system is going to replace the entire address space for the child with the code for the program that we will be loading and that will be 
something different than the code of our parent. And this is something that we will be doing in assignment#1. 
So in assignment#1, we have the producer code and then we will use excec() system call to load the consumer. So we will have a producer and a consumer running
concurrently. 
Now each one of these above processes is going to be a different executable. So we will have a producer.exe and a consumer.exe 

=========
C Program forking separate Process
============
#include <sys/types.h>
#include <stdio.h>
#include <unistd.h>

int main () 
{ 
 pid_t pid; 

 /*for a child process*/
 pid = fork(); 

 if (pid<0) { /* error occured*/ 
   fprint(stderr, "Fork Failed");
   return f; 
} 
else if (pid == 0) { /*child process*/
  execlp("/bin/ls", "ls", NULL);
}

else { /*Parent process*/
 /*parent will wait for the child to complete*/ 
  wait (NULL);
  printf("Child Complete");
}
return 0; 
}

=========
About above example
========
In this case, the parent is launching a child and then the parent is not doing anything. It is just waiting for the child. So this example is useful for 
showing us, how the fork, exec pair of system calls can be used. But it is not going to be doing anything useful. So think about it, when there is a parent
that is launching a single child. And then the single child is doing the work and then the parent is not doing anything then what is the point. WE only 
have one process running. Normally we use the forking and multiprocessing in order to run things in parallel or concurrently. We will later understand the 
diff b/w parallelism and concurrency. 
In the assignment we will be doing something useful. Our producer will launch the consumer. And then the producer will not be just waiting. It will produce 
some numbers before going to waiting. And as it is producing, it is expecting the consumer that is running concurrently because the system is switching 
between them. And in-fact if you have a multi-core processor then the system will not be switching between them, most of themm each one of them will be running
on a separate core. If you have a multicore processor then each one of them will have their own CPUs. And then there will be two parallelisms.

===========
Process creation on windows
==========
This is more intuitive. The fork() style is not intuitive. It is odd because you are forking and you are creating a different process and it is hard to 
digest how below that point there are two processes running and they are executing the same code. But the windows style is more intuitive. So the parent will 
create a process and among the parameters that it passes is a complete path to that process/excecutable, and that is it. So it just starts a new process and
that is an executable. And then this is still the parent code. So on windows they are not assuming that when you create another child, the child will be a 
clone of the parent. On windows, they are assuming that whenever you create a child, it will be a different process running a different program. Which could 
be (there is nothing that prevents it) from loading the same program (because it is still an option). But 99.9% of the time, we will be loading the same
program.

==============================================================================================
Is it not a overhead in UNIX that it first clones the process and then we have to override it?
=============================================================================================== 
Anything that happens once is not going to affect the performance. We are not sure why it was designed this way. I don't know why the UNIX assumes that most 
of the times the child will be executing the same code as the parent. 

=========
Continuing
========
Same thing here, it is going to wait for a single object. Of course it is a different system call. The name is different. So again this is not a useful program
because the parent is not doing anything useful before waiting for the child. By the way, when the parent waits for the child, the parent will be in the 
waiting state (which means that it will be off the ready queue -- i.e placed outside it). 


=====================
Terminating a process
======================
When a process terminates, it makes a system call to tell the system that it is terminating. Like the exit() system call. And the exit can return a code that 
indicates the status, like whether it is normal termination or abnormal termination. When the process terminates, the system will deallocate all the 
resources that were allocated to that process, but it will still keep the process table.

============
Process table
=============
It has an entry for each process in the system. It will keep it because the parent may need it. Because usually the parent waits for the child. So usually
the system will keep the entry in the process table, so that when the parent calls wait() then the parent will have access to the exit() status of the child. 


                                    =======================================================================================
                                     Operating Systems Lecture 16: Processes (Part 6): Inter-process Communication
                                    =======================================================================================

========
Intro
=======
There are two main methods for inter process communication. Ofcourse whwne we create multiple processes to do something useful, usually we want them to 
communicate with each other. So we don't just create independent processes. At least you know these two processes belong to the same application, so they
have to communicate and we have to synchronize them. 

==================================================================
Difference between Shared Memory Communication and Message Passing
==================================================================
================
1. Message pasing: 
===============
Here you have two processes. They both will have their own address spaces. And then we will have the address space for the kernel somewhere down in the memory. 
Messages are managed and maintaained and hosted by the kernel. So the messages themselves and everything is in the Kernel address space. And whenever a 
process needs to communicate with another process, it is going to have to go through the kernel and it means making a system call. So, a send or a receive 
is a system call. Every message that is sent or received goes through the kernel. 
___________
|_ProcessA_| ----\
|_ProcessB_|---\ |
|__________|   | |
|          |   | |
|          |   | |
|__________|   | |
| message  |   | |
|__queue___|_</_ |
|m0|m1|m2|.|mn|<-/
|_____________|
|____Kernel___|   


=================
2. Shared memory
=================
It is a totally different scheme. you have two processes in memory. And one of the processes creates a "SHARED MEMORY BLOCK" and that block physically belongs
to address space of one of the processes (usually the address block of the process that creates it). So suppose that "Process A" creates a shared memory 
block and that shared memory block would be put in the address space of process#A. But it will be a special memory block that, under system control, can
be accessed by multiple processes. So remember, it is the KERNEL'S JOB TO DO MEMORY PROTECTION and prevent a process from acessing the address space of 
another process. But by doing shared memory, you are asking the system to make an exception. Or you are telling the system, "OK this memory block is in my 
address space, but other memory processes are allowed to access it". So it is like overriding memory protection for this particular piece of memory i.e 
shared memory. But the good thing about shared memory is once the kernel sets up the shared memory block, the kernel does not intervene anymore.
So unlike messaging passing, in meessage passing there is kernel intervention with every message, but with shared memory, once the kernel sets up the shared
memory block, no kernel intervention is needed.
|Process A     |
|______________|
|Shared memory |<-\
|______________|  |
|Process B     |__|
|______________|
|              |
|              |
|              |
|______________|
|              |
|  Kernel      |
|______________|
====================================================================
Kernel has nothing to do with the format of the shared memory block
====================================================================
So the user processes thatare communicating through this shared memory, they will define their own shared memory format. Or their own COMMUNICATION PROTOCOL.
So if you have a producer and a consumer, there is a certain format that we will define (we as application programmers). So the producer writes the data in 
this buffer in a certain format and the consumer knows that format, and knowing that format will allow it to consume that data or understand what is in 
 the shared memory buffer. So this is controlled by the processes. 


=================
Which will be faster? 
=================
Shared memory will be generally faster. But in certain cases, on a multiprocessing system, beacuse of "CACHE COHERENCE PROBLEMS", shared memory may end up 
become too slow (and slower than message passing). 

============
Cache Coherence / Cache Coherency 
================
It is something that is handled by the hardware.  So the hardware implements its own CACHE COHERENCY PROTOCOL, but that is going to slow shared memory 
communication. Because it you have CPU1, and CPU2 and each CPU will normally have its own cache and then there is a shared memory. And suppose that we have
multiple memory locations. Suppose that we have a memory location with a value of 7, and then both CPUs load this in their cache. So, now, both CPUs cache
have 7. Now, what if at some point, CPU1 modifies this value IN ITS CACHE and changes it to 19. Now what is the problem? So shared memory and CPU2 do not 
have the latest value. Of course the hardware ENSURES CORRECTNESS. So when CPU2 tries to access its own "7" value from the cache, the hardware will recognize
that this "7" is outdated, because the other CPU has modified that in its own cache. So the hardware will detect this, but the hardware will not have a 
magical solution to this. This is going to take time. Because, in other to resolve this, the hardware will first have to copy the latest value (that is 
stored inside the cache of CPU1) into memory, update memory and then load this updated value from the shared buffer into the CPU2's cache. Now this update
will ensure cache coherency. This will take some time. It is not going to happen in zero time. 
Now you can imagine, how slow this whole thing is going to get if you have many many processors. So if you have a system with many CPUs then this will become
slower and slower. So it may become slow to the degree that message passing becomes faster.

                                    ===================================================================================================
                                      Operating Systems Lecture 17: Processes (Part 7): Shared Memory and the Bounded-Buffer Problem
                                    ====================================================================================================

========
Concept
========
After today's lecture, we will have all the material that we need for assignment#1. 


===============================================================
More description on Interprocess Communication -- Shared Memory
===============================================================
- An area of memory shared among the processes that wish to communicate. 
- Typicaly shared memory resides in the address space of the process creating the shared memory segment. 
- The communication is under the control off the user processes not the operating systme. 
- Major issue is to provide mechanism that will allow the user processes to synchronize their actions when they access shared memory. 
- Synchronization is discussed in great details in Chapter 5.
- Shared memory can be faster than message passing because the latter is implemented by system calls (there is more kernel intervention with message passing
  ).
- Shared memory is not necessarily faster than message passing on system with several processing cores, due to cache coherency issues.

=================================================
Producer-consumer problem OR Bounded Buffer Problem
===================================================
So we have a producer and a consumer i.e two processes. A process that consumes data and a process that produces data. In our assignment, the producer will 
generate some random numbers. And how many random numbers? That is going to be a command line argument to that program. Now the producer and consumer will 
be communicating through a buffer. That buffer could be bounded or unbounded. Now, if the buffer is unbounded, the problem is very easy -- in the sense that 
the producer will never have to wait. There is always rooom in the buffer for producer to write. Now if the buffer is bounded, then the producer must wait 
when the buffer is full. So now, what is the point in using a bounded buffer. 

==============================================================================================================
If you are designing an application with a producer/consumer scenario, why would you make the buffer bounded? 
==============================================================================================================
So you want to limit the resources or the amount of memory that you are using. You want to keep your app from just using up all the resources on a computer.

=============================================================================
Why is an unbounded buffer sufficient? Why don't we need an unbounded buffer?
=============================================================================
In many cases, unbounded buffer is not necessary. The point is that when the consumer reads an item, we no longer need that space. We can reuse it. So the 
idea is reusing a bounded buffer. So if you are going to generate a 1000 items, we don't need a buffer than can fit 1000 items. A buffer that can fit 10 
items may be enough. Because the producer is producin, and the consumer is running concurrently and is consuming the items as they are produced. And then
whenever an item is consumed by the consumer, we can just reuse the space. So the POINT IS SPACE EFFICIENCY.

=====================================================================
A bounded buffer solution that does not fill the buffer completely
=========================================================================
So if BufferSize = n. Then the solution that we are about to describe is going to fill n-1 cells. It can never fill all "n" cells in the buffer> Why? This 
will not make sense until we get to "Chapter 5" and attempt to develop a solution that will fill the buffer completely and then face some challenges. So for
now, just accept it that our solution will fill n-1. So if the buffer can fit 10 items then we will only fill 9 items. We have 9 items in the buffer that we 
consider it for. 

==================
Let's take an example
================== 
So let's assume that the size of our buffer is 3. And we will have 2 indexes:  

i) index-in : index of next item to produce 
ii) index-out: index of next item to consume 

Initially: in = out = 0 
              _______                   ______ here, in = 1             _______here in = 2                   _______  here in = 2     ________
in = out = 0 |       |     P(8)      0 |  8   |      out = 0   p(4)    |   8   |    out = 0                0|       |  out = 1 ,P(7) |        |
              -------    ------>        ------                 ----->   -------   full buffer now  C(1)      -------  -------------> |--------
             |       |  //produce    1 |______|                        |   4   |            ----------->    |   4   |                |   4    |
              -------       8        2 |______|                         --------                             -------                  --------
             | ______|                                                 |_______|                            |_______|                |___7____|



======================
One thing to note here
======================
The consumer will always consume the item at the index out. So whatever item that we have at the index out, it will get consumed. Also when we increment these
above indices , we are going to increment them in a CIRCULAR manner. By doing something like : in = (in + 1)%buff_size
Now, in the code that we will be writing, the CONSUMER will wait when the Buffer is empty. 

=========================
Condition for empty buffer
==========================
Now we can generalize both the conditions for an empty buffer, as well as a condition for a full buffer by observing the above illustratons.
 
 in == out? 

===========
Condition for full buffer
==============
(in+1)%buff_size = out


===========================================
Detailed info on -- Bound-Buffer - Producer
==========================================
pseudocode: 

item next_produced;

while (true)  {
       /* produce an item in next produced */ 
      while (((in+1)%BUFFER_SIZE) == out) 
               /*do nothing */ 
      buffer[in] = next_produced;
      in = (in +1)%BUFFER_SIZE;
} 


COMMENTS: So here, the consumer is going to be in a busy waiting loop. As long as this condition is true, the producer is waiting, so it will keep looping. 
          When this condition becomes false, the producer will procede and produce the next item. Then it will increment "in" in a circular manner.

========
Busy waiting
===========
We call the outer loop above as "busy waiting loop", because the producer is constantly checking on this condition. We will see more efficient ways of 
making a process wait in chapter 5. So in chapter 5, we will be looking at more advanced ways of synchronizing a producer and a consumer without wasting 
CPU Cycle. The producer here is WASTING tons of CPU cycles for this condition to become false. 

========================================
Details on -- Bounded Buffer - Consumer
==========================================
Pseudocode:

item next_consumed; 

while (true) {

   while (in == out)
            /*do nothing*/ 
   next_consumed = buffer[out]; 
   out = (out + 1)%BUFFER_SIZE; 
   
   /* consume the itme in next consumed */

Comments: Now, on the consumer side, the consumer will wait when the 
buffer is empty and the condition for empty is : in == out 
Then it will put the next item in the buffer and it will increment "out" in a circular manner.


=========================================================
How to create a shared memory buffer using the POSIX API
========================================================= 
So there are three APIs that we use to create a shared memory buffer. 

#1: There is shared memory open and you can give it a name. You give it a name and there are some flags. The flags that we are interested is create and 
    read/write. So this flag indicates that this is the creation of the buffer. So this process is creating the buffer. So here it means create it if it 
    doesn't exist. And read/write means that this process can read and write. 

   Syntax:   shm_fd = shm_open(name, O_CREAT | O_RDWR, 0666); 

So we are opening or creating a buffer. And then below is truncation. ftruncate below sets the size of the buffer to 4k here. 
    SYNTAX: ftruncate(shm_fd, 4096)

Then the third API maps that buffer into a pointer into the address space of the process that did the shared memory open. So, here you pass the shared memory
file descriptor (this will get returned by this API). It is what you pass to ftruncate and it is what you pass to map. And then below it is saying that map
4k of this buffer into this pointer and this is the file decsriptor AND I NEED THIS BUFFER TO BE WRITABLE.

   SYNTAX: ptr = mmap(0, 4096, PORT_WRITE, MAP_SHARED, shm_fd, 0; 

===================================================
Detailed info on -- Examples of IPC Systems - POSIX
===================================================
- POSIX Shared Memory 
  - Process first creates shared memory segment 
    shm_fd = shm_open(name, O_CREATE | O_RDWR, 0666); 
  - Also used to open an existing segment to share it 
  - set the size of the object
      ftruncate(shm_fd, 4096);
  - Map file to memory
      ptr = mmap(0,4096, PORT_WRITE, MAP_SHARED, shm_fd,0);
  - Now the process could write to the shared memory
      sprintf(ptr, "Writing to shared memory"); 


==================
IPC POSIX PRODUCER
==================
[CODE LOCATED ON FIGURE 3.17 from the book. Description is: Producer process illustrating POSIX shared-memory API.] 
A very simple example for a producer that produces Hello World. 
So the producer here creates a couple of strings and then it creates a shared memory buffer and it sets the size to a certain size. And then it does the mapping.
And it maps in to a certain pointer in the address space of the process that is creating this. Then once you get this pointer. This pointer is going to be a 
void pointer -- which means that it is a GENERIC pointer that you can typecast to anything. So here you can treat that pointer as an array of characters  
which is a string. And the process is writing a couple of strings in it. In the assignment, we will not be doing strings, we will be doing numbers. Now in 
this example, the consumer is going to be reading the strings. In assignment#1, readyOnly will not be enough because the consumer will be modifying it and not
just reading it. WHy? Because our buffer in the assignment 1 will have a header. So our header will have 4 fields in it. And then we will have the body of 
the buffer that is going to have the items. In the header, we hvae the bufferSize, itemCount, int and out. So these int and out indices are going to be 
included in the header. So now it must be obvious why does the consumer need to write to the buffer (the answer is to the change the out index). So if in 
the assignment, we open with readonly then it is not going to work. In fact our consumer is not going to touch any of the values, so it is going to read 
everything, except the "out" becaues this particular number will be modified by the consnumer.

===============================================================================================================
How to the tell the system to open the same shared memory object that the producer made when doing a consumer?
===============================================================================================================
Well it is going to be the name. The name of your shared memory. So if you don't use the same name in the producer and the consumer, it will not do anything. 
If you don't add the create flag on the consumer side then it will fail because it will be trying to open an object that doesn't eist. 

==============================================================
Detailed info on Interprocess Communication -- Message Passing
==============================================================
- Mechansim for processes to communicate and to synchronize their actions. 
- Message system -- processes communicate with each other without resorting to shared variables 
- IPC facility provides two operations 
  - send(message) 
  - receive(message) 
- The message size is either fixed or variable.  



                                    ===================================================================================================
                                      Operating Systems Lecture 18: Processes (Part 8): Other Inter-process Communication Methods
                                    ====================================================================================================

=========
Intro
=========
So now we will be going through other interprocess communication methods that we will not be implementing in the assignment. 


============
Message Passing
================
So this is another interprocess communication method that is implemented by the system. So two processes communicate through the system. By using system calls
or APIs. Like send and receive. 
The FIRST scheme of communication is direct communication. So in direct communication ,you have Process P and Process Q. And they just communicate. So you 
send a Process Id and a message. (Same for receiving). 

     P ------------------Q
        send(PID,msg);


========
Indirect 
=======
Here you send to a mailboxID. 

Send(MID,msg...).


So in this case, you have a mailbox, and you have processes P and Q. 


p  ------------[MailBox]--------Q


==========================
Difference between the two
==========================
With direct communication, a communication can happen or established ONLY BETWEEN A PAIR OF PROCESSES. But in the indirect communication, you have can have
any number of prcesses that can all communicate with each ther. You can even have another mailbx with processes.

=====================================
Demonstration of multiple Mailboxes
=====================================

P--------[MAILBOX] ---- Q 
                 \----R
                \-----S
                \-----T

A ----------[MailBox2]-----B
          /           \----R
P--------/


So here, for each mailbox, you can have many processes. And you can have the same pair of processes communicate using multiple mailboxes. So this is the difference
between direct and indirect.

=====================================
Detailed info  on -- Message Passing
=====================================
- If processes P and Q wish to communicate, they need to: 
   - Establish a communication link between them. 
   - Excahnge messages via send/receive. 

- Implementation issues: 
  - How are links established?
  - Can a link be associated with more than two processes? 
  - How many links can there be between every pair of communicating processes? 
  - What is the capacity of a link? 
  - Is the size of a message that the link can accomodate fixed or variable. 
  - Is a link unidirectional or bidirectional? 

===================================
logical aspects of message passing
===================================
So in our discussion we will care about the logical aspects of message passing. Like direct/indirect. Sychronous and asynchronous and the buffering issues. 
We will not worry about the physical considerations or the physical issues like how it is actually implemented? Like how is message passing implemented? 
So we are only focused on the logical issues.

==================================
Detials on Logical Considerations
==================================
Implementation of communication link. 
  - Physical: 
    - Shared memory
    - Hardware bus 
    - Network 

  - Logical 
    - Direct or indirect 
    - Synchrnous or asynchronous 
    - Automatic or explicit buffering 
  
   - Our focus here is on the logical implementation. 


=========
Details on Direct communication
===================
You send and receive by specifying a processID. And the link is established automatically between a pair of processes. 
Processes must name each other explicitly: 
  - send (P, message) -- send a message to process P 
  - receive (Q, message) -- receive a message from process Q 
Properties of communication link: 
  - Links are established automatically. 
  - A link is associated with exactly one pair of communicating processes 
  - Between each pair there exists exactly one link 
  - The link may be unidirectional, but is usually bi-directional.


=======
Indirect  communication
=========
You specify a mailbox. And two procsses are communicating through the mailbox. 
Properties of communication link: 
  - Link established only if processes share a common mailbox
  - A llink may be assciated with many processes  
  - Each pair of processes may share several communication links 
  - Link may be unidirectional or bi-directional.

=================
Synchronization
=================
Another important concept when it comes to message passing is this. So the communication be synchronous or asynchronous. Or blocking or non-blocking.
Blocking = synchronous.  Non-blocking = asynchronous.

With blocking it is simpler, but it is not necessarily efficient. When a process does "send()", the process will block until that message is received.
And block here means that this process is in the "WAITING STATE". So a process sends, it will be in the waiting state. It cannot do anything until
that message is received. The name is a very good description of what happens i.e blocking.  And same applies to receive. So when you receive, you 
block until the message is received. If the process does receive, and the message is not available yet, that process will be blocked, until the message
becomes vailable.

=========
Non-blocking
===========
With non-blocking, the sender doesn't have to wait, and the receiver doesn't have to wait. So the sender can send and then do other things. So whne this 
send returns, it doesn't mean that the message has been delivered (but in blocking, when the send returns, it means that the message has been delivered).
The non-blocking sender can just do other things.
Now it is more interesting with receive. Now if a process does:

while (...)
{
    result = receive()
    if (result == valid)
         process message
    else  
         do other things
}

It means that it can be much more efficient if the receiver has other things to do. So  the receiver will keep calling receive on regular basis.  
Like is there a message? is there a message? If there is no message then the receiver can do other things (if it has other things to do). ANd if 
there is a message then the receiver will have to process that message.

=======================================================
Non-blocking (especially receivers) is good for servers
========================================================
This is something that can be useful for servers. So the server, a server process will always have other things to do if a message is not available 
yet. It can continuously check to see if there is a message then process it otherwise do other things.

===========
Combination of blcoking and non-blocking
=========
There are 4-different comibnations:
i) Blocking send , non-blocking recieve 
ii) Non-blocking send, blockign receive.
iii) blocking, blocking
iv) non-blocking, non-blocking

============
Rendezvous
===========
If both sender and receiver are blocking then we have a rendezvous. When both processes are going in lockstep that is. 

==============
Blocking send and blocking receive solution to the bounded buffer problem
==============
Now we can have a better solution to the bounded buffer problem. So here the producer will producer will produce one item and will do a blocking 
in send. Which means that this send will not return until a consumer receives it. So obviously in this case, we can only produce one item, send it and then
the consumer consumes that item. After that item has been consumed, the producer will produce the next item.
So obviously, this approach is NOT THE MOST EFFICIENT WAY of doing things. And if the producer and the consumer are going to take time in producing and consuming 
each item, then a SCHEME like that we were doing in assignment 1 where the producer can produce multiple items, while the consumer is working on consuming thme. 
In assignment 1, it is not blocking. Both can be in parallel. The producer can be producing while the consumer is producing other items. And if we have dual 
core CPUs then each one of them will be running on a different CPU and both of them will be making progress.

================
Sychronization Code
=================
The producer-consumer becomes trivial: 
Pseudocode: 

   message next_produced;
   while (true) {
     /* produce an item in next produced */ 
    send(next_produced);
   }

  message next_consumed; 
  while (true) { 
    receive(next_consumed); 
   /* consume the item in the next consumed */ 
  }


=================
Buffering 
==================
When the system doesn't offer a buffer then we must do blockingg send and receive, because in order to do non-blocking OR in other words for this sender to 
send a message and then do other things then that message must be buffered by the system (the system must store it somewhere until it is delivered) and that 
will just enable the sender to do other things. 
Now if there is a buffer and that buffer is limited then the producer must wait when the buffer is full. When the buffer is unbounded then the producer will 
never have to wait. 
OFCOURSE IN REAL SYSTEM -- STRICTLY SPEAKING everything is bounded. Any computer system has a FINITE STATE OF MEMORY. Unbounded is physically impossible. But 
if we have sufficient amount of space then from a practical point of view, it is unbounded. Or if the amount of space that is available is much greater than 
the amount of data that we will be communicating from a practical point of view is unbounded.


============================
Detailed info on Buffering 
============================
Temporary queue of messages attached to the link. 
Implemented in one of three ways: 
1. Zero capacity -- no messages are queued on a link. Sender must block until the recipient receives the message (rendezvous) 
2. Bounded capacity -- finite length of n messages. Sender must wait if and only if link is full. 
3. Unbounded capacity -- infinite length. Sender never waits. 

=======================================
Communications in Client-Server Systems
=======================================
Other methods include 
- Sockets 
- Remote Procedure calls 
- Pipes 

========
Sockets
========
Sockets are endpoints for communication. So the communication processes will create sockets and will be communicating through these sockets. Each socket is 
identified by an IP address and a port number.  Port numbers below 1000 are well known or standard that are used for standar services like Telnet and FTP. 
Address, IP Address 127.0.0.1 is a loopback address. 

-------More details on sockets------------
- Sockets allow low-level unstructured communication. 
- Concentration of IP address and port -- a number included at start of message packet to differentiate network services on a host. 
- The socket 161.25.19.8.1625 refers to port 1625 on host 161.25.19.8
- Communication takes place between a pair of sockets 
- All ports below 1024 are well known, used for standard services (Telnet 23, FTP:21, HTTP:80)
- Special IP Address 127.0.0.1 (loopback) to refer to system on which process is running.

===================================================
Remote Procedure Calls  -- CONCEPTS OF MARSHALLING 
===================================================
It is a procedure call that is actually implemented as a message accross the network. It is a process making a function call, and that function call is 
implemented on another machine (a server) and the system is taking these parameters of that function and marshalling them (packaging them in a way that 
can be transmitted over the network). It packages them into a message and it sends that message over the network. And then that message is received by the 
server that will unpack them.
Unpacking, execution and the returning the result is done on the server side.

======
Execution of RPC
========
The remote procedure calls are done on the server side. And for each procedure, there is a port. So for procedure X we have a port P1. And for procedure
Y, we have a port P2. 

----     ---
Proc     Port
----     ----
X         P1 
Y         P2 
Z         P3 
.
.
. 

Now when an application on the client side calls the system requesting a procedure X or a function X, the KERNEL will receive that request but the Kernel 
doesn't know on which port is this function X implemented. You know for each function, there is a DAMEON on the server side that is listening on a certain
port. So the client doesn't know which port.  So that's why, it will first send a message to the matchmaker on the serverside. And that matchmaker will look
up the corresponding port number.And then it returns that port number to the kernel on the client side. Now the kernel has the parameters  and has the port
number. With the parameters and the port number, it is going to construct a message. With a port number annd the parameters packaged in a certain format 
that can be transmitted over the network and then it will send them over to the client side. Now there will be a dameon at that port listening for requests
and then it will take that message with the parameters packaged. And then it will unpack the message and it will execute the function and it will return the 
results to the kernel on the client side and the kernel will pass them to the application.

==============================================
Pipes -- Last method of interprocess communication
==================================================
There are two kinds of pies: 
i) Ordinaryp pipes / Classical pipes
ii) Named pipes


========
Ordinary pipes
=======
Each pipe has a reading end and a writing end and when a process calls pipe, with an array of integers (an array that takes two integers) the system is 
going to put an address or a FILE DESCRIPTOR for the reading side of the pipe and fd[0]. And a file descriptor for the writing side of the pipe in fd[1].
File descriptor means that the processes can read to and then write from that pipe using the read and write system call. Or the system calls that are used 
for reading and writing to files. 
Once the process get the file descriptor, it can treat that process as a file But with the reading end and the writing end. 
Ordinary pipes can be used for the communication between a parent and a child.

========
Named Pipes
========
These are more powerful pipes. Here the pipe has a name and it can be used for the communication between any two procsses that will not necessarily have 
to have a parent-child relation. 

=======
Matchmaker
=========
It is like a lookup table. it gets a procedure name and it looks up the corresponding port for that function name.  


=================================
Details on Remote Procedure Calls
==================================
RPC abstracts procedure calls between processes on networked systems. 
  - Again, uses ports for service differentiation 
Stubs -- client side proxy for the actual procedure on the server 
The client-side stub locates the server and marshals the parameters (packages the parameters into a form that can be transmitted over a network). 
The server-side stub receives this message, unpacks the marsaled parameters, and performs the procedure on the server.
OS typically provides a matchmaker service to connect client and server
Data representation handled via "EXTERNAL DATA REPRESENTATION (XDR)" format to account for different architecture 
 - Big-endian vs Little-endian.

================
Execution of RPC
================
.
.  LARGE IMAGE. CAN BE FOUND ONLIEN IF YOU GOOGLE EXECUTION OF RPC
.

========
Pipes
========
Acts as a conduit allowing two processes to communicate. 
Issues include:
  - Is communication unidirectional or bidirectional? 
  - In the case of two-way communication, is it half or full duplex?
  - Must there exist a relationship (i.e, parent-child) between the communicating processes? 
Ordinary pipes -- cannot be accessed from outside the process that created it. Typically, a parent process creates a pipe and uses it to communicate with a 
                  child process that it created. 
Named pipes -- it can be accessed without a parent-child relationship. 

=======
Details on Ordinary Pipes
===========
Ordinary pipes allow communication in standard producer-consumer style.
Producer writes to one end (the write-end of the pipe). 
Consumer reads from the other end (the read-end of the pipe)
Ordinary pipes are therefore unidirectional. 
   COnstructed using pipe(int fd[])
Communicating processes use ordinary read() and write() system calls 
Require parent-child relationship between communicating processes
        
       Parent                                 Child
   fd[0]   fd[1]                          fd[0]    fd[1]      
     |      |       ..<____________________|          |                          
     |      |_______________________>                 |            
     |________>()--------------------() <_____________|
                       Pipe 

Windows calls these "ANONYMOUS PIPES"
See UNIX and Windows code samples in textbook.


===================
Detials on Named Pipes
=====================
More powerful than ordinaryp ipes
Communication is bidirectional
No parent-child relationship is necessary between the communicating processes.
Several processes can use the named pipe for communication.
Provided on both UNIX and Windows systems.


                                    ===================================================================================================
                                                 Operating Systems Lecture 19: Threads (Part 1): Basic Concept
                                    ====================================================================================================


=======
Thread 
=======
A thread is a lightweight process. And an application may be divided into multiple processes and each process may be divided into multiple threads. An application
may be single process, single thread, multiple processes, single thread and it could be single process but with multiple threads.
In this chapter, we will understand the difference between multiprocessing and multithreading.  Differences between these two. There are some advantages in 
dividing an application into threads as opposed to processes. But many of the concepts apply to both threads and processes in parituclar PARALLELISM. 
The main diff b/w multiprocessing and multithreading is that when you create THREAD then that will take less time and less additional resources.

=======
Threads creation
========
These are created within a process within one address space. While when you create a process, you create a whole new address space with everything (Data 
and code). While when you create multiple threads within the same process, they share the code  and they share some of the data.

========
Single vs Multithreaded
=========
Each thread has its own code, data , open files, stack and registers. Now when we have multiple threads, the threads are going to share the code and the data.
Now here data, what kind of variables are we referring to? What kind of variables does it need to share.
Does it make sense to share global variables? Global and static. So in this section, we have GLOBAL and STATIC variables that we need to share between the 
threads between the same processes. 


========
local variables
=============
Now local variables are on the stack. So it makes sense to give each thread its own stack. SO each thread is going to have its own stack. But the global
variables are shared. 
Now sharing global variables is the something that we will be taking advantage of in the ASSIGNMENT#2. We will be using the fact that global variables are 
shared among threads within the same process to do fast and cheap communication without having to define a shared memory object. 
IF you have these threads within the same process (or the same address same address space) then they all have access to (or they all can share) the same 
global variables.

========
Synchronized Access
=============
Ofcourse access to these global variables must be synchronized very well.

=========================
Single and Multithreading PRocess Image
==================

//SINGLE THREAD

[CODE]   [DATA]    [Files]
---------------------------
[Registers]        [stack]
--------------------------
  thread -> ~

//Multithread

[CODE]   [DATA]    [Files]
---------------------------
[Reg]  | [reg]   | [reg]  
--------------------------
[Stack]|[stack]  | [stack]
--------------------------
 ~     |   ~     |   ~      <----- THREAD


=========
Modern applications
============
Many applications these days are multithreaded. Operating systems themselves are multithreaded. And you know, multithreading can woork really well in many 
applications where you have a main thread that interacts with the user. And then the actual processing of the tasks can be done in separate threads.
And this done perfect in the client-server architecture. Or in the server.


======
Server
======
A server can have a main thread that receives client requests. Like client1, client2, #3 and so on. 
And then if the processing of a client request requires a fair amount of computation then put it in a separate thread. 
You know like create thread#1 to do the actual processing of Client#1's request. And create thread#2 to do the actual processing of client#2's request. 
The processing of these requests can be done in separate threads. 
Responsiveness is one important advantage of dividing a program into multiple threads. 
The thread that inteacts with the user, we will like to keep it dedicated to inteacting with the user. 

=================================
Multithreaded server architecture
=================================
        (1) request          (2) create new 
                              thread to service
                              the request
[Client]------------>[Server]------------->[Thread]
                      / \  |
                       |___|
                   (3) resume listening 
                     for additional 
                     client requests.


==========
Benefits
==========
1. Responsiveness. 
2. Scalability: Taking advantage of a multiprocessor.
3. Resource sharing.
4. Economy and speed

What is interesting about these 4 benefits is that two of them (2 and 1) can be achieved with multithreading and multiprocessing. So you can get responsivness
by dividing the application into multiple processes instead of dividing it into multiple threads. And you can achieve scalability (which is taking advantage
of a multicore system) by dividing your aggregation into multiple processes. But the advantage of dividing it into multiple threads, as opposed to dividing 
it in multiple processes include:
i) Resourece sharing: This itself has multiple benefits. Like the following: 
        i) When we create a thread, we use less additional resources.
        ii) Easy communication through GLOBAL VARIABLES. 

ii) Economy and speed:  When we create a thread, it is faster becaues it is not creating a whole new process. So there is less resources and less bookkeeping 
                        that the OS needs to do. Less OS data structures are needed to be created and updated. It is quicker and uses less resources.


=======
One thing to note about scalability
========
You cannot achieve this WITHOUT dividing your application into multiple processes or multiple threads.
Let's say that we have an application running on a single process and single thread. 
Now when we buy a computer with Multiple CPU like quad core or 8 core system, this appplication is (assuming that the processors have the same speed) is not 
going to run any faster on a quad-core system. Assuming that the processors on the quad core have the same speed and the same power as a single processor. 
So buying the multicore systems is not going to make the application faster. 

===============
OS is NOT magic
===============
OS is NOT MAGIC. It is not going to divide your application into processes or threads for you. The OS can just run it on just one core then. 


=========================
More details on Benefits
=========================
- Responsivness: May alow continued execution if part of process is block, especially important for user interfaces. 
- Scalability: process can take advantage of multiprocessor architectures 
    
Advantages of threads relative to processes: 
- Resource sharing: Thread share resources of a process. Sharing global variables make communication easier and faster than shared memory or message passing. 
- Economy and speed: cheapter than process creation, thread context switchingg lower overhead than process context switching.

===========================
Concurrency vs Parallelism
===========================
Now another imp concept is this. What do we mean by paralleism and what do we mean by concurrency
Paralleism is two prallels where threads and processes are running PHYSICALLY in parallel on a multicore system. 
While concurrency is what we can achieve on a single CPU when we do TIMESHARING. So a TIMESHARING OS that we have been focusing on, it keeps switching from
one process to another, or from one thread to another.

==============================================
Impact on scheduling through multiple threads
==============================================
When there are multiple threads or when the Kernel/System supports multithreading then scheduling is going too be done at the thread level rather than the 
process level. So the system will switch between the different threads then.
Also there is some kernel time, the kernel cannot switch withoutt spending some time on the CPU doing context switching. But the point is that what we are
achieving  here with this switching on a single CPU is that we are allowing multiple processes to make progress. So the user will feel that  these processes
are running simultaneously (but they are not). But they are not running simultaneously because if you take a snapshot at any given point in time then here, 
only one thread is running. So if you take a snapshot at any given time then there is only thread running so it is not parallelism.

=========================
Parallelism on multicores
==========================
The system, you know , assigns each thread to each core. Then in that case, we will have true physical parallelism. If you we take a snapshot at any given 
time then there are two threads running.

========
Conclusively
==========
Parallelism is the REAL THING. While concurrency is switching between processes to give the user illusion that multiple threads or processes are making 
progress.

================================
Other advantages of concurrency
================================
iF one process/thread runs into an infinite loop then that thread will be wasting its own time quantum, but it will not be affecting other processes or 
threads.

==============================================
Can we achieve real speed up on a single CPU?
==============================================
Can we? If I have two threads, and can running them or doing concurrency achieve real speed up? Like thread1 and thread2 and I run them on a single CPU and 
they take 10 secs when executed sequentially. 


|Single CPU NO-TimeSharing|
----------------------------
   10secs

|Single CPU TimeSharing|
----------------------------
    Can it run in less 
    than 10 secs?

Answer: The key IO.s So I may get real speed up. With Time sharing, they may complete in 8s.  Because with concurrency, even though there is no real parallelism
        on the CPU, but there is parallelism that is happening between the CPU and I/O. So when t1 is running on the CPU, if t1 is doing I/O, while t2 is 
        on the CPU, then t1 will be making progress on the I/O device. So the I/O device is servicing t1. So t1 is actually making progress. Now t2 is 
        utilizing CPU. So if we execute them sequentially, while t1 needs I/O then we will have to wait, and everything will have to wait until the I/O for 
        t1 is complete. But with concurrency or time sharing, we can achieve that level of parallelism with CPU and I/O. Even though at CPU at any given point
        in time, there is only one process on the CPU. But the CPU is one of the many resources that a process needs in order to make progress.

=======================
Balance b/w I/O and CPU
=======================
If we have more balance between the I/O and CPU  then we are more likely to get speed up by doing Time-sharing. So if you give me a CPU-bound process and an 
I/O bound process, then I am more likely to get speed. But if you give me two CPU Bound proceses, then I am less likely to get speed up because if one of them
are CPU bound then they are going to be spending more time in the ready waiting for the CPU. Same thing for the I/O bound (they will be in the queue for the 
I/O device and less time on the CPU. I.E CPU will be underutilized). 

======================
Multicore Programming
======================
We will focus on multicore because many of today's systems are multicore, but the concepts apply to multicore proessor or classical multiproccesors systems
where you have multiple processors on separate chips, not on one chip like multicore.
Now in utilizing a multicore system, we have some challenges. The OS is NOT MAGIC again. It is not going to divide a single threaded application into threads.
There are libraries and programming language extensions that can help you divide your application into multiple threads, but it is all at the application level
and not at the OS level. 
The challenge here is dividing the application. And dividing an application -- you may divide it into tasks (that do different operations) or you can divide it into
different pieces of data on which you apply the same operation.


======================================
Task parallelism and Data parallelism
======================================

Example1: 
  for (...) 
    A[i] += 5; //Add op 
    B[i] *=4;  //Mult op 

Example2: 
 for(....)  {
    A[i] += 4;
}
 
In example 1, one thread does the add and the other thread does the multiply. So that is what we call Task PARALLELISM. 

Another concept is data parallelism. In example 2, we have just a single array and we have divided into chunks, where each chunk will be processed by a
thread. Like Chunk1 by thread1, chunk2 by thread 2 and so on. So when you do something like this (which is something that we will do in assignment#2), this 
what we call DATA PARALLELISM. 

So in task parallelism, each thread is doing a different operation/function but in Data Parallelism, all threads are doing the same operation on different 
divisions of data. 

=============
Challenges -- dependencies and all that stuff
=============
Now in dividing an application into threads, we face challenges.
The main challenge is dependency. Because if two tasks are dependent then you will not be able to execute them in parallel. Or to assign each one of them 
to a separate thread. So for example:

  for(......)  
      A[i] +=5;
      A[i] *= 4;

SO now you have dependency. So dependency, sometimes, prevents you from doing things in parallel. Or atleast it may complicate the paralllization of the core.
So with dependencies, things become more complicated. And it becomes a challenge to parallelize execution. SO you have to account for the dependencies. When
you have dependencies, you may be still able to divide your program into threads, but you have to do MORE SYNCHRONIZATION. You have to do that carefully, 
because you have to synchronize them well, so all of these dependencies are satisfied well, and the execution is correct.
Another imp challenge is balance. If the threads are not balanced, if they are not doing about the same amount of work, or atleast if you don't have a 
significant amount of work in each thread then you may not be able to get a speedup.

Example, if you have a thread1 that does 90% of the work and then thread2 does 10% of the work. In this case, you may not get any speedup. Things may end dup
being more slower than doing things sequentially. WHY? WHy does the overall perfromance of a multithreaded ver of this program runs slower than single
threaded ver? 
When you do multithreading, you have to think about the overhead. Multithreading doesn't come for free. There is the overhead of creating threads, and maintinaing
threads. There is overhead in the system. And there is overhead at the application level. Overhead in synchronization. Or they have to wait or they must wait.
So the time that is lost in the overhead may overweight this small benefit of dooing 90% of the task in Thread1. 
So let's say that sequential version of this program does the task in 10s. And the threaded version does this in 9s, but you will not get the ideal version
because of all these overheads. 
Now testing and debugging is another challenge because debugging a multithreaded program is hard than debugging a sequential application.

=======================================
Detailed info on Multicore programming
=======================================
Parallelism: It implies a system can perform more than one task simultaneously.
Concurrency; supports more than oen task making progress: 
    - Single processor/core, scheduler providing concurrency

Multicore or multiprocessor systems putting pressure on programmers, challenges include:
- Dividing acitivities 
- Data splitting 
- Data dependency 
- Balance 
- Testing and debugging.


                                    ===================================================================================================
                                                Operating Systems Lecture 20: Threads (Part 2): Amdahl's Law
                                    ====================================================================================================


====
Intro
===
Very related to the speed up that we can get from multithreading


======
Scenario
=======
WE have a program and one quarter of this program is not paralleizable because of dependencies. We will call it "S" for sequential (it cannot be parallelized
because it has lots of dependencies). And the remaining 3 quarters are parallelizable because there is enough independent world which is parallelizable.
So suppose that we divide our 3 quarters into 2 quarters. So we have P1 and P2 after division.
Now assuming that we divided it equally, what will the time be? It will be 3/(4/2) = 3/8.

     ______            _____
1/4 |  S   |      1/4 |  S  |
     ------            -----  -----
    |  P   |....3/(4/2)|P1 | | P2 | 
 3/4|      |           -----  -----
    |______|

So my speed up will be:  1/ (1/4) + (3/8)  => 1.6 

So my speed up here is 1.6. 
So this 1.6 tells me that because we have this sequential part, I could not parallelize the whole program. I could parallelize part of it (or 3 quarters of 
it in this case). My speed up was 1.6 and not 2.0.

==========
The above doesn't account for overhead
========
Ideal conditions; we are ignoring : System overhead (which is thread creation, context switching etc). WE are also ignoring process synchronization and communication. 
This concept applies very well to multiprocessing and multithreading. Because we are ignoring the overhead, it is the same concept. In-fact in practice, 
we expect the overhead to be much more in multiprocessing than in multithreading. 
Because creation and context switching for threads is easier than the same for processes.
We are also ignoring the overhead for thread communication and synchronization. 
Another idealization that we are making in this calculation is that the the threads are doing the same amount of work i.e we are assuming a PERFECTLY 
BALANCED LOAD. 
The actual speed up that we get will indeed be (in practical) less than 1.6. So 1.6 here is the upperbound.


===========
What if we have speed up with 4 cores under the same ideal conditions?
==========
It will be: speedup = 1/(1/4) + 3/16 => 16/7 = 2 * 2/7


=======
Conclusively
==========
The maximum speed with 2 cores is 1.6 and the maximum speed up with 4 course is 2.3 under the same ideal conditions.

===========================================
What if we have an infinite number of cores
===========================================
speedup = 1 / (1/4) + 0    
ofcourse if you have an infinite number of cores then you have infinite parallelization. And if you apply limit then you get a zero for the parallelized part
and then you only have to account for the sequential part.
In reality there is no infinite number of cores and dividing a program into threads is harder. In reality it is harder to find independent work that can be 
done by multiple threads.
So EVEN IF YOU HAVE AN INFINITE number of cores, you will not get speed up by a factor of infinity, you will only get speed up by a factor of 4. 
We don't account for system synchronization or system overhead. We are assuming that the load is perfectly balanced. 
In reality, parallel programming tends to be experimental. So you divide your program into threads and there isn't much that you can do with system overhead,
but with synchronization, you try to minimize it. By the way, synchronization is something that we will be studying in detail in chapter 5.

==========
In the end
==========
- In the end you try to balance the load 
- Minimize sychronization 
- You may or may not get speed up (depending on how much independent work there is in the program). 

===========
Details on Amadhl's Law
===============
- Identifies performance gains from adding additonal cores to an application that has both serial and parllel components
- S is serial portion
- N processing cores
                       Speedup <= 1/ S + (1-S)/N 

- That is, if application is 75% parallel / 25%serial, moving from 1 to 2 cores result in speed up of 1.6 times 
- As N approaches infinity, speedup approaches 1/S 

  IMPORTANT!: SERIAL PORTION OF AN APPLICATION HAS DISPROPORTIONATE EFFECT ON PERFORMANCE GAINED BY ADDITIONAL CORES.



                                    ===================================================================================================
                                     Operating Systems Lecture 21: Threads (Part 3): User-Level & Kernel-Level Threads, Thread Libraries
                                    ====================================================================================================


=======
Intro
========
Now, the relation between user thread and kernel threads. Most of the time we use thread libraries. Like in assignment#2, we will be using a library that 
uses POSIX standard. POSIX thread that is what we will be using. There are other libs like Windows Threads and Java threads. 
Now these thread librareies allow users to define threads. But how it does the mapping between user level threads and kernel level threads? Now most modern
OS support threads at the Kernel level. S

=======
Multithreaded Models
=======
The mapping between the user level threads and the kernel level threads can be done in one of the following ways: 
i) Many to one 
ii) One to one 
iii) Many to many 


=====================
What does this mean? 
=====================
Many to one -- means that you map all user threads into one kernel thread. Which is something that only older systems used to do. But it is not giving 
               you true paralellism. And if one thread blocks, it is going to kill or block all other threads. Some of the older systems used to do this. 

One-to-one -- modern systems do this. Like Windows, Linux and the newer versions of solaris. So they just map each user level thread into a Kernel level 
             thread. And in this case you can have maximum parallelim. But the problem is that it is not setting a limit on the kernel level threads. Usually
             we want to minimize # of Kernel level threads because there is an OVERHEAD. System overhead when a process has too many threads within it. 

==============================================
How do we limit number of Kernel level threads
==============================================
Well we may map an arbitrary number of user level threads into a fixed or limited number of kernel level threads.So like we can have 4 user level threads
mapped into 3 kernel level threads. So this will TRY TO achieve a balance between many to one and many to many.
So you are still getting some true parallelism. Parallelism or concurrency. If you have 3 cores then this is parallelism (since 4 user level threads are 
being mapped onto 3 kernel level threads) otherwise it is just concurrency.
But you have this parallelism or concurrency but it is limited. So you are limiting the number of threads to limit the overhead of thread creation and 
maintenance.

===========
Two level model
==============
In this we have the many to many model. So we have many user level threads mapped into many kernel level threads. But some user level threads may be 
mapped into their own kernel level threads. So this is another scheme where you try to get the advantages of both schemes. 


===============
Thread libraries
=================
We will be using a library that implements POSIX standards. 
We have the concept of asynchronous threads and synchronous threads.
Asynchronous -> the parent thread and the child thread are running in parallel. 
Synchronous -> Parent thread is going to wait for the child thread. 

===========
What we will be doing
=============
We will have a parent that will create multiple threads that will be rnning in parallel but we will do some syncrhonization because at the end the parent
will be waiting for the child thread. Because the parnet will look at all thread results and it will print the final result based on the result of all threads.
So this is very high level and very vague description. 
This will make more sense in assignment#2. 


=========
Details on User level and Kernel Level threads
=============
User threads -- management done by user-level thread library. 
Three primary thread libraries: 
- POSIX Pthreads 
- Windows threads 
- Java threads 

Kernel thread -- supported by kernel
Virtually all general purpose operating systems support kernel threads: 
- Windows
- Solaris
- Linux 
- Tru64 UNIX 
- MAC OS 


========
Details on Many to many
=========
Many user level threads mapped to single kernel thread
One thread blockijng causes all to block
Multiple threads may not run in parallel on multicore system because only one may be in kernel at a time
Few systems currently use this model.

- EXAMPLES: 
   - Solaris Green Threads -- adopted in early versions of Java
   - GNU Portable Threads 


==========
Details on One to One
==========
Each user levle thread maps to kernel thread
Creating a user-level thread creates a kernel thread
More concurrency than many to one
  - If a thread blocks, it does not block others
  - Can utilize multiprocessing 
Number of threads per process sometimes restricted due to overhead 
Examples: 
 - Windows
 - Linux 
 - Solaris 9 and later 

Visually:   ~ -> (K)
            ~ -> (K)
            ~ -> (K)
            ~ -> (K) 

=========
Many to many model
=================
Allows many user level threads to be mapped to many kernel threads 
Allows the operating system to create a sufficient number of kernel threads, application specific or machine specific (e.g. more threads on systems with more
cores). 
An attempt to mitigate the shortcomings fo the other two models.

===============
Details on Two Level Model
=====================
Similar to many-to-many, execpet that it allows a useer thread to be bound to kernel thread
Examples: 
  - IRIX
  - HP-UX 
  - Tru64 UNIX 
 - Solaris 8 and earlier 


==========
Details on Thread Libraries
===========
Thread library -- provides programmer with API for creating and managing threads. 
Three main thread libraries: 
 - POSIX PThreads 
 -  Windows 
 - Java
 
Asynchrnous threading: parent creates child threads and resume execution 
  - Typicalyl there is little data sharing (e.g. multi-threaded server).

Synchronous threading: parent creates child threads and waits for children to complete (fork-join strategy). 
  - Typically, there is significant data sharing (e.g. parent combines results generated by children).

====
Pthread
========
A POSIX stnadard (IEEE 1003.1c) API for thread creation and synchronization
Specification, not implementation 
API specifies behavior of the thread library, implementation is up to the library developeres 
Common in UNIX operating systems (Solaris, Linux, Mac OS X).



                                    ===================================================================================================
                                         Operating Systems Lecture 22: Threads (Part 4): Thread Creation and Cancellation
                                    ====================================================================================================

=====================================
Asynchronous vs Synchronous Threading
=====================================
Async means that the parent thread will create a child thread and both will do things in parallel. While synchrnous means that there is synchornoizaiton and 
the parent will wait for the child at some point. In fact waiting for hte child is only method of synchronization or it is only thing that we can do to 
synchronize parent and child. We will be learning other methods of synchronizing parent and child. 

======
Introducation related to assignment
=========
Our parnet will be creating multiple threads. It will search an array. An array will be divided into a number of divisions (like 4 divisions for ex) and each
thread will be processing or searching. ANd the divisions will be equal (this is an example of DATA PARALLELISM). 
We will need some synchronizaiton. The parent is going to create the child threads and at some point it will have to wait because it will need the results of
these threads. 

=======
Pthreads Example
=======
Here is an example for creating threads using the POSIX API.
So basically there is an API for creating a thread. One Linux it is : pthread-create 
On Windows it is called : createthread 
Then among the parameters for the pthread-create is a function-name.  And this function will be a function that we define. ANd this function is where the
start will start executing. And ofcourse there is nothing that prevents you from defining many other fuctions that this function is going to call. 
So think of this function as being the main function of your thread. So this is the idea. 
Now conceptually, once you do pthread-create and this returns successfully. So the system completes this request successfully. There will now be a parnet 
thread that will start executing from the line after the pthread-create(...,fx,...)   and in parallel the child will be executijng from the function fx.
The first line in function fx. So from that point on. Now these will be running in parallel. aND THEY WILL be two threads in the system (competing for hte
resources and competing for the CPU) and we cannot make any assumptioon about the relative speed. 
So we cannot say that the function fx cannot execute its first line until the main thread is on line 4. They're just running in parallel. And if these are 
running on a single CPU then you cannot get two parallelisms. The system will be switching. And we don't know when it will be switching. So it may excecute
the first 2 lines for the parent and then it will execute 10 lines for the child. But it could also be that the child could execute first and then the parent
runs after 3-4 lines of child have been excecuted. 
There will be other processes and other threads running on the system, so each one of them will be getting some CPU cycle, but if they are running on two
different CPUs or cores then things may be happening in parallel.
So this is the concept. 

=====
code example // CODE FOUND IN THE BOOK
========
This example is a single example that shows a parent that creates a child to calculate the sum of numbers from 1 to n. And n is a Command line argument. So 
if you pass 100, it is going to sum the numbers from 1 to 100. 
So the parent is going to create a child. To create a child, it takes 4 parameters. 
i) first parma is the theadId. It is an ID assigned by the system to this thread. We are not passing. 
We are passing to the system a pointer to an integer that the system will be storing the child id in. So we are telling the system to assign a thread ID to
the thread and then write the threadID into the integer to which we are passing a pointer. 
ii) Then we pass an attribute data structure. There are two different types here:
    - One type is the threadID/tid which is basically an integer. And then there is an attribute data structure. And an attribute will have certain attributes
      for the child threads. Among these attributes are scheduling attributes that we will be studying later. For now, we will just set the attributes to the 
      default attributes by calling the pthread in it to initialize the struture to the default attribute format. So now we are not setting any default attributes.

iii) Param 3 is the name of the function (FX).
iv) Param 4 is the arguments that you pass to this function. Now what are the type of these arguments? The type of these arguments is a void pointer. Why 
    a void pointer? TO make it generic. SO you can pass anything that you want. So you can pacakage all the parameters that you need to pass to pass in a
    structure or object then you can pass a pointer to that object as a parameter argument. Ofcourse from typing and static type checking point of view is 
    not a good thing because C is not good at static type checking.


So now the parent creates a child and then it will do nothing and then it will do pthread_join (which is like waiting for the chil). So the parent is not 
doing anything useful in between. B/w the creaiton and the join. It is not achieving any parallelism. We create threads to take advantage of a multicore 
system. But here it is not useful because the parent is not doing aything between create thread and joijn line. So the creation of a child can be useful
only if the parent does something before join or the parent creates multiple-children. Here it is just like having one company with one manager and one 
technical guy and the manager's only job is to boss that technical guy. If there are two technical people then it will make sense. s
So here the runner is doing the summation and it is storing them into sum. Now the question is how does the parent print the same sum that the child is computing?
We communicate through a GLOBAL VARIABLE. And we said that the global variable is shared among the threads within the same process. The child just writes sum to
it and the parent just picks it up. 
What will happen if I delete the pthread_join or remove that line? will it work correctly? NO! If we remove p_thread_join() (it is a synchronization method) 
and do print here then we may or may not get the right sum. We're synchronizing them by doing pthread_join. We are syaing taht it doesn't make sense for 
the parnet to print the sum until the child has completed calculating that sum. 
There is still a chance of printing hte right sum because if the system happened to give enough CPU cycles to the child to perform all of this work and calculate
or compute the sum correctly before the system executes the printf then it will work correctly. But without join there is no guarantee that it will work
correctly.
This is the problem with parallel computing. SO it is unknown how fast the child will be running relative to the parent. We cannot make any assumption whether
most of the CPU cycles will be going to the child or the parent. 
Also if you delete pthread_join and then run it several times then maybe sometimes you'll get the right answer and sometimes you'll not get the right answer.
It depends on how fast this child will go.
In the assignment, we will not be creating a single child, we will be creating multiple children. So our pthread_create is going to be in a loop to create k
threads or k children with each child assigned a division of the array. So that is why in the assignment when we do pthread_join again we will put it in a loop. 
So our creation and joining is going to be in a loop.


================================
Windows Multithreaded C Program // CODE FOUND IN THE BOOK  
================================
It is same concept exccept the system call that creates the thread is called CreateThread , and the way that we are waiting for a child is called a "WaitForAsingleObject",
it is a generic function that can wait for many different things, including waiting for another thread. 
So the concept is the same, the syntax is just different. 

===================
Implicit Threading
===================
There are some concepts that we should be aware of. So far we have been creating threads explicitly. When we do pthread or create thread , we make a systemcall
or an API call. But there are methods for creating threads implicitly through language extensions or libraries. It doesn't mean that the OS creates the threads
implicitly for you. But there are libs that create threads for you. 


========
Thread Pool
==========
A thread pool is creating a number of threads before hand and then reusing these created threads up to a certain limit. For example, you create 10 threads. 
And if you are a server with a main thread. And creating a thread per client. Now instead of creating a new thread each time for a new client, we reuse one 
of the 10 threads that are available. So if we need an 11th thread then we need to wait until one of the 10 threads become available. So the idea here is 
to limit the number of threads because when the number of threads is too large then there is a system overhead and that may degrade perfromance. So we would
like to limit the number of threads. Also, we would like to avoid creating a new thread each time. So we create a certain number of threads before hand and 
then we keep reusing it. 

=========
OpenMP
=========
- OpenMP is a language extension or COMPILER DIRECTIVES. Like:
   #pragma omp parallel {code}
- This allows you to pull the compiler to certain sections in your code that you think are parallelizable. So you are telling an EXTENDED COMPILER (and not 
  a standard compiler) that you think that this code can be parallelized for you, and then the compiler will generate multiple threads and it will parallelize
  that code section if possible.
- This extension requires compile time support as well as runtime support. 


==================
Details on OpenMP
==================
- Set of Compiler Directives and an API for C, C++, FORTRAN
- Provides support for parallel programming in shared-memory environments
- Identifies parallel regions -- blocks of code that can run in parallel

#pramga omp parallel
 Create as many threads as there are cores 

#pragma omp parallel for 
  for (i = 0; i<N; i++) {
     c[i] = a[i] + b[i];
  }

Run for loop in parallel

==============================
Concept of Thread Cancellation
==============================
Thread cancellation is something that we will be doing in the assignment. 
So in the assignment, the parent will be creating  multiple threads to search for something and if one of them finds what we are looking for then at that 
point we no longer need the other threads, so the parent is going to cancel the remaining threads because we just found what we found. 
So if the fourth thread is searching for something in parallel and once one of them finds what we are looking for, we will no longer need the other threads.
Cancelling threads can be done asynchronously. So the parent does cancel and the thread gets cancelled right away. The problem with that is that a thread
may get cancelled at a bad point (where it  has not freed some resources that it has allocated). 
So that is why, a better scheme is "Deferred cancellation".

====================
Deferred Cancelation
====================
In deferred cancellation, the parent will send cancellation request and the child will be checking for cancellation requests. And if there is a cancellation
request, the child is going to first do the right clean up or clean the resources that it has allocated. And then it will cancel itself. The point here is 
to make sure that every child does the right clean up before it is cancelled. 

===============================
Details on Thread Cancellation
===============================
Terminating a thread before it has finished (e.g. multithreaded DB search) 
Thread to be canceled is "targeted thread".
Two general approaches: 
  - Asynchronous cancellation: terminates the target thread immediately (difficulty reclaiming resources). 
  - Deferred cancellation: allows the target thread to periodically check if it should be cancelled and terminates itself cleanly 

Asynchrnous cancellation not recommended in Pthreads documentation 
Pthread code:
  //given in the book 



=====================
pthread_testcancel()
=====================
This like checking if there is cancellation request from the parent. If there is then the child is going to invoke a cleanup handler to do the cleanup. And 
then it will cancel itself.

=======================================
Details on Thread Cancellation (Cont.)
======================================
Invoking thread cancellation requests cancellation, but actual cancellation depends on thread state. 
If thread has cancellation disabled, cancellation remains pending until thread enables it. 
Default type is deferred:
  - Cancellation only occurs when thread reaches "Cancellation point"
     - invoke pthread_testcancel()
     - then invoke cleanup handler if there is a cancellation request

====================
Thread-Local Storage
===================
Some systems allow each thread to have its own private data. And that is not LOCAL data. So local variables are accessible only by the function in which
they are declared. But "Thread-Local Storage", you can think of being Global that are accessible only by that THREAD and are NOT accessible by other 
threads (unlike the real global variables that are used for establishing communication between threads).

===============
Windows Threads
================
Windows implements one to one mapping b/w threads. Each thread has a : 
i) Thread id
ii) set of registers
iii) the stack
iv) It's own private data storage (aka Context of the thread). 


==============
Linux Threads
==============
In Linux, there is the notion of a task, and the task could be a process or a thread  -- depending on how much resource sharing there is. 
There is a clone() system call that creates a new task. And you are allowed to control the amount of resource sharing -- like sharing file system 
info or sharing memory. So if you clone with no sharing then you are CREATING A NEW PROCESS. And if you do clone with sharing then you are creating a 
thread.
So the system here supports multiple levels of sharing. 
pthread_create() internally uses clone.
In the assignment, global variables and code (such as functions) will be shared by all the threads.

=====================================
Detials on Thread Local Storage (TLS)
====================================
TLS allows each thread to have its own copy of data. 
Different from local variables:
 - Local variables visible only during single function invocation. 
 - TLS visible across function invocations. 
Similar to "static" data but unique to each thread. 
Supported in Pthreads, Windows and JAVA.

==========================
Details on Windows Thread
=========================
Windows implements the Windows XP -- primary API for Win 98, Win NT, Win 2000, Win XP, and Win 7 
Implements the one-to-one mapping, kernel-level
Each thread contains:
 - A thread id 
 - Register set representing the state of the processor. 
 - Stack 
 - Private data storage area used by run-time libraries and dynamic link libraries (DLLs)

The register set, stack andd private storage area are known as the context of the thread. 


=============
Linux Threads
=============
Lnux refers to them as tasks rather than threads. 
Thread creation is done through clone() system call
clone() allows a child task to share the address space of the parent task (process)
 - Flags control behavior   

  ---------------------------
|   Flag     |   meaning
 ----------------------------
CLONE_FS      file-system information is shared 
CLONE_VM      The same memory space is shared
CLONE_SIGHAND Singal handlers are shared 
CLONE_FILES   The set of open files is shared 



- Struct task_struct points to process data structures (shared or unique). 


                                    ===================================================================================================
                                      Operating Systems Lecture 23: Process Synchronization (Part 1): The Critical-Section Problem
                                    ====================================================================================================

=====
Intro
=====
We are at Chapter 5 i.e Process Synchronization. When you have multiple threads running in parallel and perfomrming certain tasks, you will need to 
synchronize them to ensure the consistency of safe data, and to make sure that you are getting the right results. 
So there are basically two fundamentally different kind of synchronization problems:
i) The first kind of synchronization problem is the critical section or the base condition kind of problem. Where you have multiple threads/processes 
   accessing some shared data. And you want to make sure that the data is consistent and these processes are not causing conflicts, or creating conflicts
   or conflicting with each other or undoing each other's work. 
ii) The other kind of syncrhonization problem is dependency. Where one process depends or needs the results of another process. 

NOTE: All the synchronization concepts, here, apply to multiple processes or multiple threads. Same concepts apply to multiprocessing or multiple threads.
      This is why, we can use Threads and processes interchangably in the context of synchronization. 

============================================================================
Why did we not fill the buffer completely in the producer-consumer problem?
===========================================================================
If the buffer size is 100, we are only filling 99 (which we consider full). Now let us attempt to fill the buffer completely and see what would happen. 
We will see what issues and what problems we will be facing. 


======================================
Candidate Solution to the above problem
========================================
counter variable here refers to the number of variables that have been produced but have not been consumed yet. Which means that the counter must be incremented
by producer. And the consumer will decrement the counter. 
So our conditions for full and empty will be simple. Full means counter  = buffer size. When counter = 0 then the consumer will have to wait and it means that 
the buffer is empty.

What is the problem here? The problem is that we have 2 processes accessing a shared variable. And these two processes may be attempting to modify this 
shared variable at the same time (i.e counter). So there is nothing that is preventing the producer and the consumer from trying to modify this at the same
time. Remember that whenever we have two processes running in parallel, we can never make any assumption about the relative speed. So we can never assume
that while the consumer is trying to modify this shared variable then the producer is doing something else. The producer could be trying to increment 
the counter at the same time that the consumer is trying to decrement the counter.


============================================================================
What would happen if both are trying to modify the counter at the same time?
============================================================================
Now the key ideas here are: 
when you do counter++ then this is one line of code in high level code. This one line will typically be translated into multiple instructions by the compiler,
because this is incrementing a variable in memory. And on many machiens, you cannot perform this arithmetic on a variable in memory by 1 instruction. On many
machines, you will need to load counter from memory into a register. Then increment the register and then store. So this would be the machine code:

i)load counter -> reg
ii)reg = reg + 1
iii)store reg -> counter


And now, since we cannot make any assumption about the relative speed, we don't know when a process may get interrupted. An interrupt may happen at any time.
It may get interrupted on line (i) or it may get interrupted on line (ii) or so on. So we cannot write our code such that the interrupt can occur  at any 
point in the code. And we want to make sure that everything will work correctly. Now if we don't do this, then we may run into problem.


=====================================
What kind of problems can we run into
=====================================
Let's assume that we have a single CPU, then assuming that the  value of counter is initially 5, then the producer may load 5, then it may increment the 
register to 6. Now the value of reg is 6. But the value of counter is still 5 at this point. Now what if the producer gets interrupted at this point and 
the CPU is given to the consumer? Now the consumer is going to load the 5 and decrement the 5 to make it 4 and then maybe the consumeer get interrupted at
this point and the CPU is given to the producer and the producer at this point will just store the value of 6 in the memory. Then the consumer will eventually
get the CPU and it will store the 4 in memory. Now if 4 the right answer here? No, 5. Beacuse we have an incrementation and a decrementation and we expect
one of them to happen before the other.
Another possibility is that the producer code executes in last and the consumer code executes before it then we end up with a "6". (see book's text on 
Race Condition in threads chapter).


======================================
How can we solve these above problems?
======================================
Well, we need a synchronization method that will ensure that we will get the right answer regardless of what happens in the system. Under all conditions, we
want to get the right answer. We don't want to be in this situation where multiple answers are possible. And this is why, parallel programming is hard to 
debug, because you may get different results DEPENDING ON HOW THE INSTRUCTIONS ARE INTERLEAVED.

============
Conclusively
============
So this is what we call the critical section problem. So in this case, our section is the modification of this shared variable. So usually, the code that 
modifies a shared variable is a critical section that must be handled carefully. And how carefully should we handle it? We should handle it in a way that 
will esnure that only one process can be in a critical section at any given point in time. 
So if modifiying counter is in critical section, then we want to make sure that only process can be modifying counter at any given point in time. And 
if the consumer is modifying counter, we want to make sure that the producer is not allowed to touch the counter until the consumer is done with its 
modification completely. Until it completes the modification of counter. This is what we call the critical section probolem.


======================================
Details on critical Section problem  -- Race Condition related
======================================
Consider system of n processes {p0, p1,...pn-1}
Each process has critical section segment of code. 
 - Process may be changing common variable, updating table, writing file, etc.
 - When one process in critical section, no other may be in its critical section 

Critical section proble mis to design a protocol to solve this. 
Each process must ask permission to enter critical section in "entry section", may follow critical section with "exit section" then "remainder section".

==========
Simple solution
===========
For this particular form of the problem there is an easier solution. But there are more complicated synchronization problems than this. So this is the most
fundamental synchronization problem. 
So with the critical section, we would like to have an empty section where a process locks or reserves the critical section to prevent other processes from
accessing that critical section while that process is modifying the shared variabl or in that critical section. Then there is an exit section where the process
will release the lock which will allow other processes to access the critical section. Then the remainder section is the rest of the code that doesn't access
the shared variable and doesn't have any problem.

========================================
Our solution to critical section problem
=========================================
Okay, so in our solution or the solutions that we will be studying for the synchronization problems in general, there are three important conditions:

1. Mutual exclusion: we want only one process to be in the critical section at any given poijnt in time. 

2. Progress: We want to make sure that we are making progress and that we are not in a deadlock. If we are not making progress then we are in a deadlock.
             And we want to make sure that waiting is bounded.
                   
3. Bounded waiting: If waiting is not bounded we get starvation. So these concepts will become clearer when we get into specific examples. But bounded
                     waiting means that if there is a process that is waiting to the enter the crit section. There is a limit on the number of terms that other processes
                     may get into the critical section while this process is waiting. So a process is guaranteed to wait for a finite or limited number of 
                     other process accesses. Now we will see where that there are situations wher ethis is not the case and where we have starvation. So 
                     the waiting process may starve.


===============================================
Details on Solution to Critical Section Problem
================================================
1. Mutual Exclusion - If process Pi is executing in its critical section, then no other processes can be executing in their critical sections.
2. Progress - If no process is executing in its critical section and there exist some processes that wish to enter their critical section and there exist some
              processes that wish too enter their critical section, then the selection of the processes that will enter the critical section next cannot be 
              postponed indefinitely, and only those processes that are not in their remainder section are considered. 

3. Bounded waiting - A bound must exist on the number of times that other processes are allowed to enter their critical sections after a process has made a 
                     request to enter its critical section and before that request is granted. 
                        - Assume that each process executes at a nonzero speed. 
                        - No assumption concerning relative speed of the n process.




                                    ===================================================================================================
                                     Operating Systems Lecture 24: Process Synchronization (Part 2): Peterson's Solution
                                    ====================================================================================================

======
Intro
======
Today we will be discussing diff solutions to the critical section problem.  So if there is a process that is modifying a shared variable then you don't want
any other process to modify this variable. So you are not interested in accessing blocking all other processes in the system. You are only interested in 
blocking the processes that are accessing the shared variable.


======
Critical handling in OS
========
critical section problem also appears in the OS themselves. Because OS and Kernel themselves are multithreaded. Most kernels are multithreaded. So a Kernel
could be pre-emptive or non-preemtive. 

=========
Pre-emptive
=========
This is what we have been describing as setting a limit on the time that a process gets on the CPU and if that time quantum expires then the process will be
pre-empted. So when the system takes CPU from a process then that is preemption. So if you have a time quantum or a time limit then that is pre-emption.


===============
Non-Preemmption
===============
If a process is given the CPU or a thread is given the CPU until it locks or it releases the CPU then that is non-preemptive. 


========
Point to pick up
===========
The point is that the preemption is what causes the critical section problem. Because you have the critical section problem. The reason why we have a critical
section problem is that the process may get interrupted in the middle of modifying a variable. But if there is no preemption then a process may not get 
interrupted in the middle and we will not have a critical section problem. 

=========
Why have preemption?
=========
For responsiveness. This is why preemption is the kind of scheduling that we focus on.

=========
Peterson's Solution
=========
This is the first solution to the critical soln problem. .Now this soln is not a solution that we can implement. In fact it is a theoretical soln or an 
algorithmic decsription of a soln. We will see in a minute why it cannot be implemented. But then what is the value of it if it cannot be implemented? Why
are we studying it?  We're studying it because it is conceptually interesting, and it shows us how the three conditions for the critical section problem can
be satisfied. So, after we describe the soln, we will see how it satisifes the three conditions that we discussed above.
Now this soln is going to work for 2 processes (i and j). And there are two shared variables. These are the variable: term (which is an integer that indicates
whose term it is now to access the critical section). So if term = i then it is i's turn otherwise it is j's turn. 
Next we have a flag variable. Flag indicates if a process is interested in the critical section. If flag[i] = true then i is interested in the critical 
section and if flag[j] = true then j is interested in the critical section. Now, the assumption is that these variables, turn and flag may be modified 
atomically. So we are guaranteed not to get interrupted in the middle of modifying any of them. And this is the condition that is hard to satisfy in practice.
So this is the solution that makes Peterson's solution impractical.

======
Algorithm for process Pi
==========
do {

   flag[i] = true;
   turn = j; 
   while (flag[j] && turn == j) ; //do nothing 
   critical section
   flag[i] = false;
      remainder section 
} while (true)


//Comments: Okay. So here's the code on the "i" side. And here's the code on the "j" side. Now we have to get used to parallel programming whhere two processes
            are executing the same code in parallel but they may be executing different lines in this code. So now we have to get used to parallel programming.
            Entry section code explanation: 
              So, we first set the flag to true for i. So i is saying that I am interested in the critical section. Then it says turn = j (or turn equals other).
               Here, it is saying that I am yielding to the other process. And then "i" is going to check if flag[j] = true  (meaning that the other process is 
               interested and the it is the turn of the other process then I am going to wait). 
               

======================================================================================
How will the above ensure that there will be only one process in the critical section?
======================================================================================
Well first of all, if one process is interested in the critical section and the other process is not interested then we don't have a problem. Now what if both
happen to be in their empty section in the same time. What if process "i" executes lines flag[i] = true and turn = j and then process of i gets inerrupted there.
Now if process j starts executing and ececutes its two lines (until turn = i). Now if i gets the CPU again and executes again, will it access the critical
section or not? Well since turn is not equal to "j" anymore then it means that means that the condition in process "i" will be false and then it will 
enter the critical section.
So, this turn variable is very important because it will ensure that a process will not enter the critical section when the other process is in the 
critical section. And processes here are not being selfish. 
Now we will appreciate the turn variable in other solutions that do not have this. So now, this satisfies all the conditions!

=============================================================================================================
Since both processes have access to the "turn" variable, wouldn't make that specific piece of code "critical"
============================================================================================================= 
Yes, it is critical, but it is guaranteed to be ATOMIC. So access to turn and flag[i] is guranteed to be done atomically. So that is why, this is a theoretical
solution. There is a mechanism that will guarantee that access to turn or flag[i] is atomic then this would work correctly.


=======================================
Details on Peterson's Solution (Cont.)
=======================================
Provable that all three Critical Section requirements are met:
  1. Mutual exclusion is preserved 
      Pi enters CS only if 
        either flag[j] = false or turn = i
      turn is either i or j but noth both 
   
   2. Progress requirements are satisfied. The process in its remainder section will have its flag set to false; therefore, it cannot be selected. The other  
      process will necessarily be selected if its flag is true.
   3. Bounded-waiting requirement is met. After executing its critical section, each process set its flag to false, and if it tries to enter its critical 
      section again, it will set turn to the other process number.

===============================================
How Peterson's solution satisfies all 3 points
===============================================
Obviously, it satisfies the mutual exclusion because turn cannot be i and j at the same time. If turn is i then i access the critical section and if it is j
then j accesses the critiical section.
And progress is also obvious. Because when two processes are interested in the critical section, then the process that turn points to is going to be the 
process that is going to enter the critical section. And processes in their remainder section are not going to get selected. Because if a process is in 
its remainder section then its flag is equal to false, which means that it is not interested. And if one of the processes is not interested then the other 
process can access the critical section without having to wait.
Now what about the bounded wait? So the bounded waiting is the one tthat we need to understand very well. So we can understand this by asking the question, if a
process attempts to enter the critical section two times in a row, while the other process is waiting then will it be able to do that? Will a process be 
able to enter the critical section two times in a row while the other process is waiting? Suppose that a processs enters the critical section, then after it
is done with the critical section, it is going to set its flag to false (so it is going to say that I am not interested). Now this remainder section could
be short or long. Now remember with all of these synchronization problems, we can never make any assumption about the relative speed of the processes. So 
we cannot assume that by the time process "i" coompletes its remainder section, process "j" will have gotten the CPU and will have found flag[i] to be false.
No. You cannot make any assumption about the relative speed and you cannot make any assumption about the fairness of the scheduler. So the scheduler may not 
be fair. Meaning (suppose now ) that process "j" is waiting because process "i" is in the critical section. Thene while process "j" is waiting, it loses the 
CPU. The last time it checked, flag[i] was true and turn was equal to "i". And it was waiting. But then it lost the CPU. For how long, it will lose the CPU, 
we don't know. So while this doesn't have the CPU, the process "i" could have completed the critical section, completed the remainder section and executed the 
second iteration of the loop.
Now when process "i" does the second iteration of the loop, it is going to turn flag[i] = true. And then it wiill yield to the other process. Now all of this
is happening (process "i" has the CPU and the process "j" still doesn't have the CPU). Now at this point, process of "i" will find that the flag[j] is true
then it is going to wait. 
So even though process "i" is enjoying more CPU time, it is going to get stuck here because this "turn = j" is going to yield to the other process. And eventually
the other process is going to get the CPU. And at this point , flag[i] is going to be true but turn is going to equal to j because the other process set the 
turn to j. And then "j" will enter the critical section.

=========
Conclusion
===========
So with this solution, it will be impossible for a process to access the critical section two times in a row, while another process is waiting. Ofcourse, it 
can access the critical section a hundred times if the other process is not interested, but if the other process is waiting then a process can access the 
critical section just once, while the other is waiting. 
And this is exactly what we mean by bounded waiting. If you are waiting then you are guaranteed that the other process will not access the critical section 
more. So bounded waiting means that there is a limit on the number of times that other processes will access the critical section. And this limit, in this 
particularr case is 1. The other processes can access the critical section only once. So this is bounded waiting.
Now if we don't have bounded waiting then we would have "Starvation".


                                    ===================================================================================================
                                      Operating Systems Lecture 25: Process Synchronization (Part 3): Synchronization Hardware
                                    ====================================================================================================

=========
Intro
=========
Now we will talk about hardware instructions for syncrhonization. Before we talk about this, we want to indicate that on a uniprocessor system, it would be
possible to solve the critical section problem by disabling interrupts. Remember that if a process is guaranteed not to get interrupted while woring in 
the critical section then we don't have a problem. 
Now one way of doing that in the kernel is if the OS may temporarily disable the interrupts and then enable them again when a process exits the critical 
section. 
Now this is feasible only on a uniprocessor and only within the kernel. On a multiprocessor system it would not be feasable. Because we will have to send
the message to all the processors.


===================================
Details on Synhronization Hardware
==================================
Many systems provide hardware support for implementing the critical section code.
All solutions below based on idea of locking
  Protecting critical regions via locks
Uniprocessors  could disable interrupts
  Currently running code would execute without preemption
  Generally too inefficient on multiprocessor systems beacuse you have to pass a message to all processors. 
Operating systems using this not broadly scalable
Modern machines provide special atomic hardware instructions
Atomic = non-interruptible
 Either test memory word and set value
 Or swap contents of two memory words

=========================
test_and_set instruction
=========================
Modern processors provide synchronization instructions like the test_and_set instruction. Now even though, this looks like a C function, this is NOT A C
function. This is a hardware function. And these three lines of code are not actual code. This is a conceptual description of this instruction (what it does).


//Comments: What does the instruction do? It takes a pointer to an integer, a variable or a boolean. And then it first saves the current value of this boolean
and then it changes the value of the boolean to true. And then it returns rv. And what does rv mean here? What will it have? The old value. So it sets it to 
true and returns the old value.

=======================================
How can this help in synchronization?
=======================================
Now this is an atomic instruction .This is a hardware instruction. You cannot get inerrupted in the middle of doing this. You cannot get interrupted in the 
middle of one instruction. Remember when we described the critical section problem, we have a critical section problem if we get interrupted in the middle
of making an update that involves multiple instructions. But you cannot get interrupted in the middle of one intruction because if you get interrupted in 
the middle of one instruction, you resume that instruction. If the instruction doesn't complete then it doesn't do anything. So it is either completely 
done or it doesn't do anything. An instruction will not be partially done. 

==========================================
So how can we use this for syncrhonization
==========================================
Well we can use test_and_set on a lock variable. So lock is a variable that indicates whether a critical section is locked or not. If lock is true then 
critical section is locked otherwise it is not locked. And every process is going to do test_and_set. So it is going to say, okay set the lock (lock it for
me) but tell me if lock was true or false before I set it.  If it was true then it means that another process must have locked it and it is going to wait. 
So waiting is going to depend on an older value of the lock. Because the new value will always be true. Test_and_set will always set the lock to true. 
So this waiting is going to depend on the older value. So you are saying if another process has locked this before me then I will wait until the lock becomes 
false. And when does it become false? When a process is done with its critical section. It is going to release the lock by setting the lock to false. 

====================================================================================================================================================
Do we have bounded waiting? Or in other words, can a process access the critical section multiple times in a row while another process is waiting?
====================================================================================================================================================
Well let's trace it. Suppose that there is a process that finds the critical section available and it locks it. While it is in the critical section, another
process arrives and it finds the critical section to be true. And the other process is here waiting. So the first process is in the crit sec. The 2nd one is
in the waiting loop. Remember in the parallel processing, we have multiple processes executing the same code, but each process could be on a different line
in the code. Now when the process that is in the critical section is done is going to set the lock to false. Now ofcourse when it does that, the other process
that is waiting, if it has the CPU then it going to detect it and then it will enter the crit section.
So if scheduling is fair then we don't have a problem. But what if the scheduling is not fair. What if the process that is waiting uses the CPU for a long
time and while that other process is not on the CPU then the current process unlocks this and then it completes its remainder section and then it completes
the next iteration of the loop. Now when it does so, what will it find the value of lock to be? It will find it false which means that it can enter the 
criticall sec again. 
So if scheduling is not fair. Or if the process that is in the crit sec gets enough CPU cycles then it may keep entering the critc sec for an unbounded number
of times. How many times? We don't know. It depends on how many CPU cycles it is going to get and how many CPU cycles the other process will get. So it depends
on the relative speed. But we cannot make any assumption about the relative speed. We cannot assume that both of them will go at hte same speed. One of them 
could be going much faster than the other. ANd we don't know which one will be going faster. 
So in conclusion, this particular solution DOES NOT set any bound or any limit on the number of times a process can access the crit section while another 
process is waiting. So this is exactly what we call "starvation".

=========
Starvation
==========
When the waiting is not bounded, a process is starving. Now starvation is different from a deadlock. So the other process is not deadlocked, it is not starving. 
So deadlock means waiting forever. But starvation doesn't mean that waiting forever. It means that there is no limit on the number of times that the process
is waiting. Or the number of times that the other processes may get the critical section. 
In practice, starvation means waiting for a long time. But not waiting forever.

===============================================
Another atomic instruction : compare_and_swap
================================================
This is another atomic instruction that does the same thing. It is called compare_and_swap. This takes a value or a pointer to an integer and the expected
value and a new_value. First it is going to store the oldValue and then it is going to say, "Okay, if value is equal to expected value then I am going to 
change it to this new value and then I am going to return the old value. And it is very similar. 
We can use it with a lock variable. WE initialize it with a 0 (which is unlocked). Then we do compare_and_swap(), so we are saying here is a pointer to the 
lock. if the expected value is zero (which means that it is unlocked) then block it for me. Then we do compare_and_swap : so we are saying, "Okay, here is a 
pointer to the lock. If the expected value is 0 (which means that it is unlocked) then lock it for me i.e set it to 1). So this is the same thing as test.


==============================
Unbounded waiting here as well
==============================
So just like test_and_set() instruciton the waiting in compare_and_swap() is unbounded as well. Which means that the process can access the critical section
multiple times while the other processes are waiting.


====================================================
Bounded-Waiting Mutual Exclusion with test_and_set
====================================================
.
. //PSEUDOCODE IN SLIDE #22
.
Intuitively in the petereson's soln, we had a way of ensuring or satisfying the bounded-waiting condition. But in the test_and_set soln, the bounded waiting
was not satisfied. So what is the diff. What is the thing that we had in the Peterson's to ensure bounded waiting that we don't have here? 
So we had that "turn" variable. And each process was yielding to that other process. 
So here, the process can be selfish. There's no mechanism for a process to yield to another process. So when it is done with the critical section, it is not
going to check if anyone is waiting. It is going to take it again and again and again if it is getting enough CPU cycles. It can keep getting that critical
section for an unlimited number of times. Now, in order to solve this problem, we will make the processes unselfish. Or we will make the processes more 
considerate of other processes. 
SO when a process is done with the critical section, it is not going to grab it again, if it is getting enough CPU cycles. It is going to check on other 
processes to see if any one of them is waiting. So the key idea is after a process is done with the crit section, it will not act selfishly and take it 
again if it is getting more CPU cycles than other processes. No, it is going to check and if it finds AT LEAST ONE OTHER PROCESS that is waiting then it 
is going to give it the critical section. Now if it doesn't find any other process that is waiting thne it will feel free to take that critical section again.
So it will take it only if no other processs is waiting.

=======================================
How do we implement the above approach?
=======================================
So there is a waiting array. So, for each process there is a waiting array flag indicating whether a process is waiting or not. And this flag that will be 
used by the process that has just exited the critical section to checkk on other processes if they are waiting or not. 
And we have the "key", which is the value returned by test_and_set().  So if the older value is locked and the current process is waiting (so remember each
process is going to set itself to waiting and then I will check for the key). And if key is locked, then it will keep waiting. So clearly here, if waiting 
is true and key is true then this process will be in an infinite loop. We are setting the key to true and waiting to true. So if none of these value changes
we will be in an infinite loop. 
But we will not be in an infinite loop. Why? Because other processes may be changing key and waiting. In fact the way that a process will be giving that
crit section to a process that has been waiting is by setting waiting for that other process to false. So now what is the purpose of setting the key to 
true here, before we enter the loop. What if we delete this line? If key is not true then the value of key could be false. And if the value of key happens
to be false then we will not execute the loop at all. So we will checkk the condition once and we will not execute the loop. Which means that we will not 
do test_and_set at all. So we can get the same thing by doing a "do while" instead of a "while". So what setting key to true here does is ensuring that
we do set_and_test at least once. Because we don't want to enter the crit sect without testing.
Ok, so now after our process is done with the critical section, it is going to say, "Okay, I am going to check on other processes".  


======================================================================================
How does a process check on other processes after going through its own critical section?
========================================================================================
Let's say that that we have n processes and let n = 5 
Total processesa re: 

   0 , 1 , 2 , 3, 4
Let i  = 2 i.e the current process that just exited the crit section. Now it is going to check on 3, 4, 0, and 1 processes and check if other are waiting. 
Let's say: 
   F  T  i  F  F
   0  1  2  3  4

So, the current processs is going to check on other processes in a circular fashion and it is going to give the critical section to the first "true" value
that it finds. And remember that if a processes's waiting is set to true then it means that it is current stuck in the first while loop. Resultantly, we 
will set its waiting to false (meaning that it can now have the critical section). 
  

===========================
What if nobody is waiting 
===========================
Well if nobody is waiting thhen it will feel free to take the critical section again. 

=====================
What is "j" variable
====================
j will check on other processes. It is initially equal to i + 1 (mod n) because we are doing this in a circular manner. 

===========================
While i != j  && !waiting
===========================
So we will exit the loop if another process is waiting or if j != i. Now what does j == i here mean? It means that all other processes's waiting were false.
If all other processe are false then it will set the lock to false. If not then someone is waiting, so it will take that j (the first process that it found
waiting) and it will set its waiting flag to false. Now when it sets its flag to false. That other process will exit the first while loop next time it gets the 
CPU.
 

                                    ===================================================================================================
                                       Operating Systems Lecture 26: Process Synchronization (Part 4): Mutex Locks and Spin Locks
                                    ====================================================================================================

======
Intro
=======
Using the hardware instructions directly may not be very convenient. Just like programming in machine code. And that is why, OS provides programmers with 
high level constructs that they can use for syncrhonization:
- Locks
- Semaphores 
- Monitors 


So in fact today, we will be looking into locks and semaphores. And in a later lecture, we will be looking into monitors.


============
What are locks and semaphors
===========
These are synchr.. constructs that are provided by the OS and they are built on top of the hardware instructions. So the OS uses hardware support to provide
programmers with these easy to use sync constructs. We will see that using the lock will easily solve hte critc sect problem. They are abstractions built on
top of hardware support. 

=====
Locks
====
What is a lock? it is an OS construct that allows a process to lock a critc sect. What does locking a critc section mean? It means that while this process 
is in the critc sect, no other process is alllowed to be there. And everything is managed by the operating system. 
So if we have a lock x. Or if process#1 does Lock(x) and it does CS and then after that it umlocks the critical sectin. 
And in a similar fashion p2 does all the same (locks(x) and unlocks(x))  And then P3 does lock(y), then it does its critical section and then it does the 
unlock(y).
Then if p#1 executes its lock, which is implemented as a syscall. If you POSIX, it will be an API, but it is an implementation or an actual system call.
The system is locking "x" for P#1. So now p#1 can proceed and use the critc section and modify the shared variable. Now if p#2 arrives, it will not be
allowed to enter the critc section. Because the critc section is locked by the p#1. And all of this is managed by the OS. You are guaranteed that the lock
Operation is atomic. And you are also guaranteed that a process will not get interrupted in the middle of locking. And you are guaranteed that whne a process
has the lock and its in the critc section, we are guaranteed that no other process will be allowed to enter the critc section. 
Now if P#3 for example is trying to access the crtic section that has been locked by the lock(y) thne P#3 can access the critc section because it is a totally
different lock, and hence a totally different critc section.  

  ******Also it is imp to understand here that if a process in a critc section, then the OS is not 
guaranteeing you that this process will not get interrupted by the timed interrupted. There is no guarantee that this process will keep the CPu
While it is in the critc section. The guarantee is that another process trying to access the critc section will be blocked. But this process
that is in the critc section may need the CPU multiple times. But even if it uses the CPU or whatever happens, any other process that is trying
to access a crtic section that is protected by the same lock (i.e lock(x)) will not be allowed to access the same critical section.

======
Acquire() keyword
=======
Lock() also means acquire(). But in the POSIX library that we will be using, they are called lock() and unlock(). But you can call them acquire() and 
release() because it is the same concept.

================================
How are these locks implemented?
================================
- acquire () { 
    while (!available)
     ;  /*busy wait*/ 
    available = false; 
  }

- release () { 
    available = true; 
  } 

- do { 
    acquire lock 
       critical section 
    release lock 
       remainder section 
} while (true); 

//Comments: Now it is imp to understand that there are two diff ways of implementing these locks. THe process that is waiting for critc section could be in
a busy waiting loop. Like the above. So the above is a conceptual description of acquire() and release() (or lock and unlock) where the processs is 
checking on a variable available. And as long ast he var is unavailable it is going to keep looping. 
So the process is busy waiting. So when the process is doing this, it's wasting CPU cycles. It's potentially wasting CPU cycles. And eventually when it 
gets into the crtic section, it's going to set available to false to lock that crtic section. And when its done, it is going to set it to true. 

Now this, this is not actual OS code. This is not how a lock is actually implemented in OS. This a cocneptual description. And in fact this implies that a 
lock IS IMPLEMENTED AS BUSY WAITING. 


========
Busy waiting lock i.e Spin Lock
=============
A spinlock implies that it is implemented using busy-waiting. Otherwise, it is just a regular mutex-lock.


=====================================
Other methods of implementing a lock
=====================================
Now if a lock is not implemented using busy waiting, how do you think it will be implemented? How else? What other alternative? USING A QUEUE.
So basically, for each lock, there is a queue. So for lock(x) there is a queue. When p1 arrives, when the queue is empty, no other process has the lock or 
is waiting for the lock then it enters the critc section. Now when process#2 arrives, the lock is taken by process#1, so it will be placed in a queue for x.
Now when process#2 is stored in a queue, what is the state of process#2? WAITING! So this is another kind of waiting. 

===========================
Different kinds of waiting
===========================
So far we have seen different kinds of waiting or different reasons for waiting. One of them was waiting for an I/O device. I/O device had a queue. Another 
one was a parent waiting for a child. Another kind of waiting? WAITING FOR A MESSAGE TO GET RECEIVED OR SENT. 
There were two kinds of message passings: 
- Blocking 
- Non-blocking.

If it is blocking send and receive then the process that is waiting is in the waiting state for that message. Sometimes in the context of OS, the blocking
and waiting may be used interchangeably. A process blocks or waits means that that process is in the waiting state. It is not on the ready queue and the 
scheduler cannot select it for the CPU.

====================================================
When does it make sense to use spinlock vs mutexlock
====================================================
The first question is: if this section of lock(x) is small. So, a critical section is normally an update of a shared variable. So if this CS is just like 
incrementing a variable. Then this is expected to complete within a short period of time. And the process that is waiting is not expected to be waiting for 
a very long time. In this case, which one makes more sense? Spinlock! It's just like waiting for something and you expect it to be done within a short period
of time, like our car to get it fixed and they tell you that it is going to be ready in 10-15 minutes and then you'll probably wait for it. 
But if it takes hours then you'll leave it and come back later. 
If we don't expect "x" to be taking a long time then it makes sense to use a spinlock. 

===============================================================
Does it make sense to use spinlocks when we have a single CPU?
===============================================================
It doesn't. The question is will it be efficient if we use spinlock on a single CPU? 
Now, the entire point of doing a spinlock is to avoid a context-switch. So if p2 is in the running state (or it has the CPU), we know that it will not be 
running for a very long time. And htat very quickly (very few cycles) the lock will become available and this process will proceed into the critical section.
Without having to lose the CPU. Without context switching. 
But can a process that is waiting , ever get the CS without losing the CPU (if there is a single CPU)? BECAUSE PROCESS#1 will have to unlock it. And even if 
there a few operations in lock(x) then this process#1 needs the CPU to make progress. And if currently process#2 is on the CPU (and we have a single CPU) then
neessarily P#1 doesn't have the CPU. And without having the CPU, it willl not be able to make any progress. So it doesn't make sense to do spinlocking when 
we have a single CPU. Beacuse the CPU cycles that this process is using here to wait.


================================================
How does a system unlock when there is a queue?
================================================
It is going to look at the queue and see if there is a process that has been waiting for this block then change its state from waiting to ready. So it will 
pick one of the processes from the queue and it will put it a READY state.

======================================================================
What if a process terminates midway without unlocking? Can it happen? 
======================================================================
If you wrote this wrote this code and then you forgot to unlock and the process exits without unlocking then that may cause a lock to be held forever. And that
is a bug. So it is the programmer's responsiblity to ensure that they unlock before exiting. 
Now if there is an exception then that means that a process may have done something wrong. And then that process will get terminated.


==========================================
When does it make sense to use Spinlocks?
==========================================
Spinlocking makes sense if you are going to wait a short period of time on a multiprocessor system. It makes sense if the waiting time is going to be short.
So p1 and p2 may be on different processes. If the CS is small then spinning will happen only for a short period of time. So we will only be wasting a few 
machine cycles. But then the process#2 may get the lock without losing the CPU or without ever getting interrupted.


                                    ===================================================================================================
                                             Operating Systems Lecture 27: Process Synchronization (Part 5): Semaphores
                                    ====================================================================================================


=====
Intro
======
With a semaphore, you can do what you can do with locks and more. The best way to describe and thinking about these is to think of a sempahore as a variable S
that counts the number of available isntances of a resource. And when we say wait for S, it means that if I have at least one instance of the resource
available then I am going to proceed and execute the rest of the code. If not then I will be waiting. 

=====
Pseudcode -- Post and wait functions 
=========
//Wait function implementation 

wait (S)
  while (S <= 0);   //means that no instance of the resource are available.
  S--; //What is the point in decrementing S? Taking an instance of S. A semaphore could be a binary semaphore or a counting semaphore. The value of semaphore is 
      // not necessarily 1 or 0. 
  ---
  ---

 //Post function implementation

post(S) //or signalling is just the releasing an instance of that resouce.
   S++;

=============================
Protecting a Critical Section
==============================
We can do it like this:

  S = 1
  wait (S)
    CS 
  post(S) 


//Now again the semaphore is implemented by teh OS. The process on waiting on the semaphore is guaranteed to be atomic. So again, this is a conceptual 
description. This is not actual code that implements a semaphore. In fact the OS has to do many things when it implements a semaphore. And again, a 
semaphore may be implemented as busy waiting or as a semaphore with a queue. And we will assume by default that it is implemented using a queue. Which means
that the process that will be waiting for a semaphore will be placed in a waiting queue for that semaphore. 
And we will be using post and signal interchangably. Both are same thing. 


================
Semaphors Usage
================
So we have a critical section problem i.e protecting access to shared variables. It's something that you can do with semaphores AND locks as well. 
But tehre are other fundamentally different sycnhronization problems. 
A fundamentally different synchronization problem is the dependence problem. 
If you have two processes P1 and P2, and you have code in P1, S1 (S1 is a code section). 
And then code sectionn P2, S2. 
And S2 is dependent on S1. So we don't want S2 to be executed until P1 has executed S1. Now remember that these are two processes running in parallel. So P1
and P2 could be different programs. 
So how do you ensure that the execution of S2 will not happen until P1 has completed S1. One way of solving this problem is using semaphores. So you will have
the dependent process wait on a certain semaphore. And you have the process that has S1 signal that when it is done. So when it is done, it is going to signal
it. Now in this case, what is the logical initial value for the semaphore. What should we initialize the semaphore? How many instances do we want to be 
available in the beginning? Do we want P2 to just enter and execute S2? No! Initially, we don't want it to. So we would like that value to be initialized to 
0.So initially, no instance of the resouce is available. And P1 after executing S1 will make an instance of the resource available. When that instance 
becomes available only then can P2 excecute S2. 
So conceptually, this is a totally different synchronization problem than protecting a critical section. So this dependence type of problem is something that 
you can do with semaphores but not with locks. So this is why semaphores are more general. 

=============
Semaphore implementation
====================
It can be implemented using busy waiting or with a queue. WE will assume the latter. So each semaphore will have a value and a queue. And when a process does 
a post(), it increases or increments the number of available instances, the system will check the queue that has all the processes that are waiting for 
this semaphore and it will pick one of them and it will change the state of that process from waiting to runing. And by the way, we will assume that this 
queue is by default implemented as a FIFO. 


                                    =============================================================================================================
                                             Operating Systems Lecture 28: Process Synch. (Part 6): Deadlocks, Starvation & Priority Inversion
                                    =============================================================================================================

======
Intro
======
These are imp concepts in synchronization.

=====
Deadlock
========
We will illustrate it by example. A deadlock means that two or more processes are waiting for each other forever. So if we two semaphores S and Q and two
processes 0 and 1. If process 0 waits on S and then it waits on Q then it does some critical section code then it signals S and signals Q and p1 does wait 
on Q and then waits on S and then some critical section cdoe then singalling Q and sigalling S. 
Now here is a possibility of having a situation where two processes are waiting for each other.


                                    ===================================================================================================
                                             Operating Systems Lecture 35: CPU Scheduling (1): Basic Concepts
                                    ====================================================================================================

=======
Intro
=======
We already know that what a scheduler does, but we are about to study the how. What does the SCHEDULER DO? It is that important part of the OS that decides 
which process the CPU if there is one CPU. If we have multiple CPU then it decides which process gets which CPU and for how long. 
Now, in the beginning part of this chapter, we will assume that there is a single processor. And then we will discuss multiprocessor scheduling. 

========
basic concepts
==============
Maximum CPU utilization obtained with multiprogramming.
CPU-I/O Burst Cycle -- process execution consists of a cycle of CPU execution and I/O wait. 
CPU burst followed by I/O burst 
CPU burst distribution is of main concern.

======
CPU Burst
=======
A CPU burst consists of all the operations that DO NOT INVOLVE THE I/O or DO NOT INVOKE the system calls requesting something from the system. So any 
arithmetic operation (like add or multiply) or even memory operation (like load and store). 
But when you do something like read from a FILE or read from a keyboard then this is something that the process requires the OS intervention for. So it is 
done via systemcall. Consider that following operations are done in sequence 

add
mult
load
store
read from file //this operation marks as the end of a CPU burst. And now I/O burst starts and the process will be waiting for I/O until that service is complete.
               // - and then the state of the process will change from waiting to ready. And we will let the scheduler choose.


=========
Comments on the above set of operations
==========
So loads and stores or memory operations, even though that they are memory operations, when a process is performing memory operation, the CPU may be idle, 
depending on the architecture, but during the execution of a load or a store the CPU may be idle and the memory system is doing the work. But even in that 
state, from OS point of view, that is considered CPU time. So, a process has the CPU whether it is doing arithmetic operations or memory operations. From 
OS point of view, the process has the CPU, and may not be utilizing the CPU. There may be a memory operation that may cause the CPU to stall. But from OS 
point of view, it counts as CPU time for that process. 
When a process is doing any CPU burst operation, it is in the running state.

===============
What is not CPU Time? 
===============
When a process is waiting for an I/O. 

============
Histogram of CPU-Burst times
==================
The horizontal axis is the duration or the CPU length and the vertical line is how frequent this occurs.  And we will see that the most  common or the most 
frequent CPU-Burst length is about 2 milliseconds. For CPU bursts that are longer than 8 milliseconds are RARE. And why is that?? 


=============================================================
Why are CPU bursts that are longer than 8 milliseconds rare? 
============================================================
It's very unusual for a process to go 8 milliseconds without I/O. I/O is what? Reading from the keyboard, writing to the screen or accessing a file. 

==========
How long is two milliseconds?
========
How many operations will a process do in 2 milliseconds? There will be MILLIONS OF CYCLES. It's very imp that we have a sense of these numbers and how long
is a 2 millisecond time period in the context of a comp system. 
If we are talking about 1GHz then that means 1 billion or 10^9 cycles/sec 
OR 
1GHz = 10^9 cycles/sec

How many operations does that mean? Well it depends on the operation. If we talk about an add operation then that is one cycle, if we talk about multiply then
that is a few cycles or and if we are talking about a load and a store then that can take 100s of cycles if it misses all levels of cache and goes into 
main memory. 
So the number of cycles for an operation depends on what kind of operation that is. 

=========
Average cycles
============
On average, 10 cycles is a good average. A 100 is a too long adnd 10 is too short for an avg number of cycles per operation. But if we use 10 then this is 
 like: 10^8 ops/sec. 

So that is a 100,000 operations per second. Which means that in one millisecond, we will have 10^5 operations.

And can you imagine a process doing 100s of 1000s of operations wihtout doing I/O (like reading, writing etc etc).  That is why long CPU bursts are very 
rare. 

===========
CPU Scheduler
=============
Our focus is on the short-term scheduler, which looks at the ready quuee and picks one process from the ready queue and gives it the CPu. Now what is that 
ready queue. Well, in fact, in OS terminology, we use the term queue, even though the ready queue may not necessarily be implemented as a queue. It could be
implemented using any DS. It could be a implemented a FIFO queue, priority queue or even a tree.
In fact, as we will see, all modern OSs have the notion of a priority. So there is the notion of a priority. All realistic scheduling algorithms are priority 
based. And whne it is priority based, you need some kind of priority queue or a tree to implement the ready queue. So its the data structure, in which the 
system maintains the "all ready" processes. Decsions will have to be made when a process switches from running to waiting (by requesting I/O). Or when a
process switch from running to ready. While, in fact, the process itself will nnot switch from running to ready. It is the system that pre-empts or interrupts
the process via a timed interrupt and take the CPU away from the procecss and give it to another process via timed interrupt.

===========================
Pre-Emptive Scheduling
===========================
So when we have a process getting interrupted with a timed interrupt, we call this pre-emptive scheduling. It requires a timer. If you don't have a timer, 
you cannot implement pre-emptive scheduling. This is an example of a hardware feature that an OS needs in order to implement this OS functionality, which is 
pre-emption.

==========
Non-preemptive
===========
If it is non-preemptive, it is not going to interrupt any process. A process will get the CPU until the process itself releases the CPU (by maybe requesting
I/O).

========
Dispatcher
===========
Itis that part of the OS that basically implements the scheduling decisions. So the scheduler makes a decision to give the CPU to process x and the disaptcher
implements this by doing the necessary work to implement this like context switching, saving and restoring states of processes, switching to user mode (THIS 
IS VERY IMPORTANT). Then jumping to the proper location in the user program to restart that program.

======
Dispatch Latency
==============
The time taken for the dispatcher to stop one process and start another. So this step involves all the work above:
 - switching context
 - switching to user mode
 - jumping to proper location in the user program to restart that program.


============
Scheduling Criteria
=============
When we evaluate different scheduling algorithms, there are multiple criterias that we can look at for evaluating scheduling algorithms, and this chapter 
is going to be mostly about evaluating different sheduling algorithms. 
The criteria may include: 
i) CPU Utilization: We would like to maximize this. 
ii) Througput: It means that given a certain time period, what is the number of operations that will complete within this given time period. We would like to 
               MAXIMIZE this.
iii) Turnaround time: It is the time from start to finish. Or from the submission of a certain task to the system to the completion of that task. 
                      We would like to MINIMIZE THIS.
iv) Waiting time: It is the time that a process spends in the ready queue. We would like to MINIMIZE THIS. 
v) Response time: This is the amount of time between submission of a task to a first response.

In most cases, all these are related. And a good system will do a good job at maximizing the criteria that must be maximized and minimizing the criteria that 
must be minimized, but sometimes there are conflicts or tradeoffs.  Sometimes, optimizing one criteria may have negative impact on another criteria. 

==========================================================================================
If I do shorter time quantum, will this affect the response time positively or negatively
===========================================================================================
Positive. Because I'm making the time quantum short, so I am allowing multiple processes to make progress concurrently and the users of these processes are 
feeling shorter delays. So the system feels more responsive when I make the time quantum shorter.


==========================================================================================
If I do shorter time quantum, will this affect the turnaroundd time positively or negatively
===========================================================================================
The total time needed to complete a process will be longer or shorter? It will be longer. It will have a negative impact on the turnaround time. Because we 
will be spending or wasting more time in Kernel doing the context switching.


==========================================================================================
If I do shorter time quantum, will this affect the throughput positively or negatively
===========================================================================================
Negative. Because we are getting less work done, but we are giving the user the feel that the system is making progress or is responsive, so the user is 
feeling a responsive system, but you are actually getting less work done.

======
Conclusion
============
In this chapter, the main criteria that we'l be evaluating or using algorithms is going to be the waiting time. Or more specifically the average waiting time. 
Because usually we optimize, we optimize the average. But sometimes, it may make sense to optimize the minimum or the maximum like minimizing the maximum
response time to make the system more interactive.


                                    ======================================================================================================
                                     Operating Systems Lecture 36: CPU Sched. (2): First-Come, First Served (FCFS) and Shortest-Job-First
                                    =======================================================================================================

=======
FSFS
=======
This is the simplest and the most intuitive algorithm, but we don't expect it to be the most efficient algorithm. It stands for first-come, first-served 
scheduling. If you have a bunch of processes like P1, P2, and P3. And P1 arrived first, then P2 and then P3, and scheduled them in the order that they 
arrived: 
P1, P2, P3. 


Now here we are assuming that P1 arrived at the time 0 , P2 at 0 + DELTA1 and P3 at 0 + DELTA3 , where DELTA2 > DELTA1 and both DELTA1 and DELTA2 are much
smaller than the CPU bursts.

P1   0 
P2   0 + DELTA1
P3   0 + DELTA2

So there are small deltas that will distinguish between these processes in terms of arrival time, but these deltas are negligible when it comes to 
calculating delays and they are neglible compared to the CPU bursts. 
--------  -------------
Process |   Burst Time  |
--------  -------------
P1            24 
P2            3
P3            3 

So in this case, what is the average waiting time? P1 is going to wait for 0 time. P2 is going to wait for 24 time units and P3 is going to wait for 27 
time units. 

Average waiting time = (0 + 24 + 27)/3 = 17. 
So this is our average waiting time. 


========================================================================================================
Is FCFS a good algorithm? And if you were doing scheduling, would you do it in this order here or not?
========================================================================================================
Is this scheduling algorithm good if our objective is minimizing the average waiting time? No.

========================================
What will make the average time better?
========================================
Intuitively, we would place the shortest job first. This is very intuitive. This is going to be our next algorithm.

=====================================
SJF -- SHORTEST JOB FIRST Scheduling 
=====================================
This is an algorithm that schedules the shortest job first. This is another example:

Process     Burst Time 
--------    ----------
P1           6  
P2           8
P3           7
P4           3 

So, which one is the shortest? P4, so we will put it first. Then the shortest is 6 burst time. Then process#3 and then process#2.

[4 ][p1    ][   P3      ][    p2     ]
0  3       9            16           24 
 
- Average waiting time = (3 + 9 + 16 + 0)/4 = 7

  COMMENTS: That is that P1 waited for 3 time units + P2 waiting for 9 time units + P3 waiting for 16 time units + P4 waiting for 0 time units 


==========
Optimality
==========
SJF is optimal. If our objective is minimizing the average waiting time, SJF is optimal algorithm. This is very intuitive. So intuitive that we would not 
provide proof or formal proof of SJF because it is just so intuitive. 

====================
Waiting time in FCFS (FIRST COME FIRST SERVE) //NOT SJF BTW 
====================
From our processes table above: 

Process      Waiting time 
--------   ---------------
P1            0 
P2            6 //this will wait for the duration of Process#1
P3          6+8 //this will wait for the burst of P1 and P2 
P4       6+8+7 //this will wait for the burst of all the previous of these.


average waiting time = (0 + 6 + (6+8) + (6+8+7))/4 = 10.25  

=============================
Assumption that we are making
==============================
First of all, we assume that each process has a single CPU burst only to simplify things. So in our study of scheduling algorithms, we will always assume this. 

=================================
What is wrong with SJF algorithm?
=================================
Now this looks like a perfect algorithm, but it has an issue.  And in fact this is a problem with all kinds of priority based algorithms. So SJF is a 
priority based algorithm where the priority is the length of the CPU burst. Shorter bursts will give higher priority. 
Generally speaking, in any priority based algo, we have a "STARVATION PROBLEM". When there are priorities, you have the risk of starvation because lower 
priority processes (which are processes with longer CPU bursts may be waiting for a long time in the queue). Now, while starvation is a problem that any
priority based algorithm faces, and OSs have solutions to this starvation problem. 


=============================================
Any solution to the above starvation problem?
==============================================
Yes, something like bounded waiting. We can make it so that a process can only have a certain number of processes execute before it. And from implementation
point of view, a practical way of implementing this is to BOOST THE PRIORITY OF A PROCESS if it has been waiting for a long time. 
This concept is known as "AGEING". 

===========================================================================================================
Another obvious Problem in actually implementing the shortest job first algo from a practical point of view
============================================================================================================
So now imagine that you are trying to implement this in a real OS. You are writing a real OS and you are implementing this code, what would be the main difficulty
that you will face. That is: 

How do you know the actual CPU burst time? The challenge is to know or estimate somehow what the lenght of the next CPU burst is. And as an OS, you don't 
know the future. So the OS has all kinds of previlleges. It can execute previlleged instructions and it is in control of the system, but it is not magic, 
it doesn't know the future. It doesn't look ahead. It is not in the business of analyzing a process or determine what the process is going to do. 
Operating Systems do not analyze processes (UNLIKE A COMPILER). When a compiler translates a program from a high levle language into machine code, it analyzes
that program and it does certain type of optimizations based on its analysis of its program.  But the OS doesn't even understand what the process is trying
to do. It doesn't have the notion of semantics of a process. It only executes operations.
Now question is: 
                    What can the Operating System do at this point? 

It tries to estimate based on history. So something that the OS knows is the history. So now I am trying to do the operation "n", but as an OS, I have 
helped this process execute operation 1, 2, 3 all the way through operation n-1, and I can track the history. So the system can track history of that process.
So it can track the lengths of the previous CPU bursts, and based on the length of the previous CPU Bursts, the system may try to predict the length of the 
next CPU Burst. So the system doesn't know the future, but it can predict the future using the past. In fact, we will see this in not only in CPU schedulijng,
but also in other OS related algorithms like memory management or disk management.  So it is common for an OS to try to predict the future behavior of a process
based on its past. And this is exactly going to be our next topic.

=============================
Length of the last CPU burst 
=============================
The system can easily store this information in the PCB of a process. And it can save the length of the last CPU burst or even the last 2 or 3 or 4 or 10 
CPU bursts and predict the future behavior based on that. 


===================
NNEW LECTURE
===================

========
Starvation
===========
Strictly speaking, even if a process waits for a long time doesn't necessarily mean "starvation". Starvation means that you are not setting a limit on the number
of other processes that may get the resource while ap rocess is waiting. 
So if you have a process x that is waiting, you are not setting a limit on the number of other processes that will be getting the resource while the process
x is waiting.

======================
Starvation-free system
=======================
To achieve this, you need a limit on the number of other processes that will get the resource while one process is waiting. So it is NOT THE LIMIT OF TIME.
It is the limit of other processes. 
So if I am in #7 of the queue, and there are other processes then to avoid starvation, I should wait for just 6 processes only. 
And each of these 6 processes have 4 turns. So 6 * 4 = 24. That will still be bounded waiting. 
And as long as there is a bound or a limit there is no starvation. But if there is no limit then we don't have bounded-waiting.

What is the biggest starvation example that we have seen in the threads? 
The readers and the writers. The first solution is that if the readers keep arriving, the writers may start. So if we have a writer waiting and an unlimited
number of readers arriving and accessing the buffer then we have a starvation problem.
 

=============
Problem with FCFS
==============
We don't have a starvation problem because it is queue based. It is not priority based. 
The problem with FCFs is the LONG WAITING TIME. 

================
Problem with SJF
================
Starvation issue because if you are waiting.  And you are a process with a relatively long CPU burst and the processes with shorter CPU bursts keep arriving
then they will keep getting the CPU and we are waitin and there is no limit on the # of other processes that may keep arriving and getting access ot the 
resources. 
So this is the problem with SJF. 
And generally speaking, starvation is the problem that a certain FAMILY OF Schedulign algorithms suffer from. SJF is a special case of a more general kind
of algorithms that suffer from starvation. 

================
What is the general kind of algorithms in previous context?
=============
Any priority based algorithm. SJF is a priority algorithm. Where priority is the length of the CPU burst. So SJF is a special case of a priority based algorithm. 


=========================
Another problem with SJF
=========================
When implementing this, there is a certain difficulty that we face. Which is knowing the CPU burst time. Because we are trying to schedule based on the 
next CPU burst time. That is what we care about. That is what we are trying to schedule. And the next belongs to the future. And the OS doesn't know the 
future. But the OS knows the history, the past of the process. And based oon the past, it may make a reasonable estimate or guess about the future. And this 
is exactly what we will be studying today. Which is one way of estimating the length of the next CPU burst based on the length of the previous CPU burst.
.
.
. SKIPPED
.

                                        =============
Operating Systems Lecture 38: CPU Scheduling (4): Shortest-Remaining-Time-First & Priority Sched.
                                         =============



===========
SRTF
==========
This is the form of a SJF. This algorithm is basically more dynamic and more adpating than SJF. So in this case, we look at the time remaining of each process.
And ofcourse, it is based on our estimate. Time remaining indicates the future. We don't know the future, but we make an educated guess of the future. 

=====
Concept of arrival time
=========
Not all processes arrive at the same time.  We have different arrival times. And the differences in the arrival times are significant. And we have the 
concept of preemption. Where the higher priority process is going to preempt a process with low priority.


======
Table
========

====
Process   Arrival Time     Burst Time 
=======                      
P1           0                  8
P2           1                  4
P3           2                  9
P4           3                  5


=====
Simulating above
=======
- At time zero, we have just one process, the system has no choice, so the system will give the CPU to P1. 
- At time 1 , the system has two options i.e P1 and P2 and it is going to make a selection based on the remaining length of the CPU burst. So it is going 
  to 
.
.
.
.

                                    ======================================================================================================
                                              Operating Systems Lecture 55: Main Memory (1): Introduction and Basic Concepts
                                    =======================================================================================================

=====
Intro
=====
In order for a progra to run, it must be loaded into memory (we don't have to load the whole program but a part of it as well). 

====
Access Time to L1-Cache
=======
On modern systems you have other levels of cache (like L2-Cache, L3-Cache etc). The access time to L1-Cache is usually a couple of cycles (2-4 cycles). 

=======
Access Time for main memory
========
Hundreds of cycles! So this is 2 orders of magnitude slower than the L1-Cache. And in fact later on, in this course we will be talking about the disc (it is 
orders of magnitudes slower than main memory. Like 1000s of cycles for accessing the disc. It is the slowest part of the system). 

=======
Concept of Protection
=========
It has nothing to do with security. Protection means protecting the OS itself from buggy user programs that try to access memory locations that do not belong
to them and protecting user programs from each other. So basically what we want to accomplish is that whenever a program attempts to access a memory location
that doesn't belong it (or either belongs to kernel or belongs to another user program). The way we do that is through HARDWARE SUPPORT to detect out of bound
or illegal memory access to catch that and TRAP (trapping this and giving control to Kernel). Usually the Kernel would terminate the user program that does an
out of bound access.

=======
Objectives 
=======
Using memory efficiently. 
Allowing and providing user programs with enough memory to run. 
As an OS we would like to protect programs from each other and protect the system itself from buggy programs.

In our study of memory management, we will start with simple schemes that are unrealistic (real OSs do not implement them). Why? First to study some of the 
basic concepts. And then to understand why these simple schemes are not efficient (it will justify our study of the more complex schemes that real OSs use).
So we will get to understand why modern OSs get to implement more complex scehemes than the schemes that we will be studying at the beginning.


=====
Base and Limit Registers
========
Each program has its own block in memory. Each user process has its own block of memory. And at this point, each process has a contiguous block of memory. 
Each process has a single piece of memory that has the code and the data that this process needs. And each block of memory has its START ADDRESS and its LIMIT.
So the base is the start address and the Limit is the maximum address that the process has.


==========
Hardware Address Protection
============
Whenever a program attempts to access a memory address, we need hardware to check on the BASE and the LIMIT. So to check whether this address is greater than
or equal to the BASE. If it is greater than or equal to the base, it is valid. But if it is not then this program is trying to access something out of 
bound. And then we trap to the OS, which means an exception will get generated and the control will be given to the kernel.
So if we pass then we have to check if the address is less than or equal to the base+limit. If yes then it is legal otherwise, we trap to the OS. 

=====
Address Binding
==========
Binding memory addresses. So what do we mean by binding? Well first of all, if we have a source file (a program or code) then our code will have some variable
names in it (like: int x, int y,...). The compiler translates this into machine code and machine code doesn't have the notion of a variable name. It has the 
notion of memory addresses and registers. So each variable is either stored in the memory or in the registers. So that is why, the compiler will be translating 
these variable names into addresses. Now what kind of addresses? Now if the compiler translates these variable names into absolute addresses (the actual addresses 
that will be used), then this will be called "Compile time binding". This is something that the modern systems do not do. This was probably done in the DOS 
OS with the command.com file where the addresses that the corresponding machine code has are absolute addresses. 
But modern systems do not do compile time binding. Compile time binding means generating the absolute address. What compilers generate normally is known as 
"relocatable code". And relocatable code means for each of these variables, it will generate AN OFFSET RELATIVE TO SOME OFFSET OR BASE ADDRESS. And when 
will that base or start get determine? That may get determined at LOAD TIME or it may get determined at EXECUTION TIME. 
If these are local variables then the addresses generated will be relative to the starting address of the STACK FRAME. 

======
Relocatable code
=======
We will think of it as consisting of offsets relative to some base address and this base address is either determined at load time or at execution time. So
if it is determined at load time, it is known as "LOAD TIME BINDING" otherwise "EXECUTION TIME BINDING".

=======
Load time binding
==========
It means that when the program gets LOADED there will be ONE BASE ADDRESS FOR THAT PROGRAM and that will NOT CHANGE AS THE PROGRAM EXECUTES. So we just assign
a  base address to that program and when it is executing that base address, it doesn't time. 

=========
Execution Time Binding
===========
The base address for a program may be changing as the program executes. Ofcourse who will be changing the base address for that program as the program 
executes? The OS ofcourse so it has more flexibility in doing memory management. 

=========
When does Loading and Execution happen?
==========
It happens when you run an application. While compiling and linking is done by the Software Provider (by the people or the company that develops the software).
Almost all compilers have a linker with them. Without a linker, we will not be able to generate the excecutable code. 

========
What happens under the hood?
===============
File1.c ---CompilesTo----> file1.obj
File2.c ---CompilesTo----> file2.obj 

Linker ----Links: file1.obj and file2.obj----> Executable 


==============
Static Linking
===============
Suppose that you have a f(x) function call in your "file.obj"' and this function call's body belongs to another library file (let's say fileN.obj). Now it is
the job of the Linker to find the body of function f(x) in the library file and put it in the executable file. 

Demonstration:
-------------
call f(x)   |
.           |
.           |
.           |
fx          |
  .         |
  .         |
  .         |
            |
------------|

=====
Static Linking
======
The linking demonstrated above is exactly just that. When you get the software, you get the executable and then you load it and then you run it. Now this is 
what we call static linking because linking the program to this library function happens at link time and its called static because it is happening before
the program runs. So its something that the software developer will do for you when you get the software.


==========
Dynamic Linking 
================
So we have a static library just as above (a library obect file that has the code for this function in it) and you link to it at link time before the program
starts running. While with dynamic linking, if you have a dynamic linked library, the executable will not have the body of f(x), it will have some information 
on how to find the body of  f(x) at runtime. So basically this will have a name of a DLL. So it will tell it, the function f(x) will be available at runtime in
some dynamic linked library. So it will have the start of f(x) there in the executable. 
Dynamic Linking happens at the execution time. Dynamically linking means that in our executable whenever we have a function call, then this start will be used 
to find f(x) in some dynamic linked library. So first it will look for that DLL. If it is loaded then it will use it. If it not loaded then it will load it. 
So this DLL will be somewhere in memory and it will have the body of f(x) in it. 

===========================
Wrapping up Dynamic Linking
===========================
1. Finding this DLL
2. Loading the DLL if it is not already loaded in memory
3. Replacing call to f(x) with the address of DLL inside the excecutable. 


So in dynamic linking, the executable does not have the body of the function f(x) in it. It only has information about the DLL that has the actual body of 
the function f(x).

===================================================
Advantages of Dynamic Linking over static linking? 
===================================================
1. The executables will be smaller because they will not have function bodies in them.  What makes it efficient is that this DLL may be used by multiple 
   programs. SO this Dynamic Linked Library is a system DLL. Then many programs may be using the same DLL. So if you have 10 programs that are using the 
   same DLL then you have only 1 copy of that DLL. So it is definitely MORE SPACE EFFICIENT! 
2. If there is a more up to date version of this DLL then you'll get that update by loadingg a newer version of the DLL. So after purchasing an executable 
   if something is available in a dll you don't need to get a newer version of the executable to get that update. And some of these dlls are system dlls, 
   so they are not even part of the application software that you are getting. So updating will be easier. It static linking, the only way to update is to buildd 
   a newer version of the executable.

========
Disadvantage of DLL
=========
It is slower (because we will have to load the library when making a function call. But do we expect to load the library with every function call? No) 
but not much slower. Why? 

========
If a program is calling a function that is hardware specific then what to do? 
=========
Obviously you wouldn't want to statically link it. 

=========
Logical vs Physical address
========
When we have this runtime translation of an address that gets generated by an application to a physical address, this execution time translation or binding 
basically maps a logical address into a physical address. To simplify things, think of a logical address as an offset or as a relative address (an address
relative to a certain base) and the physical address is just equal to that relative address + the base, but the base may change in physical change. 

==========
Who does the Translation from logical address to a physical address?
========
A hardware unit called "MMU" (memory management unit) and it uses a "relocation register" which is a base register. It is called "base register" to emphasize 
that this address in the register may be changed by the operating system. So your logical address is this number 346 and it gets added into value of the 
relocation register and that gives you a physical address of 14346 and that is what gets sent to memory. So what the mmu here does is adding the value of 
the relocation register. Now this relocation register has the base address of the current process. The value in here is set by the operating system, clearly
the instruction that sets this value must be a previlleged instruction. So only the kernel has the previllege of changing this value so that it can control 
the base or the start address of each program and so that it can move programs around in memory to utilize the memory as efficiently as possible. So the value
here, the base, is in a register. And this certainly will be a part of the context switching, so when the kernel switches to a program, it must load into the
relocation register the value of the initial base address of that program.

========
Dynamic Linking
==========
Some systems use the term, "Shared library" or "Shared Object" with the extension ".so" (you see this on linux systems). These are equivalent to Windows DLL.

=======
Swapping 
=========
It is moving the memory block that belongs to a program from memory to disk because we don't have enough memory. 

========
Scenario for swapping
===========
let's say that we need to run process#2 and memory is full, so we need to make some space available in memory and the system may do this by swapping out 
process#1. And ofcourse the system will need to copy the whole memory image from memory to disk and then swapping in means copying the whole image from disk
to memory. 
This is a scheme that modern systems do not use.

======
Inefficiency in swapping
========
The size may be huge. So we are copying a contiguous memory block that corresponds to code and data for the whole program. Copying this to disk is extremely
expensive in terms of time (extremely slow) because the disk is very slow. We will see how slow when we get to disk management. 
Just to get an idea of how slow, if the transfer rate is 50MB/sec and memory that belongs to that process is a 100MB then we need 2 secs to copy that from meory
to disk or from disk to memory. And 2 secs is too much in computer world (and something that the user feels extremely slow). And if kernel is going to do this
as part of context switching then 2 secs is too much for context switchin because we expect the time for context switching to be in microseconds and even 
1 millisecond will be too slow for context-switching.
Anyways, this is something that the modern operating systems do not do because this is inefficient. Modern operating systems swap in pages instead of the whole
program becauses swapping a page is much much faster than swapping a whole program.

=======
Standard Swapping
========
So anyways, standard swapping is not used in modern operating systems and it may be used if memory is completely or the demand for memory exceeds a certain
threshold.

